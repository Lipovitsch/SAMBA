{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "761d17de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:05.357471Z",
     "iopub.status.busy": "2025-04-23T14:19:05.356825Z",
     "iopub.status.idle": "2025-04-23T14:19:05.360830Z",
     "shell.execute_reply": "2025-04-23T14:19:05.360186Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.011235,
     "end_time": "2025-04-23T14:19:05.361955",
     "exception": false,
     "start_time": "2025-04-23T14:19:05.350720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/Ali-Meh619/SAMBA.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bec690df",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:05.371128Z",
     "iopub.status.busy": "2025-04-23T14:19:05.370754Z",
     "iopub.status.idle": "2025-04-23T14:19:18.666323Z",
     "shell.execute_reply": "2025-04-23T14:19:18.665290Z"
    },
    "papermill": {
     "duration": 13.301548,
     "end_time": "2025-04-23T14:19:18.667933",
     "exception": false,
     "start_time": "2025-04-23T14:19:05.366385",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch-geometric\r\n",
      "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.11.16)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.3.2)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\r\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\r\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.2.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\r\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.5.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.2.0)\r\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.19.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.1.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.1.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.1.31)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.1.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\r\n",
      "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torch-geometric\r\n",
      "Successfully installed torch-geometric-2.6.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -q torchview\n",
    "! pip install -q -U graphviz\n",
    "! pip install torch-geometric\n",
    "# ! pip install torch-sparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab27f66",
   "metadata": {
    "papermill": {
     "duration": 0.004181,
     "end_time": "2025-04-23T14:19:18.677149",
     "exception": false,
     "start_time": "2025-04-23T14:19:18.672968",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac618192",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:18.687419Z",
     "iopub.status.busy": "2025-04-23T14:19:18.687160Z",
     "iopub.status.idle": "2025-04-23T14:19:29.015181Z",
     "shell.execute_reply": "2025-04-23T14:19:29.014237Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 10.335168,
     "end_time": "2025-04-23T14:19:29.016595",
     "exception": false,
     "start_time": "2025-04-23T14:19:18.681427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from __future__ import annotations\n",
    "# MODEL\n",
    "import argparse\n",
    "import configparser\n",
    "import copy\n",
    "import csv\n",
    "import json\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "from einops import einsum, rearrange, repeat\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    d_model: int\n",
    "    n_layer: int\n",
    "    vocab_size: int\n",
    "    seq_in: int\n",
    "    seq_out: int\n",
    "    d_state: int = 128\n",
    "    expand: int = 2\n",
    "    dt_rank: int | str = \"auto\"\n",
    "    d_conv: int = 3\n",
    "    pad_vocab_size_multiple: int = 8\n",
    "    conv_bias: bool = True\n",
    "    bias: bool = False\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.d_inner = int(self.expand * self.d_model)\n",
    "\n",
    "        if self.dt_rank == \"auto\":\n",
    "            self.dt_rank = math.ceil(self.d_model / 16)\n",
    "\n",
    "\n",
    "class Mamba(nn.Module):\n",
    "    def __init__(self, args: ModelArgs, hid):\n",
    "        \"\"\"Full Mamba model.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "        self.nl = args.n_layer\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "        self.layers = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        self.layers2 = nn.ModuleList([ResidualBlock(args) for _ in range(args.n_layer)])\n",
    "\n",
    "        # self.layers3 = nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        # self.layers3=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),AVWGCN(args.seq_in,args.seq_in,2,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        # self.layers4=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),gconv(args.seq_in,hid,2,10,args.d_model),nn.ReLU(),gconv(hid,args.seq_in,2,10,args.d_model)) for _ in range(args.n_layer)])\n",
    "\n",
    "        self.lin = nn.ModuleList(\n",
    "            [\n",
    "                nn.Sequential(\n",
    "                    nn.LayerNorm(args.seq_in),\n",
    "                    nn.Linear(args.seq_in, hid),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid, args.seq_in),\n",
    "                )\n",
    "            ]\n",
    "            + [\n",
    "                nn.Sequential(\n",
    "                    RMSNorm(args.seq_in),\n",
    "                    nn.Linear(args.seq_in, hid),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid, args.seq_in),\n",
    "                )\n",
    "                for _ in range(args.n_layer - 2)\n",
    "            ]\n",
    "            + [\n",
    "                nn.Sequential(\n",
    "                    RMSNorm(args.seq_in),\n",
    "                    nn.Linear(args.seq_in, hid),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hid, args.seq_in),\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # self.lin2=nn.ModuleList([nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in)) for _ in range(args.n_layer-2)]+[nn.Sequential(RMSNorm(args.seq_in),nn.Linear(args.seq_in,hid),nn.ReLU(),nn.Linear(hid,args.seq_in))])\n",
    "\n",
    "        self.norm_f = nn.LayerNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size)\n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(args.seq_in, hid), nn.ReLU(), nn.Linear(hid, args.seq_in)\n",
    "        )\n",
    "\n",
    "        self.nnl = nn.LayerNorm(args.vocab_size)\n",
    "\n",
    "        # self.proj=nn.Linear(2*ModelArgs.vocab_size, ModelArgs.vocab_size)\n",
    "        # self.lm_head.weight = self.embedding.weight  # Tie output projection to embedding weights.\n",
    "        # See \"Weight Tying\" paper\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            logits: shape (b, l, vocab_size)\n",
    "\n",
    "        Official Implementation:\n",
    "            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        x1 = x\n",
    "        x2 = x\n",
    "\n",
    "        for i in range(self.nl):\n",
    "            x1 = self.layers[i](x1)\n",
    "            x2 = self.layers2[i](x2.flip([1]))\n",
    "\n",
    "            x = x1 + x2.flip([1]) + x\n",
    "\n",
    "            x = self.lin[i](x.permute(0, 2, 1)).permute(0, 2, 1) + x\n",
    "\n",
    "            x1 = x\n",
    "            x2 = x\n",
    "\n",
    "        x = self.norm_f(x)\n",
    "\n",
    "        logits = self.lm_head(x)\n",
    "\n",
    "        #        a=logits.shape\n",
    "\n",
    "        #       #sq=torch.reshape(logits,(a[0],a[2],a[1]))\n",
    "\n",
    "        #      out=self.out(sq)\n",
    "\n",
    "        #     b=out.shape\n",
    "\n",
    "        #    out=torch.reshape(out,(b[0],b[2],b[1]))\n",
    "\n",
    "        return logits\n",
    "\n",
    "    @staticmethod\n",
    "    def from_pretrained(pretrained_model_name: str):\n",
    "        \"\"\"Load pretrained weights from HuggingFace into model.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name: One of\n",
    "                * 'state-spaces/mamba-2.8b-slimpj'\n",
    "                * 'state-spaces/mamba-2.8b'\n",
    "                * 'state-spaces/mamba-1.4b'\n",
    "                * 'state-spaces/mamba-790m'\n",
    "                * 'state-spaces/mamba-370m'\n",
    "                * 'state-spaces/mamba-130m'\n",
    "\n",
    "        Returns:\n",
    "            model: Mamba model with weights loaded\n",
    "\n",
    "        \"\"\"\n",
    "        from transformers.utils import CONFIG_NAME, WEIGHTS_NAME\n",
    "        from transformers.utils.hub import cached_file\n",
    "\n",
    "        def load_config_hf(model_name):\n",
    "            resolved_archive_file = cached_file(\n",
    "                model_name, CONFIG_NAME, _raise_exceptions_for_missing_entries=False\n",
    "            )\n",
    "            return json.load(open(resolved_archive_file))\n",
    "\n",
    "        def load_state_dict_hf(model_name, device=None, dtype=None):\n",
    "            resolved_archive_file = cached_file(\n",
    "                model_name, WEIGHTS_NAME, _raise_exceptions_for_missing_entries=False\n",
    "            )\n",
    "            return torch.load(\n",
    "                resolved_archive_file, weights_only=True, map_location=\"cpu\", mmap=True\n",
    "            )\n",
    "\n",
    "        config_data = load_config_hf(pretrained_model_name)\n",
    "        args = ModelArgs(\n",
    "            d_model=config_data[\"d_model\"],\n",
    "            n_layer=config_data[\"n_layer\"],\n",
    "            vocab_size=config_data[\"vocab_size\"],\n",
    "        )\n",
    "        model = Mamba(args)\n",
    "\n",
    "        state_dict = load_state_dict_hf(pretrained_model_name)\n",
    "        new_state_dict = {}\n",
    "        for key in state_dict:\n",
    "            new_key = key.replace(\"backbone.\", \"\")\n",
    "            new_state_dict[new_key] = state_dict[key]\n",
    "        model.load_state_dict(new_state_dict)\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"Simple block wrapping Mamba block with normalization and residual connection.\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.mixer = MambaBlock(args)\n",
    "        self.norm = nn.LayerNorm(args.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            Block.forward(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L297\n",
    "\n",
    "            Note: the official repo chains residual blocks that look like\n",
    "                [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> [Add -> Norm -> Mamba] -> ...\n",
    "            where the first Add is a no-op. This is purely for performance reasons as this\n",
    "            allows them to fuse the Add->Norm.\n",
    "\n",
    "            We instead implement our blocks as the more familiar, simpler, and numerically equivalent\n",
    "                [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> [Norm -> Mamba -> Add] -> ....\n",
    "\n",
    "        \"\"\"\n",
    "        output = self.mixer(self.norm(x))\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class gconv(nn.Module):\n",
    "    def __init__(self, inp, hid, embed, cheb_k, n):\n",
    "        super(gconv, self).__init__()\n",
    "\n",
    "        self.node_num = n\n",
    "\n",
    "        self.inp = inp\n",
    "\n",
    "        self.cheb_k = cheb_k\n",
    "\n",
    "        self.adj = nn.Parameter(torch.randn(n, embed), requires_grad=True)\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, hid))\n",
    "\n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed, hid))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        # output shape [B, N, C]\n",
    "\n",
    "        ADJ = F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "\n",
    "        support_set = [torch.eye(self.node_num).cuda(), ADJ]\n",
    "\n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "\n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "\n",
    "        weights = torch.einsum(\n",
    "            \"nd,dkio->nkio\", self.adj, self.weights_pool\n",
    "        )  # N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)  # N, dim_out\n",
    "        x_g = torch.einsum(\"knm,bmc->bknc\", supports, x)  # B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out_6 = torch.einsum(\"bnki,nkio->bno\", x_g, weights) + bias  # B,N,D_OUT\n",
    "\n",
    "        return out_6\n",
    "\n",
    "\n",
    "class AVWGCN(nn.Module):\n",
    "    def __init__(self, dim_in, hid, cheb_k, n):\n",
    "        super(AVWGCN, self).__init__()\n",
    "\n",
    "        self.node_num = n\n",
    "\n",
    "        self.inp = dim_in\n",
    "\n",
    "        self.cheb_k = cheb_k\n",
    "        self.node_embeddings = nn.Parameter(\n",
    "            torch.randn(n, dim_in, dim_in), requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(cheb_k, n, dim_in, hid))\n",
    "\n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(n, hid))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shaped[B, N, C], node_embeddings shaped [N, D] -> supports shaped [N, N]\n",
    "        # output shape [B, N, C]\n",
    "\n",
    "        supports = F.softmax(F.relu(self.node_embeddings), dim=2)\n",
    "\n",
    "        I = torch.eye(self.inp).cuda()\n",
    "\n",
    "        I2 = I[None, :, :].repeat(x.size(1), 1, 1)\n",
    "\n",
    "        support_set = [I2, supports]\n",
    "\n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "\n",
    "        # N, dim_out\n",
    "        x_g = torch.einsum(\"bnc,kncm->bknm\", x, supports)  # B, cheb_k, N, dim_in\n",
    "        # x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        x_gconv = (\n",
    "            torch.einsum(\"bknm,knmo->bno\", x_g, self.weights_pool) + self.bias_pool\n",
    "        )  # b, N, dim_out\n",
    "        return x_gconv\n",
    "\n",
    "\n",
    "class MambaBlock(nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        \"\"\"A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1].\"\"\"\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.embedding = nn.Linear(args.vocab_size, args.d_model)\n",
    "\n",
    "        self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.in_proj_r = nn.Linear(args.d_model, args.d_inner, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(\n",
    "            args.d_inner, args.dt_rank + args.d_state * 2, bias=False\n",
    "        )\n",
    "\n",
    "        self.norm_f = RMSNorm(args.d_model)\n",
    "\n",
    "        self.lm_head = nn.Linear(args.d_model, args.vocab_size, bias=False)\n",
    "\n",
    "        # self.x_proj_r = nn.Linear(args.d_inner, args.dt_rank + args.d_state, bias=True)\n",
    "\n",
    "        # self.x_proj = FourierKANLayer(args.d_inner, args.dt_rank + args.d_state * 2, 100)\n",
    "\n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "        # self.dt_proj=FourierKANLayer(args.dt_rank, args.d_inner, 100)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), \"n -> d n\", d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d)\n",
    "\n",
    "        Official Implementation:\n",
    "            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d) = x.shape\n",
    "\n",
    "        # x=self.embedding(x)\n",
    "\n",
    "        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)\n",
    "        (x, res) = x_and_res.split(\n",
    "            split_size=[self.args.d_inner, self.args.d_inner], dim=-1\n",
    "        )\n",
    "\n",
    "        x = rearrange(x, \"b l d_in -> b d_in l\")\n",
    "\n",
    "        x = self.conv1d(x)[:, :, :l]\n",
    "        x = rearrange(x, \"b d_in l -> b l d_in\")\n",
    "\n",
    "        x = F.silu(x)\n",
    "\n",
    "        gate = x * (1 - F.sigmoid(res))\n",
    "\n",
    "        y = self.ssm(x)\n",
    "        y = y * F.silu(res)\n",
    "\n",
    "        output = self.out_proj(y)\n",
    "\n",
    "        # o1=self.norm_f(output)\n",
    "\n",
    "        # o2=self.lm_head(o1)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def ssm(self, x):\n",
    "        \"\"\"Runs the SSM. See:\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        Args:\n",
    "            x: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311\n",
    "\n",
    "        \"\"\"\n",
    "        (d_in, n) = self.A_log.shape\n",
    "\n",
    "        # Compute ∆ A B C D, the state space parameters.\n",
    "        #     A, D are input independent (see Mamba paper [1] Section 3.5.2 \"Interpretation of A\" for why A isn't selective)\n",
    "        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4,\n",
    "        #                                  and is why Mamba is called **selective** state spaces)\n",
    "\n",
    "        A = -torch.exp(self.A_log.float())  # shape (d_in, n)\n",
    "        D = self.D.float()\n",
    "\n",
    "        x_dbl = self.x_proj(x)  # (b, l, dt_rank + 2*n)\n",
    "\n",
    "        (delta, B, C) = x_dbl.split(\n",
    "            split_size=[self.args.dt_rank, n, n], dim=-1\n",
    "        )  # delta: (b, l, dt_rank). B, C: (b, l, n)\n",
    "        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)\n",
    "\n",
    "        y = self.selective_scan(\n",
    "            x, delta, A, B, C, D\n",
    "        )  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        return y\n",
    "\n",
    "    def selective_scan(self, u, delta, A, B, C, D):\n",
    "        \"\"\"Does selective scan algorithm. See:\n",
    "            - Section 2 State Space Models in the Mamba paper [1]\n",
    "            - Algorithm 2 in Section 3.2 in the Mamba paper [1]\n",
    "            - run_SSM(A, B, C, u) in The Annotated S4 [2]\n",
    "\n",
    "        This is the classic discrete state space formula:\n",
    "            x(t + 1) = Ax(t) + Bu(t)\n",
    "            y(t)     = Cx(t) + Du(t)\n",
    "        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).\n",
    "\n",
    "        Args:\n",
    "            u: shape (b, l, d_in)    (See Glossary at top for definitions of b, l, d_in, n...)\n",
    "            delta: shape (b, l, d_in)\n",
    "            A: shape (d_in, n)\n",
    "            B: shape (b, l, n)\n",
    "            C: shape (b, l, n)\n",
    "            D: shape (d_in,)\n",
    "\n",
    "        Returns:\n",
    "            output: shape (b, l, d_in)\n",
    "\n",
    "        Official Implementation:\n",
    "            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86\n",
    "            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.\n",
    "\n",
    "        \"\"\"\n",
    "        (b, l, d_in) = u.shape\n",
    "        n = A.shape[1]\n",
    "\n",
    "        # Discretize continuous parameters (A, B)\n",
    "        # - A is discretized using zero-order hold (ZOH) discretization (see Section 2 Equation 4 in the Mamba paper [1])\n",
    "        # - B is discretized using a simplified Euler discretization instead of ZOH. From a discussion with authors:\n",
    "        #   \"A is the more important term and the performance doesn't change much with the simplification on B\"\n",
    "        deltaA = torch.exp(einsum(delta, A, \"b l d_in, d_in n -> b l d_in n\"))\n",
    "        deltaB_u = einsum(delta, B, u, \"b l d_in, b l n, b l d_in -> b l d_in n\")\n",
    "\n",
    "        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])\n",
    "        # Note that the below is sequential, while the official implementation does a much faster parallel scan that\n",
    "        # is additionally hardware-aware (like FlashAttention).\n",
    "        x = torch.zeros((b, d_in, n), device=deltaA.device)\n",
    "        ys = []\n",
    "        for i in range(l):\n",
    "            x = deltaA[:, i] * x + deltaB_u[:, i]\n",
    "            y = einsum(x, C[:, i, :], \"b d_in n, b n -> b d_in\")\n",
    "            ys.append(y)\n",
    "        y = torch.stack(ys, dim=1)  # shape (b, l, d_in)\n",
    "\n",
    "        y = y + u * D\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = (\n",
    "            x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps) * self.weight\n",
    "        )\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b956990",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:29.026665Z",
     "iopub.status.busy": "2025-04-23T14:19:29.026329Z",
     "iopub.status.idle": "2025-04-23T14:19:29.036131Z",
     "shell.execute_reply": "2025-04-23T14:19:29.035417Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.015837,
     "end_time": "2025-04-23T14:19:29.037215",
     "exception": false,
     "start_time": "2025-04-23T14:19:29.021378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SAMBA(nn.Module):\n",
    "    def __init__(self, ModelArgs, hidden, inp, out, embed, cheb_k):\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "        self.mam1 = Mamba(ModelArgs, hidden)\n",
    "        self.cheb_k = cheb_k\n",
    "\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))\n",
    "        self.adj = nn.Parameter(\n",
    "            torch.randn(ModelArgs.vocab_size, embed), requires_grad=True\n",
    "        )\n",
    "\n",
    "        self.embed_w = nn.Parameter(torch.randn(embed, embed), requires_grad=True)\n",
    "        self.weights_pool = nn.Parameter(torch.FloatTensor(embed, cheb_k, inp, out))\n",
    "        self.bias_pool = nn.Parameter(torch.FloatTensor(embed, out))\n",
    "        self.proj = nn.Linear(ModelArgs.vocab_size, 1)\n",
    "        self.proj_seq = nn.Linear(ModelArgs.seq_in, 1)\n",
    "\n",
    "    def gaussian_kernel_graph(self, E_A, x, gamma=1.0):\n",
    "        # Compute pairwise squared Euclidean distance\n",
    "\n",
    "        x_mean = torch.mean(x, dim=0)\n",
    "\n",
    "        x_time = torch.mm(x_mean.permute(1, 0), x_mean)\n",
    "\n",
    "        N = E_A.size(0)\n",
    "        # Expanding the dimensions to compute pairwise differences\n",
    "        E_A_expanded = E_A.unsqueeze(0).expand(N, N, -1)\n",
    "        E_A_T_expanded = E_A.unsqueeze(1).expand(N, N, -1)\n",
    "        # Pairwise squared Euclidean distances\n",
    "        distance_matrix = torch.sum((E_A_expanded - E_A_T_expanded) ** 2, dim=2)\n",
    "\n",
    "        # Apply Gaussian kernel\n",
    "        A = torch.exp(-gamma * distance_matrix)\n",
    "\n",
    "        dr = nn.Dropout(0.35)\n",
    "\n",
    "        # A=torch.tanh(torch.mm(self.adj, self.adj.transpose(0, 1))+1)\n",
    "\n",
    "        # Optional: Normalize the adjacency matrix with softmax (row-wise)\n",
    "        A = F.softmax(A, dim=1)\n",
    "\n",
    "        return dr(A)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # x_mean=torch.mean(input_ids,dim=0)\n",
    "        # ADJ=F.softmax(torch.mm(x_mean.permute(1,0),x_mean),dim=1)\n",
    "\n",
    "        # >>> Bi-mamba result <<<\n",
    "        xx = self.mam1(input_ids)\n",
    "        # m = nn.LeakyReLU(0.1)\n",
    "        # ADJ=F.softmax(F.relu(torch.mm(self.adj, self.adj.transpose(0, 1))), dim=1)\n",
    "        # dr=nn.Dropout(0.35)\n",
    "        # ADJ=dr(F.softmax(F.relu(torch.mm(torch.mm(self.adj, self.embed_w),self.adj.transpose(0, 1))),dim=1))\n",
    "\n",
    "        # >>> AGC block <<<\n",
    "        # 1) Tính adjacency graph\n",
    "        ADJ = self.gaussian_kernel_graph(self.adj, xx, gamma=self.gamma)\n",
    "        # degree = torch.sum(ADJ, dim=1)\n",
    "        # laplacian is sym or not\n",
    "        # attention = 0.5 * (attention + attention.T)\n",
    "        # degree_l = torch.diag(degree)+1e-5\n",
    "        # deg=torch.diag(1 / (degree + 1e-5))\n",
    "        # diagonal_degree_hat = torch.diag(1 / (torch.sqrt(degree) + 1e-5))\n",
    "        # attention = torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))#milan\n",
    "        # A=torch.matmul(diagonal_degree_hat,torch.matmul(attention, diagonal_degree_hat))\n",
    "        # r=torch.rand(1).cuda()\n",
    "        # d1=torch.diag(1 / (torch.pow(degree,1-r) + 1e-5))\n",
    "        # d2=torch.diag(1 / (torch.pow(degree,r) + 1e-5))\n",
    "        # L = torch.eye(input_ids.size(2)).cuda()-torch.matmul(d1,torch.matmul(ADJ,d2))\n",
    "\n",
    "        # 2) Tạo tập hỗ trợ Chebyshev polynomials\n",
    "        I = torch.eye(input_ids.size(2)).cuda()\n",
    "        # L=I-ADJ\n",
    "        # out=self.mlp(xx)\n",
    "        support_set = [I, ADJ]  # (math.sqrt(2)/2)*L,(-math.sqrt(2)/2)*L]\n",
    "        for k in range(2, self.cheb_k):\n",
    "            support_set.append(torch.matmul(2 * ADJ, support_set[-1]) - support_set[-2])\n",
    "        supports = torch.stack(support_set, dim=0)\n",
    "\n",
    "        # 3) Lấy filter weights và biases cho từng node\n",
    "        weights = torch.einsum(\n",
    "            \"nd,dkio->nkio\", self.adj, self.weights_pool\n",
    "        )  # N, cheb_k, dim_in, dim_out\n",
    "        bias = torch.matmul(self.adj, self.bias_pool)  # N, dim_out\n",
    "\n",
    "        # 4) Áp dụng graph convolution qua Chebyshev polynomials\n",
    "        x_g = torch.einsum(\n",
    "            \"knm,bmc->bknc\", supports, xx.permute(0, 2, 1)\n",
    "        )  # B, cheb_k, N, dim_in\n",
    "        x_g = x_g.permute(0, 2, 1, 3)  # B, N, cheb_k, dim_in\n",
    "        out = torch.einsum(\"bnki,nkio->bno\", x_g, weights) + bias  # B,N,D_OUT\n",
    "\n",
    "        return self.proj(out.permute(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca01f26",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:29.046443Z",
     "iopub.status.busy": "2025-04-23T14:19:29.046249Z",
     "iopub.status.idle": "2025-04-23T14:19:38.386034Z",
     "shell.execute_reply": "2025-04-23T14:19:38.385468Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 9.34598,
     "end_time": "2025-04-23T14:19:38.387418",
     "exception": false,
     "start_time": "2025-04-23T14:19:29.041438",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAMBA_GATv2\n",
    "# Tích hợp GATv2 vào SAMBA - thay thế AGC, giữ Gaussian kernel graph\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "\n",
    "class SAMBA_GATv2(nn.Module):\n",
    "    def __init__(self, model_args, hidden, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.args = model_args\n",
    "        # BI-Mamba block xử lý chuỗi thời gian\n",
    "        self.mam1 = Mamba(model_args, hidden)\n",
    "\n",
    "        # Gaussian kernel graph parameters (giữ giống SAMBA gốc)\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))\n",
    "        self.adj = nn.Parameter(\n",
    "            torch.randn(model_args.vocab_size, hidden), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # GATv2 block thay thế AGC\n",
    "        self.num_heads = num_heads\n",
    "        in_ch = 1\n",
    "        out_ch = hidden\n",
    "        N = model_args.vocab_size\n",
    "        self.register_buffer(\"edge_index\", self._build_full_edge_index(N))\n",
    "        self.gatv2 = GATv2Conv(\n",
    "            in_ch, out_ch, heads=num_heads, concat=True, dropout=0.2, share_weights=True\n",
    "        )\n",
    "\n",
    "        # Projection cuối\n",
    "        self.proj = nn.Linear(N * out_ch * num_heads, model_args.seq_out)\n",
    "\n",
    "    def _build_full_edge_index(self, N):\n",
    "        row = torch.arange(N).repeat_interleave(N)\n",
    "        col = torch.arange(N).repeat(N)\n",
    "        return torch.stack([row, col], dim=0)\n",
    "\n",
    "    def gaussian_kernel_graph(self, E_A, x, gamma=1.0):\n",
    "        # Tạo MA trận adjacency theo Gaussian kernel\n",
    "        N = E_A.size(0)\n",
    "        E_exp = E_A.unsqueeze(0).expand(N, N, -1)\n",
    "        E_Texp = E_A.unsqueeze(1).expand(N, N, -1)\n",
    "        dist2 = torch.sum((E_exp - E_Texp) ** 2, dim=2)\n",
    "        A = torch.exp(-gamma * dist2)\n",
    "        A = F.softmax(A, dim=1)\n",
    "        return F.dropout(A, p=0.35, training=self.training)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (B, L, N)\n",
    "        B, L, N = input_ids.size()\n",
    "        # 1) BI-Mamba\n",
    "        xx = self.mam1(input_ids)  # (B, L, N)\n",
    "        # 2) Tính adjacency\n",
    "        ADJ = self.gaussian_kernel_graph(self.adj, xx, gamma=self.gamma)\n",
    "        # 3) GATv2\n",
    "        x = xx.view(B * L, N, 1)  # (B*L, N, 1)\n",
    "        edge_index = self.edge_index.to(x.device)\n",
    "        outs = []\n",
    "        for i in range(x.size(0)):\n",
    "            node_feats = x[i]  # (N, 1)\n",
    "            h = self.gatv2(node_feats, edge_index)  # (N, out_ch * num_heads)\n",
    "            outs.append(h.view(-1))  # flatten (N*out)\n",
    "        out = torch.stack(outs, dim=0)  # (B*L, N*out)\n",
    "        out = out.view(B, L, -1)  # (B, L, N*out)\n",
    "        # 4) Projection\n",
    "        return self.proj(out)  # (B, L, seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c569533",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:38.398100Z",
     "iopub.status.busy": "2025-04-23T14:19:38.397298Z",
     "iopub.status.idle": "2025-04-23T14:19:38.417890Z",
     "shell.execute_reply": "2025-04-23T14:19:38.417368Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.02677,
     "end_time": "2025-04-23T14:19:38.419057",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.392287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SAMBA_GraphSAGE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Giả định Mamba class đã được định nghĩa hoặc import trước đó\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "class SAMBA_GraphSAGE(nn.Module):\n",
    "    def __init__(self, model_args, hidden, num_samples=8):\n",
    "        super().__init__()\n",
    "        self.args = model_args\n",
    "        # 1) BI-Mamba block xử lý chuỗi thời gian\n",
    "        self.mamba = Mamba(model_args, hidden)\n",
    "\n",
    "        # 2) Gaussian kernel graph parameters\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))\n",
    "        self.adj_embed = nn.Parameter(torch.randn(model_args.vocab_size, hidden))\n",
    "\n",
    "        # 3) Neighbor sampling\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "        # 4) Weights cho mean aggregator (GraphSAGE style)\n",
    "        # input feature dim = 1, output dim = hidden\n",
    "        self.weight_self = nn.Parameter(torch.randn(1, hidden))\n",
    "        self.weight_neigh = nn.Parameter(torch.randn(1, hidden))\n",
    "\n",
    "        # 5) Template full graph indices để sampling\n",
    "        N = model_args.vocab_size\n",
    "        row = torch.arange(N).repeat_interleave(N)\n",
    "        col = torch.arange(N).repeat(N)\n",
    "        self.register_buffer(\"full_edge_index\", torch.stack([row, col], dim=0))\n",
    "\n",
    "        # 6) Projection cuối: flatten N*hidden -> seq_out\n",
    "        self.proj = nn.Linear(N * hidden, model_args.seq_out)\n",
    "\n",
    "    def gaussian_kernel_graph(self, embed, gamma):\n",
    "        # Xây ma trận adjacency (N x N) bằng Gaussian RBF trên embeddings\n",
    "        N = embed.size(0)\n",
    "        E1 = embed.unsqueeze(0).expand(N, N, -1)\n",
    "        E2 = embed.unsqueeze(1).expand(N, N, -1)\n",
    "        dist2 = ((E1 - E2) ** 2).sum(dim=2)\n",
    "        A = torch.exp(-gamma * dist2)\n",
    "        A = F.softmax(A, dim=1)\n",
    "        return A\n",
    "\n",
    "    def sample_neighbors(self, A):\n",
    "        # Weighted sampling: mỗi nút sample k neighbors\n",
    "        N = A.size(0)\n",
    "        k = min(self.num_samples, N)\n",
    "        # tránh xác suất zero\\\n",
    "        probs = A + 1e-6\n",
    "        idx = torch.multinomial(probs, num_samples=k, replacement=False)  # (N, k)\n",
    "        src = torch.arange(N, device=A.device).unsqueeze(1).repeat(1, k)\n",
    "        edge_index = torch.stack([src.flatten(), idx.flatten()], dim=0)\n",
    "        return edge_index\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # input_ids: (B, L, N)\n",
    "        B, L, N = input_ids.size()\n",
    "        # 1) BI-Mamba: nạp và xử lý chuỗi\n",
    "        x_seq = self.mamba(input_ids)  # (B, L, N)\n",
    "\n",
    "        # 2) Tính adjacency từ embedding\n",
    "        A = self.gaussian_kernel_graph(self.adj_embed, self.gamma)  # (N, N)\n",
    "\n",
    "        # 3) Lấy sparse edge_index qua sampling\n",
    "        edge_index = self.sample_neighbors(A)  # (2, N*k)\n",
    "\n",
    "        # 4) Xây features nodes và aggregator\n",
    "        # flatten batch/time: (B*L, N, 1)\n",
    "        x_flat = x_seq.view(B * L, N, 1)\n",
    "        # gather neighbor features\n",
    "        src, dst = edge_index\n",
    "        k = dst.numel() // N\n",
    "        neigh_feats = x_flat[:, dst].view(B * L, N, k, 1)  # (BL, N, k, 1)\n",
    "        mean_neigh = neigh_feats.mean(dim=2)  # (BL, N, 1)\n",
    "\n",
    "        # 5) GraphSAGE update via einsum\n",
    "        # h_self: (BL, N, hidden)\n",
    "        h_self = torch.einsum(\"bni,ih->bnh\", x_flat, self.weight_self)\n",
    "        # h_neigh: (BL, N, hidden)\n",
    "        h_neigh = torch.einsum(\"bni,ih->bnh\", mean_neigh, self.weight_neigh)\n",
    "        h = F.relu(h_self + h_neigh)  # (BL, N, hidden)\n",
    "\n",
    "        # 6) Flatten và project\n",
    "        h_flat = h.view(B, L, N * h.size(-1))  # (B, L, N*hidden)\n",
    "        y = self.proj(h_flat)  # (B, L, seq_out)\n",
    "        # 7) Chỉ lấy giá trị cuối thời gian\n",
    "        return y[:, -1:, :]  # (B, 1, seq_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "305ff73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:38.428346Z",
     "iopub.status.busy": "2025-04-23T14:19:38.428156Z",
     "iopub.status.idle": "2025-04-23T14:19:38.435947Z",
     "shell.execute_reply": "2025-04-23T14:19:38.435309Z"
    },
    "papermill": {
     "duration": 0.013613,
     "end_time": "2025-04-23T14:19:38.436922",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.423309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 1) Định nghĩa lớp GraphSAGE\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats: int, out_feats: int):\n",
    "        super().__init__()\n",
    "        # chúng ta sẽ concat h_v và mean_{u in N(v)} h_u\n",
    "        self.fc = nn.Linear(in_feats * 2, out_feats)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, A: torch.Tensor) -> torch.Tensor:\n",
    "        # h: [B, N, F_in], A: [N, N] row‐normalized adjacency\n",
    "        neigh = torch.einsum(\"nm,bmf->bnf\", A, h)  # sum neighbors\n",
    "        deg = A.sum(dim=1, keepdim=True)  # degree mỗi node\n",
    "        neigh = neigh / (deg + 1e-6)  # mean‐aggregator\n",
    "        h_cat = torch.cat([h, neigh], dim=-1)  # [B, N, 2*F_in]\n",
    "        return F.relu(self.fc(h_cat))  # [B, N, F_out]\n",
    "\n",
    "\n",
    "# 2) Cập nhật class SAMBA\n",
    "class SAMBA_GraphSAGE(nn.Module):\n",
    "    def __init__(self, ModelArgs, hidden, inp, out, embed):\n",
    "        super().__init__()\n",
    "        self.args = ModelArgs\n",
    "\n",
    "        # giữ nguyên BI-Mamba block\n",
    "        self.mam1 = Mamba(ModelArgs, hidden)\n",
    "\n",
    "        # tham số adaptive adjacency\n",
    "        self.gamma = nn.Parameter(torch.tensor(1.0))\n",
    "        self.adj = nn.Parameter(\n",
    "            torch.randn(ModelArgs.vocab_size, embed), requires_grad=True\n",
    "        )\n",
    "\n",
    "        # thay Chebyshev‐pooling bằng GraphSAGE\n",
    "        # inp  = seq_in (T), out = seq_out (dự báo chiều)\n",
    "        self.graphsage = GraphSAGE(in_feats=inp, out_feats=out)\n",
    "\n",
    "        # projection giữ nguyên nếu muốn map từ node‐features về giá trị\n",
    "        self.proj = nn.Linear(ModelArgs.vocab_size, 1)\n",
    "        self.proj_seq = nn.Linear(ModelArgs.seq_in, 1)\n",
    "\n",
    "    def gaussian_kernel_graph(self, E_A, x, gamma=1.0):\n",
    "        # ... (giữ nguyên như trước) ...\n",
    "        distance_matrix = torch.sum((E_A.unsqueeze(0) - E_A.unsqueeze(1)) ** 2, dim=2)\n",
    "        A = torch.exp(-gamma * distance_matrix)\n",
    "        A = F.softmax(A, dim=1)\n",
    "        return nn.Dropout(0.35)(A)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        # 1) Bi-Mamba để trích feature theo thời gian\n",
    "        xx = self.mam1(input_ids)  # [B, T, N]\n",
    "\n",
    "        # 2) Xây dựng adjacency động\n",
    "        ADJ = self.gaussian_kernel_graph(self.adj, xx, gamma=self.gamma)  # [N, N]\n",
    "\n",
    "        # 3) Chuẩn bị feature per node\n",
    "        h = xx.permute(0, 2, 1)  # [B, N, T]\n",
    "\n",
    "        # 4) GraphSAGE layer\n",
    "        h2 = self.graphsage(h, ADJ)  # [B, N, out]\n",
    "\n",
    "        # 5) Dự báo: quay lại [B, out, N] rồi dùng proj để map\n",
    "        return self.proj(h2.permute(0, 2, 1))  # [B, 1, N] hoặc [B, out, 1] tuỳ cấu hình"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6b6cd",
   "metadata": {
    "papermill": {
     "duration": 0.003969,
     "end_time": "2025-04-23T14:19:38.445220",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.441251",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRAINER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d12f319",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:38.454729Z",
     "iopub.status.busy": "2025-04-23T14:19:38.454285Z",
     "iopub.status.idle": "2025-04-23T14:19:38.486754Z",
     "shell.execute_reply": "2025-04-23T14:19:38.486072Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.038732,
     "end_time": "2025-04-23T14:19:38.488086",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.449354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINER\n",
    "def get_logger(root, name=None, debug=True):\n",
    "    # when debug is true, show DEBUG and INFO in screen\n",
    "    # when debug is false, show DEBUG in file and info in both screen&file\n",
    "    # INFO will always be in screen\n",
    "    # create a logger\n",
    "    logger = logging.getLogger(name)\n",
    "    # critical > error > warning > info > debug > notset\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # define the formate\n",
    "    formatter = logging.Formatter(\"%(asctime)s: %(message)s\", \"%Y-%m-%d %H:%M\")\n",
    "    # create another handler for output log to console\n",
    "    console_handler = logging.StreamHandler()\n",
    "    if debug:\n",
    "        console_handler.setLevel(logging.DEBUG)\n",
    "    else:\n",
    "        console_handler.setLevel(logging.INFO)\n",
    "        # create a handler for write log to file\n",
    "        logfile = os.path.join(root, \"run.log\")\n",
    "        print(\"Creat Log File in: \", logfile)\n",
    "        file_handler = logging.FileHandler(logfile, mode=\"w\")\n",
    "        file_handler.setLevel(logging.DEBUG)\n",
    "        file_handler.setFormatter(formatter)\n",
    "    console_handler.setFormatter(formatter)\n",
    "    # add Handler to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    if not debug:\n",
    "        logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "\n",
    "def init_seed(seed):\n",
    "    \"\"\"\n",
    "    Disable cudnn to maximize reproducibility\n",
    "    \"\"\"\n",
    "    torch.cuda.cudnn_enabled = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "\n",
    "def init_device(opt):\n",
    "    if torch.cuda.is_available():\n",
    "        opt.cuda = True\n",
    "        torch.cuda.set_device(int(opt.device[5]))\n",
    "    else:\n",
    "        opt.cuda = False\n",
    "        opt.device = \"cpu\"\n",
    "    return opt\n",
    "\n",
    "\n",
    "def init_optim(model, opt):\n",
    "    \"\"\"\n",
    "    Initialize optimizer\n",
    "    \"\"\"\n",
    "    return torch.optim.Adam(params=model.parameters(), lr=opt.lr_init)\n",
    "\n",
    "\n",
    "def init_lr_scheduler(optim, opt):\n",
    "    \"\"\"\n",
    "    Initialize the learning rate scheduler\n",
    "    \"\"\"\n",
    "    # return torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=opt.lr_scheduler_rate,step_size=opt.lr_scheduler_step)\n",
    "    return torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer=optim, milestones=opt.lr_decay_steps, gamma=opt.lr_scheduler_rate\n",
    "    )\n",
    "\n",
    "\n",
    "def print_model_parameters(model, only_num=True):\n",
    "    print(\"*****************Model Parameter*****************\")\n",
    "    if not only_num:\n",
    "        for name, param in model.named_parameters():\n",
    "            print(name, param.shape, param.requires_grad)\n",
    "    total_num = sum([param.nelement() for param in model.parameters()])\n",
    "    print(\"Total params num: {}\".format(total_num))\n",
    "    print(\"*****************Finish Parameter****************\")\n",
    "\n",
    "\n",
    "def get_memory_usage(device):\n",
    "    allocated_memory = torch.cuda.memory_allocated(device) / (1024 * 1024.0)\n",
    "    cached_memory = torch.cuda.memory_cached(device) / (1024 * 1024.0)\n",
    "    return allocated_memory, cached_memory\n",
    "    # print('Allocated Memory: {:.2f} MB, Cached Memory: {:.2f} MB'.format(allocated_memory, cached_memory))\n",
    "\n",
    "\n",
    "def MAE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true - pred))\n",
    "\n",
    "\n",
    "def MSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean((pred - true) ** 2)\n",
    "\n",
    "\n",
    "def RMSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.mean((pred - true) ** 2))\n",
    "\n",
    "\n",
    "def RRSE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.sqrt(torch.sum((pred - true) ** 2)) / torch.sqrt(\n",
    "        torch.sum((pred - true.mean()) ** 2)\n",
    "    )\n",
    "\n",
    "\n",
    "def MAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(torch.div((true - pred), true)))\n",
    "\n",
    "\n",
    "def PNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    indicator = torch.gt(pred - true, 0).float()\n",
    "    return indicator.mean()\n",
    "\n",
    "\n",
    "def oPNBI_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    bias = (true + pred) / (2 * true)\n",
    "    return bias.mean()\n",
    "\n",
    "\n",
    "def MARE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.div(torch.sum(torch.abs((true - pred))), torch.sum(true))\n",
    "\n",
    "\n",
    "def SMAPE_torch(pred, true, mask_value=None):\n",
    "    if mask_value != None:\n",
    "        mask = torch.gt(true, mask_value)\n",
    "        pred = torch.masked_select(pred, mask)\n",
    "        true = torch.masked_select(true, mask)\n",
    "    return torch.mean(torch.abs(true - pred) / (torch.abs(true) + torch.abs(pred)))\n",
    "\n",
    "\n",
    "def All_Metrics(pred, true, mask1, mask2):\n",
    "    # mask1 filter the very small value, mask2 filter the value lower than a defined threshold\n",
    "    assert type(pred) == type(true)\n",
    "    # if type(pred) == np.ndarray:\n",
    "    #    mae  = MAE_np(pred, true, mask1)\n",
    "    #    rmse = RMSE_np(pred, true, mask1)\n",
    "    #    mape = MAPE_np(pred, true, mask2)\n",
    "    #    rrse = RRSE_np(pred, true, mask1)\n",
    "\n",
    "    # corr = CORR_np(pred, true, mask1)\n",
    "    # pnbi = PNBI_np(pred, true, mask1)\n",
    "    # opnbi = oPNBI_np(pred, true, mask2)\n",
    "    if type(pred) == torch.Tensor:\n",
    "        mae = MAE_torch(pred, true, mask1)\n",
    "        rmse = RMSE_torch(pred, true, mask1)\n",
    "        rrse = RRSE_torch(pred, true, mask1)\n",
    "\n",
    "        # pnbi = PNBI_torch(pred, true, mask1)\n",
    "        # opnbi = oPNBI_torch(pred, true, mask2)\n",
    "    else:\n",
    "        raise TypeError\n",
    "    return mae, rmse, rrse\n",
    "\n",
    "\n",
    "def SIGIR_Metrics(pred, true, mask1, mask2):\n",
    "    rrse = RRSE_torch(pred, true, mask1)\n",
    "    corr = CORR_torch(pred, true, 0)\n",
    "    return rrse, corr\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, epoch=None):\n",
    "    if model_dir is None:\n",
    "        return\n",
    "    if not os.path.exists(model_dir):\n",
    "        os.makedirs(model_dir)\n",
    "    epoch = str(epoch) if epoch else \"\"\n",
    "    file_name = os.path.join(model_dir, epoch + \"_stemgnn.pt\")\n",
    "    with open(file_name, \"wb\") as f:\n",
    "        torch.save(model, f)\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        args,\n",
    "        lr_scheduler=None,\n",
    "    ):\n",
    "        super(Trainer, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss = loss\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        # self.scaler = scaler\n",
    "        self.args = args\n",
    "        self.lr_scheduler = lr_scheduler\n",
    "        self.train_per_epoch = len(train_loader)\n",
    "        if val_loader != None:\n",
    "            self.val_per_epoch = len(val_loader)\n",
    "        self.best_path = os.path.join(self.args.get(\"log_dir\"), \"best_model.pth\")\n",
    "        self.loss_figure_path = os.path.join(self.args.get(\"log_dir\"), \"loss.png\")\n",
    "        # log\n",
    "        if os.path.isdir(args.get(\"log_dir\")) == False and not args.get(\"debug\"):\n",
    "            os.makedirs(args.get(\"log_dir\"), exist_ok=True)\n",
    "        self.logger = get_logger(\n",
    "            args.get(\"log_dir\"), name=args.get(\"model\"), debug=args.get(\"debug\")\n",
    "        )\n",
    "        self.logger.info(\"Experiment log path in: {}\".format(args.get(\"log_dir\")))\n",
    "        # if not args.debug:\n",
    "        # self.logger.info(\"Argument: %r\", args)\n",
    "        # for arg, value in sorted(vars(args).items()):\n",
    "        #     self.logger.info(\"Argument %s: %r\", arg, value)\n",
    "\n",
    "    def val_epoch(self, epoch, val_dataloader):\n",
    "        self.model.eval()\n",
    "        total_val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(val_dataloader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = self.model(data)\n",
    "                # if self.args.get('real_value'):\n",
    "                # label = self.scaler.inverse_transform(label)\n",
    "                loss = self.loss(output, label)\n",
    "                # a whole batch of Metr_LA is filtered\n",
    "                if not torch.isnan(loss):\n",
    "                    total_val_loss += loss.item()\n",
    "        val_loss = total_val_loss / len(val_dataloader)\n",
    "        self.logger.info(\n",
    "            \"**********Val Epoch {}: average Loss: {:.6f}\".format(epoch, val_loss)\n",
    "        )\n",
    "        return val_loss\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        loss_values = []\n",
    "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
    "            data = data\n",
    "            label = target  # (..., 1)\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # data and target shape: B, T, N, F; output shape: B, T, N, F\n",
    "            output = self.model(data)\n",
    "            # if self.args.get('real_value'):\n",
    "            #   label = self.scaler.inverse_transform(label)\n",
    "\n",
    "            loss = self.loss(output, label)\n",
    "            loss = self.loss(output, label)\n",
    "            loss.backward()\n",
    "\n",
    "            # add max grad clipping\n",
    "            if self.args.get(\"grad_norm\"):\n",
    "                torch.nn.utils.clip_grad_norm_(\n",
    "                    self.model.parameters(), self.args.get(\"max_grad_norm\")\n",
    "                )\n",
    "            self.optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            loss_values.append(loss.item())\n",
    "\n",
    "            # log information\n",
    "            if batch_idx % self.args.get(\"log_step\") == 0:\n",
    "                self.logger.info(\n",
    "                    \"Train Epoch {}: {}/{} Loss: {:.6f}\".format(\n",
    "                        epoch, batch_idx, self.train_per_epoch, loss.item()\n",
    "                    )\n",
    "                )\n",
    "        train_epoch_loss = total_loss / self.train_per_epoch\n",
    "        self.logger.info(\n",
    "            \"**********Train Epoch {}: averaged Loss: {:.6f}\".format(\n",
    "                epoch, train_epoch_loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # learning rate decay\n",
    "        if self.args.get(\"lr_decay\"):\n",
    "            self.lr_scheduler.step()\n",
    "        return train_epoch_loss\n",
    "\n",
    "    def train(self):\n",
    "        best_model = None\n",
    "        best_loss = float(\"inf\")\n",
    "        not_improved_count = 0\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        start_time = time.time()\n",
    "        for epoch in range(1, self.args.get(\"epochs\") + 1):\n",
    "            # epoch_time = time.time()\n",
    "            train_epoch_loss = self.train_epoch(epoch)\n",
    "            # print(time.time()-epoch_time)\n",
    "            # exit()\n",
    "            if self.val_loader == None:\n",
    "                val_dataloader = self.test_loader\n",
    "            else:\n",
    "                val_dataloader = self.val_loader\n",
    "            val_epoch_loss = self.val_epoch(epoch, val_dataloader)\n",
    "\n",
    "            # print('LR:', self.optimizer.param_groups[0]['lr'])\n",
    "            train_loss_list.append(train_epoch_loss)\n",
    "            val_loss_list.append(val_epoch_loss)\n",
    "            if train_epoch_loss > 1e6:\n",
    "                self.logger.warning(\"Gradient explosion detected. Ending...\")\n",
    "                break\n",
    "            # if self.val_loader == None:\n",
    "            # val_epoch_loss = train_epoch_loss\n",
    "            if val_epoch_loss < best_loss:\n",
    "                best_loss = val_epoch_loss\n",
    "                not_improved_count = 0\n",
    "                best_state = True\n",
    "            else:\n",
    "                not_improved_count += 1\n",
    "                best_state = False\n",
    "            # early stop\n",
    "            if self.args.get(\"early_stop\"):\n",
    "                if not_improved_count == self.args.get(\"early_stop_patience\"):\n",
    "                    self.logger.info(\n",
    "                        \"Validation performance didn't improve for {} epochs. \"\n",
    "                        \"Training stops.\".format(self.args.get(\"early_stop_patience\"))\n",
    "                    )\n",
    "                    break\n",
    "            # save the best state\n",
    "            if best_state == True:\n",
    "                self.logger.info(\n",
    "                    \"*********************************Current best model saved!\"\n",
    "                )\n",
    "                best_model = copy.deepcopy(self.model.state_dict())\n",
    "\n",
    "        training_time = time.time() - start_time\n",
    "        self.logger.info(\n",
    "            \"Total training time: {:.4f}min, best loss: {:.6f}\".format(\n",
    "                (training_time / 60), best_loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with open(\"milan_sms_mamaba.txt\", \"a\") as f:\n",
    "            f.write(str(epoch))\n",
    "            f.write(\"\\n\")\n",
    "            f.write(str(training_time / 60))\n",
    "            f.write(\"\\n\")\n",
    "\n",
    "        # save the best model to file\n",
    "        if not self.args.get(\"debug\"):\n",
    "            torch.save(best_model, self.best_path)\n",
    "            self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "        # test\n",
    "        self.model.load_state_dict(best_model)\n",
    "        # self.val_epoch(self.args.epochs, self.test_loader)\n",
    "        y1, y2 = self.test(self.model, self.args, self.test_loader, self.logger)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        state = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"config\": self.args,\n",
    "        }\n",
    "        torch.save(state, self.best_path)\n",
    "        self.logger.info(\"Saving current best model to \" + self.best_path)\n",
    "\n",
    "    @staticmethod\n",
    "    def test(model, args, data_loader, logger, path=None):\n",
    "        if path != None:\n",
    "            check_point = torch.load(path)\n",
    "            state_dict = check_point[\"state_dict\"]\n",
    "            args = check_point[\"config\"]\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.to(args.get(\"device\"))\n",
    "        model.eval()\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (data, target) in enumerate(data_loader):\n",
    "                data = data\n",
    "                label = target\n",
    "                output = model(data)\n",
    "\n",
    "                y_true.append(label)\n",
    "                y_pred.append(output)\n",
    "\n",
    "                # print(model.forward(data, [], teacher_forcing_ratio=0))\n",
    "        # y_true = scaler.inverse_transform(torch.cat(y_true, dim=0))\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_true = torch.cat(y_true, dim=0)\n",
    "        # if not args.get('real_value'):\n",
    "        #    y_pred = torch.cat(y_pred, dim=0)\n",
    "        # else:\n",
    "        # y_pred = scaler.inverse_transform(torch.cat(y_pred, dim=0))\n",
    "        # np.save('./{}_true.npy'.format(args.get('dataset')), y_true.cpu().numpy())\n",
    "        # np.save('./{}_pred.npy'.format(args.get('dataset')), y_pred.cpu().numpy())\n",
    "        # for t in range(y_true.shape[1]):\n",
    "        #    mae, rmse, mape, _ = All_Metrics(y_pred[:, t, ...], y_true[:, t, ...],\n",
    "        #                                        args.get('mae_thresh'), args.get('mape_thresh'))\n",
    "        #    logger.info(\"Horizon {:02d}, MAE: {:.2f}, RMSE: {:.2f}, MAPE: {:.4f}%\".format(\n",
    "        #        t + 1, mae, rmse, mape*100))\n",
    "        mae, rmse, _ = All_Metrics(\n",
    "            y_pred, y_true, args.get(\"mae_thresh\"), args.get(\"mape_thresh\")\n",
    "        )\n",
    "        logger.info(\"Average Horizon, MAE: {:.4f}, MSE: {:.4f}\".format(mae, rmse))\n",
    "        return y_pred, y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_sampling_threshold(global_step, k):\n",
    "        \"\"\"\n",
    "        Computes the sampling probability for scheduled sampling using inverse sigmoid.\n",
    "        :param global_step:\n",
    "        :param k:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return k / (k + math.exp(global_step / k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da225567",
   "metadata": {
    "papermill": {
     "duration": 0.003919,
     "end_time": "2025-04-23T14:19:38.496538",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.492619",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#  Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96516ad1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:38.505673Z",
     "iopub.status.busy": "2025-04-23T14:19:38.505337Z",
     "iopub.status.idle": "2025-04-23T14:19:39.040873Z",
     "shell.execute_reply": "2025-04-23T14:19:39.040184Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.541921,
     "end_time": "2025-04-23T14:19:39.042479",
     "exception": false,
     "start_time": "2025-04-23T14:19:38.500558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py:73: RuntimeWarning: invalid value encountered in greater\n",
      "  return op(a, b)\n",
      "/tmp/ipykernel_19/847913275.py:82: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "  X, Y = TensorFloat(X), TensorFloat(Y)\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADER\n",
    "X = pd.read_csv(\n",
    "    \"/kaggle/input/stock-market-prediction/combined_dataframe_IXIC.csv\",\n",
    "    index_col=\"Date\",\n",
    "    parse_dates=True,\n",
    ")\n",
    "# basic preprocessing: get the name, the classification\n",
    "# Save the target variable as a column in dataframe for easier dropna()\n",
    "name = X[\"Name\"].iloc[0]\n",
    "X.drop(columns=[\"Name\"], inplace=True)\n",
    "cols = X.columns\n",
    "X[\"Target\"] = (X[\"Price\"].pct_change().shift(-1) > 0).astype(int)\n",
    "X.dropna(inplace=True)\n",
    "# Fit the standard scaler using the training dataset\n",
    "\n",
    "\n",
    "class MinMaxNorm01(object):\n",
    "    \"\"\"scale data to range [0, 1]\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, x):\n",
    "        self.min = x.min()\n",
    "        self.max = x.max()\n",
    "        # print('Min:{}, Max:{}'.format(self.min, self.max))\n",
    "\n",
    "    def transform(self, x):\n",
    "        x = 1.0 * (x - self.min) / (self.max - self.min)\n",
    "        return x\n",
    "\n",
    "    def fit_transform(self, x):\n",
    "        self.fit(x)\n",
    "        return self.transform(x)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        x = x * (self.max - self.min) + self.min\n",
    "        return x\n",
    "\n",
    "\n",
    "a = X.to_numpy()\n",
    "\n",
    "# data1=train_data.to_numpy()\n",
    "\n",
    "mmn = MinMaxNorm01()\n",
    "\n",
    "data = a\n",
    "\n",
    "\n",
    "dataset = mmn.fit_transform(data)\n",
    "\n",
    "window = 5\n",
    "predict = 1\n",
    "\n",
    "ran = data.shape[0]\n",
    "i = 0\n",
    "X = []\n",
    "Y = []\n",
    "while i + window < ran:\n",
    "    X.append(torch.Tensor(dataset[i : i + window, 1:]))\n",
    "    Y.append(torch.Tensor(dataset[i + window : i + window + predict, 0]))\n",
    "    i += 1\n",
    "\n",
    "XX = torch.stack(X, dim=0)\n",
    "YY = torch.stack(Y, dim=0)\n",
    "YY = YY[:, :, None]\n",
    "\n",
    "\n",
    "test_len = int(0.15 * XX.shape[0])\n",
    "val_len = int(0.05 * XX.shape[0])\n",
    "train_len = XX.shape[0] - test_len - val_len\n",
    "\n",
    "\n",
    "X_test = torch.Tensor.float(XX[:test_len, :, :]).cuda()\n",
    "Y_test = torch.Tensor.float(YY[:test_len, :, :]).cuda()\n",
    "\n",
    "X_train = torch.Tensor.float(XX[test_len : test_len + train_len, :, :]).cuda()\n",
    "Y_train = torch.Tensor.float(YY[test_len : test_len + train_len, :, :]).cuda()\n",
    "\n",
    "X_val = torch.Tensor.float(XX[-val_len:, :, :]).cuda()\n",
    "Y_val = torch.Tensor.float(YY[-val_len:, :, :]).cuda()\n",
    "\n",
    "\n",
    "def data_loader(X, Y, batch_size, shuffle=True, drop_last=True):\n",
    "    cuda = True if torch.cuda.is_available() else False\n",
    "    TensorFloat = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "    X, Y = TensorFloat(X), TensorFloat(Y)\n",
    "    data = torch.utils.data.TensorDataset(X, Y)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        data, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_loader = data_loader(X_train, Y_train, 64, shuffle=False, drop_last=False)\n",
    "val_loader = data_loader(X_val, Y_val, 64, shuffle=False, drop_last=False)\n",
    "test_loader = data_loader(X_test, Y_test, 64, shuffle=False, drop_last=False)\n",
    "\n",
    "\n",
    "def masked_mae_loss(scaler, mask_value):\n",
    "    def loss(preds, labels):\n",
    "        if scaler:\n",
    "            preds = scaler.inverse_transform(preds)\n",
    "            labels = scaler.inverse_transform(labels)\n",
    "        mae = MAE_torch(pred=preds, true=labels, mask_value=mask_value)\n",
    "        return mae\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c5612",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:19:39.061199Z",
     "iopub.status.busy": "2025-04-23T14:19:39.060868Z",
     "iopub.status.idle": "2025-04-23T14:33:21.063448Z",
     "shell.execute_reply": "2025-04-23T14:33:21.062873Z"
    },
    "papermill": {
     "duration": 822.009751,
     "end_time": "2025-04-23T14:33:21.064625",
     "exception": false,
     "start_time": "2025-04-23T14:19:39.054874",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:19: Experiment log path in: ./\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "graphsage.fc.weight torch.Size([1, 10]) True\n",
      "graphsage.fc.bias torch.Size([1]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240663\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:19: Train Epoch 1: 0/37 Loss: 0.203863\n",
      "2025-04-23 14:19: Train Epoch 1: 20/37 Loss: 0.081352\n",
      "2025-04-23 14:19: **********Train Epoch 1: averaged Loss: 0.111571\n",
      "2025-04-23 14:19: **********Val Epoch 1: average Loss: 0.104022\n",
      "2025-04-23 14:19: *********************************Current best model saved!\n",
      "2025-04-23 14:19: Train Epoch 2: 0/37 Loss: 0.701049\n",
      "2025-04-23 14:19: Train Epoch 2: 20/37 Loss: 0.112110\n",
      "2025-04-23 14:19: **********Train Epoch 2: averaged Loss: 0.206401\n",
      "2025-04-23 14:19: **********Val Epoch 2: average Loss: 0.112225\n",
      "2025-04-23 14:19: Train Epoch 3: 0/37 Loss: 0.751231\n",
      "2025-04-23 14:19: Train Epoch 3: 20/37 Loss: 0.180462\n",
      "2025-04-23 14:19: **********Train Epoch 3: averaged Loss: 0.314167\n",
      "2025-04-23 14:19: **********Val Epoch 3: average Loss: 0.189700\n",
      "2025-04-23 14:19: Train Epoch 4: 0/37 Loss: 0.475044\n",
      "2025-04-23 14:19: Train Epoch 4: 20/37 Loss: 0.031135\n",
      "2025-04-23 14:19: **********Train Epoch 4: averaged Loss: 0.287654\n",
      "2025-04-23 14:19: **********Val Epoch 4: average Loss: 0.317079\n",
      "2025-04-23 14:19: Train Epoch 5: 0/37 Loss: 0.296887\n",
      "2025-04-23 14:19: Train Epoch 5: 20/37 Loss: 0.103931\n",
      "2025-04-23 14:19: **********Train Epoch 5: averaged Loss: 0.222656\n",
      "2025-04-23 14:19: **********Val Epoch 5: average Loss: 0.296691\n",
      "2025-04-23 14:19: Train Epoch 6: 0/37 Loss: 0.363348\n",
      "2025-04-23 14:19: Train Epoch 6: 20/37 Loss: 0.057322\n",
      "2025-04-23 14:19: **********Train Epoch 6: averaged Loss: 0.206517\n",
      "2025-04-23 14:19: **********Val Epoch 6: average Loss: 0.175968\n",
      "2025-04-23 14:19: Train Epoch 7: 0/37 Loss: 0.245826\n",
      "2025-04-23 14:19: Train Epoch 7: 20/37 Loss: 0.046291\n",
      "2025-04-23 14:19: **********Train Epoch 7: averaged Loss: 0.144362\n",
      "2025-04-23 14:19: **********Val Epoch 7: average Loss: 0.032356\n",
      "2025-04-23 14:19: *********************************Current best model saved!\n",
      "2025-04-23 14:19: Train Epoch 8: 0/37 Loss: 0.364619\n",
      "2025-04-23 14:19: Train Epoch 8: 20/37 Loss: 0.013433\n",
      "2025-04-23 14:19: **********Train Epoch 8: averaged Loss: 0.127621\n",
      "2025-04-23 14:19: **********Val Epoch 8: average Loss: 0.093363\n",
      "2025-04-23 14:19: Train Epoch 9: 0/37 Loss: 0.224153\n",
      "2025-04-23 14:19: Train Epoch 9: 20/37 Loss: 0.021848\n",
      "2025-04-23 14:19: **********Train Epoch 9: averaged Loss: 0.082066\n",
      "2025-04-23 14:19: **********Val Epoch 9: average Loss: 0.187622\n",
      "2025-04-23 14:19: Train Epoch 10: 0/37 Loss: 0.156595\n",
      "2025-04-23 14:19: Train Epoch 10: 20/37 Loss: 0.033112\n",
      "2025-04-23 14:19: **********Train Epoch 10: averaged Loss: 0.091331\n",
      "2025-04-23 14:19: **********Val Epoch 10: average Loss: 0.064768\n",
      "2025-04-23 14:19: Train Epoch 11: 0/37 Loss: 0.025246\n",
      "2025-04-23 14:19: Train Epoch 11: 20/37 Loss: 0.006745\n",
      "2025-04-23 14:19: **********Train Epoch 11: averaged Loss: 0.074364\n",
      "2025-04-23 14:19: **********Val Epoch 11: average Loss: 0.115019\n",
      "2025-04-23 14:19: Train Epoch 12: 0/37 Loss: 0.074829\n",
      "2025-04-23 14:20: Train Epoch 12: 20/37 Loss: 0.029410\n",
      "2025-04-23 14:20: **********Train Epoch 12: averaged Loss: 0.074897\n",
      "2025-04-23 14:20: **********Val Epoch 12: average Loss: 0.062722\n",
      "2025-04-23 14:20: Train Epoch 13: 0/37 Loss: 0.199782\n",
      "2025-04-23 14:20: Train Epoch 13: 20/37 Loss: 0.118989\n",
      "2025-04-23 14:20: **********Train Epoch 13: averaged Loss: 0.106831\n",
      "2025-04-23 14:20: **********Val Epoch 13: average Loss: 0.058593\n",
      "2025-04-23 14:20: Train Epoch 14: 0/37 Loss: 0.163029\n",
      "2025-04-23 14:20: Train Epoch 14: 20/37 Loss: 0.032287\n",
      "2025-04-23 14:20: **********Train Epoch 14: averaged Loss: 0.053730\n",
      "2025-04-23 14:20: **********Val Epoch 14: average Loss: 0.126901\n",
      "2025-04-23 14:20: Train Epoch 15: 0/37 Loss: 0.021982\n",
      "2025-04-23 14:20: Train Epoch 15: 20/37 Loss: 0.058677\n",
      "2025-04-23 14:20: **********Train Epoch 15: averaged Loss: 0.043523\n",
      "2025-04-23 14:20: **********Val Epoch 15: average Loss: 0.129450\n",
      "2025-04-23 14:20: Train Epoch 16: 0/37 Loss: 0.095458\n",
      "2025-04-23 14:20: Train Epoch 16: 20/37 Loss: 0.009615\n",
      "2025-04-23 14:20: **********Train Epoch 16: averaged Loss: 0.050722\n",
      "2025-04-23 14:20: **********Val Epoch 16: average Loss: 0.055246\n",
      "2025-04-23 14:20: Train Epoch 17: 0/37 Loss: 0.060542\n",
      "2025-04-23 14:20: Train Epoch 17: 20/37 Loss: 0.022752\n",
      "2025-04-23 14:20: **********Train Epoch 17: averaged Loss: 0.076028\n",
      "2025-04-23 14:20: **********Val Epoch 17: average Loss: 0.051248\n",
      "2025-04-23 14:20: Train Epoch 18: 0/37 Loss: 0.133484\n",
      "2025-04-23 14:20: Train Epoch 18: 20/37 Loss: 0.034925\n",
      "2025-04-23 14:20: **********Train Epoch 18: averaged Loss: 0.053025\n",
      "2025-04-23 14:20: **********Val Epoch 18: average Loss: 0.074819\n",
      "2025-04-23 14:20: Train Epoch 19: 0/37 Loss: 0.089939\n",
      "2025-04-23 14:20: Train Epoch 19: 20/37 Loss: 0.020733\n",
      "2025-04-23 14:20: **********Train Epoch 19: averaged Loss: 0.061857\n",
      "2025-04-23 14:20: **********Val Epoch 19: average Loss: 0.100134\n",
      "2025-04-23 14:20: Train Epoch 20: 0/37 Loss: 0.058278\n",
      "2025-04-23 14:20: Train Epoch 20: 20/37 Loss: 0.023834\n",
      "2025-04-23 14:20: **********Train Epoch 20: averaged Loss: 0.057794\n",
      "2025-04-23 14:20: **********Val Epoch 20: average Loss: 0.057583\n",
      "2025-04-23 14:20: Train Epoch 21: 0/37 Loss: 0.111721\n",
      "2025-04-23 14:20: Train Epoch 21: 20/37 Loss: 0.037511\n",
      "2025-04-23 14:20: **********Train Epoch 21: averaged Loss: 0.049862\n",
      "2025-04-23 14:20: **********Val Epoch 21: average Loss: 0.098480\n",
      "2025-04-23 14:20: Train Epoch 22: 0/37 Loss: 0.219292\n",
      "2025-04-23 14:20: Train Epoch 22: 20/37 Loss: 0.052243\n",
      "2025-04-23 14:20: **********Train Epoch 22: averaged Loss: 0.064755\n",
      "2025-04-23 14:20: **********Val Epoch 22: average Loss: 0.100167\n",
      "2025-04-23 14:20: Train Epoch 23: 0/37 Loss: 0.256626\n",
      "2025-04-23 14:20: Train Epoch 23: 20/37 Loss: 0.042059\n",
      "2025-04-23 14:20: **********Train Epoch 23: averaged Loss: 0.071084\n",
      "2025-04-23 14:20: **********Val Epoch 23: average Loss: 0.028291\n",
      "2025-04-23 14:20: *********************************Current best model saved!\n",
      "2025-04-23 14:20: Train Epoch 24: 0/37 Loss: 0.201511\n",
      "2025-04-23 14:20: Train Epoch 24: 20/37 Loss: 0.046358\n",
      "2025-04-23 14:20: **********Train Epoch 24: averaged Loss: 0.071766\n",
      "2025-04-23 14:20: **********Val Epoch 24: average Loss: 0.035920\n",
      "2025-04-23 14:20: Train Epoch 25: 0/37 Loss: 0.131019\n",
      "2025-04-23 14:20: Train Epoch 25: 20/37 Loss: 0.050688\n",
      "2025-04-23 14:20: **********Train Epoch 25: averaged Loss: 0.058417\n",
      "2025-04-23 14:20: **********Val Epoch 25: average Loss: 0.173929\n",
      "2025-04-23 14:20: Train Epoch 26: 0/37 Loss: 0.039451\n",
      "2025-04-23 14:20: Train Epoch 26: 20/37 Loss: 0.025339\n",
      "2025-04-23 14:20: **********Train Epoch 26: averaged Loss: 0.031985\n",
      "2025-04-23 14:20: **********Val Epoch 26: average Loss: 0.146417\n",
      "2025-04-23 14:20: Train Epoch 27: 0/37 Loss: 0.227304\n",
      "2025-04-23 14:20: Train Epoch 27: 20/37 Loss: 0.055658\n",
      "2025-04-23 14:20: **********Train Epoch 27: averaged Loss: 0.070380\n",
      "2025-04-23 14:20: **********Val Epoch 27: average Loss: 0.061401\n",
      "2025-04-23 14:20: Train Epoch 28: 0/37 Loss: 0.083050\n",
      "2025-04-23 14:20: Train Epoch 28: 20/37 Loss: 0.010538\n",
      "2025-04-23 14:20: **********Train Epoch 28: averaged Loss: 0.033319\n",
      "2025-04-23 14:20: **********Val Epoch 28: average Loss: 0.114646\n",
      "2025-04-23 14:20: Train Epoch 29: 0/37 Loss: 0.013563\n",
      "2025-04-23 14:20: Train Epoch 29: 20/37 Loss: 0.009894\n",
      "2025-04-23 14:20: **********Train Epoch 29: averaged Loss: 0.037639\n",
      "2025-04-23 14:20: **********Val Epoch 29: average Loss: 0.033017\n",
      "2025-04-23 14:20: Train Epoch 30: 0/37 Loss: 0.060679\n",
      "2025-04-23 14:20: Train Epoch 30: 20/37 Loss: 0.035029\n",
      "2025-04-23 14:20: **********Train Epoch 30: averaged Loss: 0.038062\n",
      "2025-04-23 14:20: **********Val Epoch 30: average Loss: 0.078949\n",
      "2025-04-23 14:20: Train Epoch 31: 0/37 Loss: 0.076027\n",
      "2025-04-23 14:20: Train Epoch 31: 20/37 Loss: 0.006184\n",
      "2025-04-23 14:20: **********Train Epoch 31: averaged Loss: 0.050826\n",
      "2025-04-23 14:20: **********Val Epoch 31: average Loss: 0.138527\n",
      "2025-04-23 14:20: Train Epoch 32: 0/37 Loss: 0.049706\n",
      "2025-04-23 14:20: Train Epoch 32: 20/37 Loss: 0.019941\n",
      "2025-04-23 14:20: **********Train Epoch 32: averaged Loss: 0.052965\n",
      "2025-04-23 14:20: **********Val Epoch 32: average Loss: 0.033866\n",
      "2025-04-23 14:20: Train Epoch 33: 0/37 Loss: 0.224941\n",
      "2025-04-23 14:20: Train Epoch 33: 20/37 Loss: 0.008126\n",
      "2025-04-23 14:20: **********Train Epoch 33: averaged Loss: 0.094297\n",
      "2025-04-23 14:20: **********Val Epoch 33: average Loss: 0.172100\n",
      "2025-04-23 14:20: Train Epoch 34: 0/37 Loss: 0.088397\n",
      "2025-04-23 14:20: Train Epoch 34: 20/37 Loss: 0.039451\n",
      "2025-04-23 14:20: **********Train Epoch 34: averaged Loss: 0.038821\n",
      "2025-04-23 14:20: **********Val Epoch 34: average Loss: 0.081164\n",
      "2025-04-23 14:20: Train Epoch 35: 0/37 Loss: 0.147233\n",
      "2025-04-23 14:20: Train Epoch 35: 20/37 Loss: 0.019631\n",
      "2025-04-23 14:20: **********Train Epoch 35: averaged Loss: 0.076386\n",
      "2025-04-23 14:20: **********Val Epoch 35: average Loss: 0.075575\n",
      "2025-04-23 14:20: Train Epoch 36: 0/37 Loss: 0.074979\n",
      "2025-04-23 14:20: Train Epoch 36: 20/37 Loss: 0.027275\n",
      "2025-04-23 14:20: **********Train Epoch 36: averaged Loss: 0.040813\n",
      "2025-04-23 14:20: **********Val Epoch 36: average Loss: 0.126260\n",
      "2025-04-23 14:20: Train Epoch 37: 0/37 Loss: 0.150623\n",
      "2025-04-23 14:20: Train Epoch 37: 20/37 Loss: 0.086858\n",
      "2025-04-23 14:20: **********Train Epoch 37: averaged Loss: 0.079916\n",
      "2025-04-23 14:20: **********Val Epoch 37: average Loss: 0.073071\n",
      "2025-04-23 14:20: Train Epoch 38: 0/37 Loss: 0.107790\n",
      "2025-04-23 14:20: Train Epoch 38: 20/37 Loss: 0.044087\n",
      "2025-04-23 14:20: **********Train Epoch 38: averaged Loss: 0.040530\n",
      "2025-04-23 14:20: **********Val Epoch 38: average Loss: 0.108417\n",
      "2025-04-23 14:20: Train Epoch 39: 0/37 Loss: 0.202456\n",
      "2025-04-23 14:20: Train Epoch 39: 20/37 Loss: 0.007581\n",
      "2025-04-23 14:20: **********Train Epoch 39: averaged Loss: 0.072481\n",
      "2025-04-23 14:20: **********Val Epoch 39: average Loss: 0.228947\n",
      "2025-04-23 14:20: Train Epoch 40: 0/37 Loss: 0.035178\n",
      "2025-04-23 14:20: Train Epoch 40: 20/37 Loss: 0.026542\n",
      "2025-04-23 14:20: **********Train Epoch 40: averaged Loss: 0.074904\n",
      "2025-04-23 14:20: **********Val Epoch 40: average Loss: 0.118044\n",
      "2025-04-23 14:20: Train Epoch 41: 0/37 Loss: 0.218584\n",
      "2025-04-23 14:20: Train Epoch 41: 20/37 Loss: 0.020099\n",
      "2025-04-23 14:20: **********Train Epoch 41: averaged Loss: 0.089818\n",
      "2025-04-23 14:20: **********Val Epoch 41: average Loss: 0.031293\n",
      "2025-04-23 14:20: Train Epoch 42: 0/37 Loss: 0.035355\n",
      "2025-04-23 14:20: Train Epoch 42: 20/37 Loss: 0.024594\n",
      "2025-04-23 14:20: **********Train Epoch 42: averaged Loss: 0.038562\n",
      "2025-04-23 14:20: **********Val Epoch 42: average Loss: 0.055370\n",
      "2025-04-23 14:20: Train Epoch 43: 0/37 Loss: 0.054882\n",
      "2025-04-23 14:20: Train Epoch 43: 20/37 Loss: 0.014141\n",
      "2025-04-23 14:20: **********Train Epoch 43: averaged Loss: 0.028535\n",
      "2025-04-23 14:20: **********Val Epoch 43: average Loss: 0.010784\n",
      "2025-04-23 14:20: *********************************Current best model saved!\n",
      "2025-04-23 14:20: Train Epoch 44: 0/37 Loss: 0.097017\n",
      "2025-04-23 14:20: Train Epoch 44: 20/37 Loss: 0.019166\n",
      "2025-04-23 14:20: **********Train Epoch 44: averaged Loss: 0.037390\n",
      "2025-04-23 14:20: **********Val Epoch 44: average Loss: 0.066684\n",
      "2025-04-23 14:20: Train Epoch 45: 0/37 Loss: 0.053245\n",
      "2025-04-23 14:20: Train Epoch 45: 20/37 Loss: 0.016900\n",
      "2025-04-23 14:20: **********Train Epoch 45: averaged Loss: 0.034016\n",
      "2025-04-23 14:20: **********Val Epoch 45: average Loss: 0.049270\n",
      "2025-04-23 14:20: Train Epoch 46: 0/37 Loss: 0.062657\n",
      "2025-04-23 14:20: Train Epoch 46: 20/37 Loss: 0.006771\n",
      "2025-04-23 14:20: **********Train Epoch 46: averaged Loss: 0.024338\n",
      "2025-04-23 14:20: **********Val Epoch 46: average Loss: 0.039867\n",
      "2025-04-23 14:20: Train Epoch 47: 0/37 Loss: 0.155130\n",
      "2025-04-23 14:20: Train Epoch 47: 20/37 Loss: 0.005845\n",
      "2025-04-23 14:20: **********Train Epoch 47: averaged Loss: 0.047634\n",
      "2025-04-23 14:20: **********Val Epoch 47: average Loss: 0.016274\n",
      "2025-04-23 14:20: Train Epoch 48: 0/37 Loss: 0.098297\n",
      "2025-04-23 14:20: Train Epoch 48: 20/37 Loss: 0.009715\n",
      "2025-04-23 14:21: **********Train Epoch 48: averaged Loss: 0.040413\n",
      "2025-04-23 14:21: **********Val Epoch 48: average Loss: 0.110561\n",
      "2025-04-23 14:21: Train Epoch 49: 0/37 Loss: 0.071288\n",
      "2025-04-23 14:21: Train Epoch 49: 20/37 Loss: 0.020058\n",
      "2025-04-23 14:21: **********Train Epoch 49: averaged Loss: 0.026872\n",
      "2025-04-23 14:21: **********Val Epoch 49: average Loss: 0.031051\n",
      "2025-04-23 14:21: Train Epoch 50: 0/37 Loss: 0.160109\n",
      "2025-04-23 14:21: Train Epoch 50: 20/37 Loss: 0.087494\n",
      "2025-04-23 14:21: **********Train Epoch 50: averaged Loss: 0.067380\n",
      "2025-04-23 14:21: **********Val Epoch 50: average Loss: 0.013236\n",
      "2025-04-23 14:21: Train Epoch 51: 0/37 Loss: 0.123234\n",
      "2025-04-23 14:21: Train Epoch 51: 20/37 Loss: 0.014503\n",
      "2025-04-23 14:21: **********Train Epoch 51: averaged Loss: 0.036725\n",
      "2025-04-23 14:21: **********Val Epoch 51: average Loss: 0.019372\n",
      "2025-04-23 14:21: Train Epoch 52: 0/37 Loss: 0.028872\n",
      "2025-04-23 14:21: Train Epoch 52: 20/37 Loss: 0.017161\n",
      "2025-04-23 14:21: **********Train Epoch 52: averaged Loss: 0.028883\n",
      "2025-04-23 14:21: **********Val Epoch 52: average Loss: 0.048264\n",
      "2025-04-23 14:21: Train Epoch 53: 0/37 Loss: 0.074699\n",
      "2025-04-23 14:21: Train Epoch 53: 20/37 Loss: 0.009630\n",
      "2025-04-23 14:21: **********Train Epoch 53: averaged Loss: 0.022827\n",
      "2025-04-23 14:21: **********Val Epoch 53: average Loss: 0.048570\n",
      "2025-04-23 14:21: Train Epoch 54: 0/37 Loss: 0.071768\n",
      "2025-04-23 14:21: Train Epoch 54: 20/37 Loss: 0.034748\n",
      "2025-04-23 14:21: **********Train Epoch 54: averaged Loss: 0.035602\n",
      "2025-04-23 14:21: **********Val Epoch 54: average Loss: 0.035867\n",
      "2025-04-23 14:21: Train Epoch 55: 0/37 Loss: 0.068589\n",
      "2025-04-23 14:21: Train Epoch 55: 20/37 Loss: 0.036042\n",
      "2025-04-23 14:21: **********Train Epoch 55: averaged Loss: 0.026187\n",
      "2025-04-23 14:21: **********Val Epoch 55: average Loss: 0.048133\n",
      "2025-04-23 14:21: Train Epoch 56: 0/37 Loss: 0.041710\n",
      "2025-04-23 14:21: Train Epoch 56: 20/37 Loss: 0.005805\n",
      "2025-04-23 14:21: **********Train Epoch 56: averaged Loss: 0.035998\n",
      "2025-04-23 14:21: **********Val Epoch 56: average Loss: 0.115838\n",
      "2025-04-23 14:21: Train Epoch 57: 0/37 Loss: 0.007361\n",
      "2025-04-23 14:21: Train Epoch 57: 20/37 Loss: 0.023791\n",
      "2025-04-23 14:21: **********Train Epoch 57: averaged Loss: 0.031932\n",
      "2025-04-23 14:21: **********Val Epoch 57: average Loss: 0.091571\n",
      "2025-04-23 14:21: Train Epoch 58: 0/37 Loss: 0.175458\n",
      "2025-04-23 14:21: Train Epoch 58: 20/37 Loss: 0.008883\n",
      "2025-04-23 14:21: **********Train Epoch 58: averaged Loss: 0.062247\n",
      "2025-04-23 14:21: **********Val Epoch 58: average Loss: 0.083734\n",
      "2025-04-23 14:21: Train Epoch 59: 0/37 Loss: 0.069693\n",
      "2025-04-23 14:21: Train Epoch 59: 20/37 Loss: 0.012339\n",
      "2025-04-23 14:21: **********Train Epoch 59: averaged Loss: 0.048314\n",
      "2025-04-23 14:21: **********Val Epoch 59: average Loss: 0.082727\n",
      "2025-04-23 14:21: Train Epoch 60: 0/37 Loss: 0.076393\n",
      "2025-04-23 14:21: Train Epoch 60: 20/37 Loss: 0.062277\n",
      "2025-04-23 14:21: **********Train Epoch 60: averaged Loss: 0.067269\n",
      "2025-04-23 14:21: **********Val Epoch 60: average Loss: 0.168298\n",
      "2025-04-23 14:21: Train Epoch 61: 0/37 Loss: 0.194819\n",
      "2025-04-23 14:21: Train Epoch 61: 20/37 Loss: 0.054969\n",
      "2025-04-23 14:21: **********Train Epoch 61: averaged Loss: 0.052916\n",
      "2025-04-23 14:21: **********Val Epoch 61: average Loss: 0.036250\n",
      "2025-04-23 14:21: Train Epoch 62: 0/37 Loss: 0.152412\n",
      "2025-04-23 14:21: Train Epoch 62: 20/37 Loss: 0.012810\n",
      "2025-04-23 14:21: **********Train Epoch 62: averaged Loss: 0.047394\n",
      "2025-04-23 14:21: **********Val Epoch 62: average Loss: 0.009173\n",
      "2025-04-23 14:21: *********************************Current best model saved!\n",
      "2025-04-23 14:21: Train Epoch 63: 0/37 Loss: 0.099884\n",
      "2025-04-23 14:21: Train Epoch 63: 20/37 Loss: 0.018097\n",
      "2025-04-23 14:21: **********Train Epoch 63: averaged Loss: 0.043648\n",
      "2025-04-23 14:21: **********Val Epoch 63: average Loss: 0.033495\n",
      "2025-04-23 14:21: Train Epoch 64: 0/37 Loss: 0.064185\n",
      "2025-04-23 14:21: Train Epoch 64: 20/37 Loss: 0.030853\n",
      "2025-04-23 14:21: **********Train Epoch 64: averaged Loss: 0.045299\n",
      "2025-04-23 14:21: **********Val Epoch 64: average Loss: 0.019737\n",
      "2025-04-23 14:21: Train Epoch 65: 0/37 Loss: 0.127464\n",
      "2025-04-23 14:21: Train Epoch 65: 20/37 Loss: 0.008420\n",
      "2025-04-23 14:21: **********Train Epoch 65: averaged Loss: 0.039112\n",
      "2025-04-23 14:21: **********Val Epoch 65: average Loss: 0.017533\n",
      "2025-04-23 14:21: Train Epoch 66: 0/37 Loss: 0.054094\n",
      "2025-04-23 14:21: Train Epoch 66: 20/37 Loss: 0.032736\n",
      "2025-04-23 14:21: **********Train Epoch 66: averaged Loss: 0.036558\n",
      "2025-04-23 14:21: **********Val Epoch 66: average Loss: 0.138074\n",
      "2025-04-23 14:21: Train Epoch 67: 0/37 Loss: 0.032724\n",
      "2025-04-23 14:21: Train Epoch 67: 20/37 Loss: 0.044437\n",
      "2025-04-23 14:21: **********Train Epoch 67: averaged Loss: 0.047873\n",
      "2025-04-23 14:21: **********Val Epoch 67: average Loss: 0.039940\n",
      "2025-04-23 14:21: Train Epoch 68: 0/37 Loss: 0.161172\n",
      "2025-04-23 14:21: Train Epoch 68: 20/37 Loss: 0.055901\n",
      "2025-04-23 14:21: **********Train Epoch 68: averaged Loss: 0.085866\n",
      "2025-04-23 14:21: **********Val Epoch 68: average Loss: 0.152413\n",
      "2025-04-23 14:21: Train Epoch 69: 0/37 Loss: 0.084069\n",
      "2025-04-23 14:21: Train Epoch 69: 20/37 Loss: 0.012891\n",
      "2025-04-23 14:21: **********Train Epoch 69: averaged Loss: 0.097189\n",
      "2025-04-23 14:21: **********Val Epoch 69: average Loss: 0.159298\n",
      "2025-04-23 14:21: Train Epoch 70: 0/37 Loss: 0.075494\n",
      "2025-04-23 14:21: Train Epoch 70: 20/37 Loss: 0.049027\n",
      "2025-04-23 14:21: **********Train Epoch 70: averaged Loss: 0.033894\n",
      "2025-04-23 14:21: **********Val Epoch 70: average Loss: 0.036126\n",
      "2025-04-23 14:21: Train Epoch 71: 0/37 Loss: 0.112288\n",
      "2025-04-23 14:21: Train Epoch 71: 20/37 Loss: 0.012707\n",
      "2025-04-23 14:21: **********Train Epoch 71: averaged Loss: 0.037613\n",
      "2025-04-23 14:21: **********Val Epoch 71: average Loss: 0.013255\n",
      "2025-04-23 14:21: Train Epoch 72: 0/37 Loss: 0.139214\n",
      "2025-04-23 14:21: Train Epoch 72: 20/37 Loss: 0.007491\n",
      "2025-04-23 14:21: **********Train Epoch 72: averaged Loss: 0.053112\n",
      "2025-04-23 14:21: **********Val Epoch 72: average Loss: 0.045924\n",
      "2025-04-23 14:21: Train Epoch 73: 0/37 Loss: 0.226450\n",
      "2025-04-23 14:21: Train Epoch 73: 20/37 Loss: 0.028868\n",
      "2025-04-23 14:21: **********Train Epoch 73: averaged Loss: 0.069123\n",
      "2025-04-23 14:21: **********Val Epoch 73: average Loss: 0.009828\n",
      "2025-04-23 14:21: Train Epoch 74: 0/37 Loss: 0.194020\n",
      "2025-04-23 14:21: Train Epoch 74: 20/37 Loss: 0.036886\n",
      "2025-04-23 14:21: **********Train Epoch 74: averaged Loss: 0.035590\n",
      "2025-04-23 14:21: **********Val Epoch 74: average Loss: 0.036553\n",
      "2025-04-23 14:21: Train Epoch 75: 0/37 Loss: 0.019601\n",
      "2025-04-23 14:21: Train Epoch 75: 20/37 Loss: 0.031786\n",
      "2025-04-23 14:21: **********Train Epoch 75: averaged Loss: 0.021471\n",
      "2025-04-23 14:21: **********Val Epoch 75: average Loss: 0.044237\n",
      "2025-04-23 14:21: Train Epoch 76: 0/37 Loss: 0.044944\n",
      "2025-04-23 14:21: Train Epoch 76: 20/37 Loss: 0.020182\n",
      "2025-04-23 14:21: **********Train Epoch 76: averaged Loss: 0.038401\n",
      "2025-04-23 14:21: **********Val Epoch 76: average Loss: 0.088126\n",
      "2025-04-23 14:21: Train Epoch 77: 0/37 Loss: 0.033927\n",
      "2025-04-23 14:21: Train Epoch 77: 20/37 Loss: 0.038888\n",
      "2025-04-23 14:21: **********Train Epoch 77: averaged Loss: 0.020559\n",
      "2025-04-23 14:21: **********Val Epoch 77: average Loss: 0.020196\n",
      "2025-04-23 14:21: Train Epoch 78: 0/37 Loss: 0.107105\n",
      "2025-04-23 14:21: Train Epoch 78: 20/37 Loss: 0.025629\n",
      "2025-04-23 14:21: **********Train Epoch 78: averaged Loss: 0.049504\n",
      "2025-04-23 14:21: **********Val Epoch 78: average Loss: 0.110596\n",
      "2025-04-23 14:21: Train Epoch 79: 0/37 Loss: 0.008472\n",
      "2025-04-23 14:21: Train Epoch 79: 20/37 Loss: 0.006280\n",
      "2025-04-23 14:21: **********Train Epoch 79: averaged Loss: 0.017638\n",
      "2025-04-23 14:21: **********Val Epoch 79: average Loss: 0.057056\n",
      "2025-04-23 14:21: Train Epoch 80: 0/37 Loss: 0.159991\n",
      "2025-04-23 14:21: Train Epoch 80: 20/37 Loss: 0.010962\n",
      "2025-04-23 14:21: **********Train Epoch 80: averaged Loss: 0.053741\n",
      "2025-04-23 14:21: **********Val Epoch 80: average Loss: 0.044807\n",
      "2025-04-23 14:21: Train Epoch 81: 0/37 Loss: 0.143695\n",
      "2025-04-23 14:21: Train Epoch 81: 20/37 Loss: 0.014769\n",
      "2025-04-23 14:21: **********Train Epoch 81: averaged Loss: 0.039881\n",
      "2025-04-23 14:21: **********Val Epoch 81: average Loss: 0.016840\n",
      "2025-04-23 14:21: Train Epoch 82: 0/37 Loss: 0.083220\n",
      "2025-04-23 14:21: Train Epoch 82: 20/37 Loss: 0.031632\n",
      "2025-04-23 14:21: **********Train Epoch 82: averaged Loss: 0.038398\n",
      "2025-04-23 14:21: **********Val Epoch 82: average Loss: 0.094378\n",
      "2025-04-23 14:21: Train Epoch 83: 0/37 Loss: 0.011690\n",
      "2025-04-23 14:21: Train Epoch 83: 20/37 Loss: 0.034913\n",
      "2025-04-23 14:21: **********Train Epoch 83: averaged Loss: 0.022383\n",
      "2025-04-23 14:21: **********Val Epoch 83: average Loss: 0.010578\n",
      "2025-04-23 14:21: Train Epoch 84: 0/37 Loss: 0.177913\n",
      "2025-04-23 14:21: Train Epoch 84: 20/37 Loss: 0.029437\n",
      "2025-04-23 14:22: **********Train Epoch 84: averaged Loss: 0.060942\n",
      "2025-04-23 14:22: **********Val Epoch 84: average Loss: 0.010170\n",
      "2025-04-23 14:22: Train Epoch 85: 0/37 Loss: 0.074682\n",
      "2025-04-23 14:22: Train Epoch 85: 20/37 Loss: 0.017852\n",
      "2025-04-23 14:22: **********Train Epoch 85: averaged Loss: 0.021920\n",
      "2025-04-23 14:22: **********Val Epoch 85: average Loss: 0.048857\n",
      "2025-04-23 14:22: Train Epoch 86: 0/37 Loss: 0.015444\n",
      "2025-04-23 14:22: Train Epoch 86: 20/37 Loss: 0.021107\n",
      "2025-04-23 14:22: **********Train Epoch 86: averaged Loss: 0.024861\n",
      "2025-04-23 14:22: **********Val Epoch 86: average Loss: 0.052249\n",
      "2025-04-23 14:22: Train Epoch 87: 0/37 Loss: 0.043772\n",
      "2025-04-23 14:22: Train Epoch 87: 20/37 Loss: 0.031521\n",
      "2025-04-23 14:22: **********Train Epoch 87: averaged Loss: 0.018990\n",
      "2025-04-23 14:22: **********Val Epoch 87: average Loss: 0.009996\n",
      "2025-04-23 14:22: Train Epoch 88: 0/37 Loss: 0.027205\n",
      "2025-04-23 14:22: Train Epoch 88: 20/37 Loss: 0.022801\n",
      "2025-04-23 14:22: **********Train Epoch 88: averaged Loss: 0.022030\n",
      "2025-04-23 14:22: **********Val Epoch 88: average Loss: 0.010270\n",
      "2025-04-23 14:22: Train Epoch 89: 0/37 Loss: 0.023837\n",
      "2025-04-23 14:22: Train Epoch 89: 20/37 Loss: 0.016040\n",
      "2025-04-23 14:22: **********Train Epoch 89: averaged Loss: 0.029922\n",
      "2025-04-23 14:22: **********Val Epoch 89: average Loss: 0.054739\n",
      "2025-04-23 14:22: Train Epoch 90: 0/37 Loss: 0.006056\n",
      "2025-04-23 14:22: Train Epoch 90: 20/37 Loss: 0.007245\n",
      "2025-04-23 14:22: **********Train Epoch 90: averaged Loss: 0.015388\n",
      "2025-04-23 14:22: **********Val Epoch 90: average Loss: 0.013425\n",
      "2025-04-23 14:22: Train Epoch 91: 0/37 Loss: 0.087688\n",
      "2025-04-23 14:22: Train Epoch 91: 20/37 Loss: 0.029717\n",
      "2025-04-23 14:22: **********Train Epoch 91: averaged Loss: 0.029729\n",
      "2025-04-23 14:22: **********Val Epoch 91: average Loss: 0.048944\n",
      "2025-04-23 14:22: Train Epoch 92: 0/37 Loss: 0.030435\n",
      "2025-04-23 14:22: Train Epoch 92: 20/37 Loss: 0.005692\n",
      "2025-04-23 14:22: **********Train Epoch 92: averaged Loss: 0.034057\n",
      "2025-04-23 14:22: **********Val Epoch 92: average Loss: 0.126325\n",
      "2025-04-23 14:22: Train Epoch 93: 0/37 Loss: 0.056747\n",
      "2025-04-23 14:22: Train Epoch 93: 20/37 Loss: 0.014547\n",
      "2025-04-23 14:22: **********Train Epoch 93: averaged Loss: 0.042362\n",
      "2025-04-23 14:22: **********Val Epoch 93: average Loss: 0.039698\n",
      "2025-04-23 14:22: Train Epoch 94: 0/37 Loss: 0.165719\n",
      "2025-04-23 14:22: Train Epoch 94: 20/37 Loss: 0.013555\n",
      "2025-04-23 14:22: **********Train Epoch 94: averaged Loss: 0.076342\n",
      "2025-04-23 14:22: **********Val Epoch 94: average Loss: 0.011633\n",
      "2025-04-23 14:22: Train Epoch 95: 0/37 Loss: 0.046141\n",
      "2025-04-23 14:22: Train Epoch 95: 20/37 Loss: 0.010812\n",
      "2025-04-23 14:22: **********Train Epoch 95: averaged Loss: 0.047425\n",
      "2025-04-23 14:22: **********Val Epoch 95: average Loss: 0.049361\n",
      "2025-04-23 14:22: Train Epoch 96: 0/37 Loss: 0.152790\n",
      "2025-04-23 14:22: Train Epoch 96: 20/37 Loss: 0.006108\n",
      "2025-04-23 14:22: **********Train Epoch 96: averaged Loss: 0.038119\n",
      "2025-04-23 14:22: **********Val Epoch 96: average Loss: 0.026621\n",
      "2025-04-23 14:22: Train Epoch 97: 0/37 Loss: 0.035587\n",
      "2025-04-23 14:22: Train Epoch 97: 20/37 Loss: 0.032463\n",
      "2025-04-23 14:22: **********Train Epoch 97: averaged Loss: 0.029772\n",
      "2025-04-23 14:22: **********Val Epoch 97: average Loss: 0.098083\n",
      "2025-04-23 14:22: Train Epoch 98: 0/37 Loss: 0.003770\n",
      "2025-04-23 14:22: Train Epoch 98: 20/37 Loss: 0.036792\n",
      "2025-04-23 14:22: **********Train Epoch 98: averaged Loss: 0.024342\n",
      "2025-04-23 14:22: **********Val Epoch 98: average Loss: 0.016360\n",
      "2025-04-23 14:22: Train Epoch 99: 0/37 Loss: 0.136862\n",
      "2025-04-23 14:22: Train Epoch 99: 20/37 Loss: 0.022014\n",
      "2025-04-23 14:22: **********Train Epoch 99: averaged Loss: 0.039003\n",
      "2025-04-23 14:22: **********Val Epoch 99: average Loss: 0.017331\n",
      "2025-04-23 14:22: Train Epoch 100: 0/37 Loss: 0.118384\n",
      "2025-04-23 14:22: Train Epoch 100: 20/37 Loss: 0.027704\n",
      "2025-04-23 14:22: **********Train Epoch 100: averaged Loss: 0.040203\n",
      "2025-04-23 14:22: **********Val Epoch 100: average Loss: 0.025147\n",
      "2025-04-23 14:22: Train Epoch 101: 0/37 Loss: 0.022005\n",
      "2025-04-23 14:22: Train Epoch 101: 20/37 Loss: 0.008297\n",
      "2025-04-23 14:22: **********Train Epoch 101: averaged Loss: 0.020852\n",
      "2025-04-23 14:22: **********Val Epoch 101: average Loss: 0.028330\n",
      "2025-04-23 14:22: Train Epoch 102: 0/37 Loss: 0.032622\n",
      "2025-04-23 14:22: Train Epoch 102: 20/37 Loss: 0.016483\n",
      "2025-04-23 14:22: **********Train Epoch 102: averaged Loss: 0.017115\n",
      "2025-04-23 14:22: **********Val Epoch 102: average Loss: 0.056924\n",
      "2025-04-23 14:22: Train Epoch 103: 0/37 Loss: 0.032629\n",
      "2025-04-23 14:22: Train Epoch 103: 20/37 Loss: 0.005578\n",
      "2025-04-23 14:22: **********Train Epoch 103: averaged Loss: 0.030995\n",
      "2025-04-23 14:22: **********Val Epoch 103: average Loss: 0.107781\n",
      "2025-04-23 14:22: Train Epoch 104: 0/37 Loss: 0.043148\n",
      "2025-04-23 14:22: Train Epoch 104: 20/37 Loss: 0.019216\n",
      "2025-04-23 14:22: **********Train Epoch 104: averaged Loss: 0.039882\n",
      "2025-04-23 14:22: **********Val Epoch 104: average Loss: 0.020979\n",
      "2025-04-23 14:22: Train Epoch 105: 0/37 Loss: 0.175389\n",
      "2025-04-23 14:22: Train Epoch 105: 20/37 Loss: 0.006580\n",
      "2025-04-23 14:22: **********Train Epoch 105: averaged Loss: 0.068749\n",
      "2025-04-23 14:22: **********Val Epoch 105: average Loss: 0.164751\n",
      "2025-04-23 14:22: Train Epoch 106: 0/37 Loss: 0.147906\n",
      "2025-04-23 14:22: Train Epoch 106: 20/37 Loss: 0.117836\n",
      "2025-04-23 14:22: **********Train Epoch 106: averaged Loss: 0.121269\n",
      "2025-04-23 14:22: **********Val Epoch 106: average Loss: 0.301577\n",
      "2025-04-23 14:22: Train Epoch 107: 0/37 Loss: 0.323767\n",
      "2025-04-23 14:22: Train Epoch 107: 20/37 Loss: 0.052043\n",
      "2025-04-23 14:22: **********Train Epoch 107: averaged Loss: 0.202030\n",
      "2025-04-23 14:22: **********Val Epoch 107: average Loss: 0.313244\n",
      "2025-04-23 14:22: Train Epoch 108: 0/37 Loss: 0.312100\n",
      "2025-04-23 14:22: Train Epoch 108: 20/37 Loss: 0.037596\n",
      "2025-04-23 14:22: **********Train Epoch 108: averaged Loss: 0.197483\n",
      "2025-04-23 14:22: **********Val Epoch 108: average Loss: 0.325511\n",
      "2025-04-23 14:22: Train Epoch 109: 0/37 Loss: 0.299833\n",
      "2025-04-23 14:22: Train Epoch 109: 20/37 Loss: 0.025981\n",
      "2025-04-23 14:22: **********Train Epoch 109: averaged Loss: 0.194001\n",
      "2025-04-23 14:22: **********Val Epoch 109: average Loss: 0.335534\n",
      "2025-04-23 14:22: Train Epoch 110: 0/37 Loss: 0.289810\n",
      "2025-04-23 14:22: Train Epoch 110: 20/37 Loss: 0.016615\n",
      "2025-04-23 14:22: **********Train Epoch 110: averaged Loss: 0.191561\n",
      "2025-04-23 14:22: **********Val Epoch 110: average Loss: 0.344114\n",
      "2025-04-23 14:22: Train Epoch 111: 0/37 Loss: 0.281229\n",
      "2025-04-23 14:22: Train Epoch 111: 20/37 Loss: 0.010669\n",
      "2025-04-23 14:22: **********Train Epoch 111: averaged Loss: 0.189633\n",
      "2025-04-23 14:22: **********Val Epoch 111: average Loss: 0.351112\n",
      "2025-04-23 14:22: Train Epoch 112: 0/37 Loss: 0.274232\n",
      "2025-04-23 14:22: Train Epoch 112: 20/37 Loss: 0.009558\n",
      "2025-04-23 14:22: **********Train Epoch 112: averaged Loss: 0.188369\n",
      "2025-04-23 14:22: **********Val Epoch 112: average Loss: 0.357075\n",
      "2025-04-23 14:22: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:22: Total training time: 3.1319min, best loss: 0.009173\n",
      "2025-04-23 14:22: Average Horizon, MAE: 0.1208, MSE: 0.1228\n",
      "2025-04-23 14:22: Average Horizon, MAE: 0.1292, MSE: 0.1297\n",
      "2025-04-23 14:22: Experiment log path in: ./\n",
      "2025-04-23 14:22: Experiment log path in: ./\n",
      "2025-04-23 14:22: Train Epoch 1: 0/37 Loss: 0.211221\n",
      "2025-04-23 14:22: Train Epoch 1: 0/37 Loss: 0.211221\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "graphsage.fc.weight torch.Size([1, 10]) True\n",
      "graphsage.fc.bias torch.Size([1]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240663\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:22: Train Epoch 1: 20/37 Loss: 0.540002\n",
      "2025-04-23 14:22: Train Epoch 1: 20/37 Loss: 0.540002\n",
      "2025-04-23 14:22: **********Train Epoch 1: averaged Loss: 0.406589\n",
      "2025-04-23 14:22: **********Train Epoch 1: averaged Loss: 0.406589\n",
      "2025-04-23 14:22: **********Val Epoch 1: average Loss: 0.238125\n",
      "2025-04-23 14:22: **********Val Epoch 1: average Loss: 0.238125\n",
      "2025-04-23 14:22: *********************************Current best model saved!\n",
      "2025-04-23 14:22: *********************************Current best model saved!\n",
      "2025-04-23 14:22: Train Epoch 2: 0/37 Loss: 0.911213\n",
      "2025-04-23 14:22: Train Epoch 2: 0/37 Loss: 0.911213\n",
      "2025-04-23 14:22: Train Epoch 2: 20/37 Loss: 0.099864\n",
      "2025-04-23 14:22: Train Epoch 2: 20/37 Loss: 0.099864\n",
      "2025-04-23 14:22: **********Train Epoch 2: averaged Loss: 0.215236\n",
      "2025-04-23 14:22: **********Train Epoch 2: averaged Loss: 0.215236\n",
      "2025-04-23 14:22: **********Val Epoch 2: average Loss: 0.043240\n",
      "2025-04-23 14:22: **********Val Epoch 2: average Loss: 0.043240\n",
      "2025-04-23 14:22: *********************************Current best model saved!\n",
      "2025-04-23 14:22: *********************************Current best model saved!\n",
      "2025-04-23 14:22: Train Epoch 3: 0/37 Loss: 0.786062\n",
      "2025-04-23 14:22: Train Epoch 3: 0/37 Loss: 0.786062\n",
      "2025-04-23 14:22: Train Epoch 3: 20/37 Loss: 0.382978\n",
      "2025-04-23 14:22: Train Epoch 3: 20/37 Loss: 0.382978\n",
      "2025-04-23 14:22: **********Train Epoch 3: averaged Loss: 0.326439\n",
      "2025-04-23 14:22: **********Train Epoch 3: averaged Loss: 0.326439\n",
      "2025-04-23 14:22: **********Val Epoch 3: average Loss: 0.136041\n",
      "2025-04-23 14:22: **********Val Epoch 3: average Loss: 0.136041\n",
      "2025-04-23 14:22: Train Epoch 4: 0/37 Loss: 0.562506\n",
      "2025-04-23 14:22: Train Epoch 4: 0/37 Loss: 0.562506\n",
      "2025-04-23 14:22: Train Epoch 4: 20/37 Loss: 0.097767\n",
      "2025-04-23 14:22: Train Epoch 4: 20/37 Loss: 0.097767\n",
      "2025-04-23 14:22: **********Train Epoch 4: averaged Loss: 0.243276\n",
      "2025-04-23 14:22: **********Train Epoch 4: averaged Loss: 0.243276\n",
      "2025-04-23 14:22: **********Val Epoch 4: average Loss: 0.410323\n",
      "2025-04-23 14:22: **********Val Epoch 4: average Loss: 0.410323\n",
      "2025-04-23 14:22: Train Epoch 5: 0/37 Loss: 1.019357\n",
      "2025-04-23 14:22: Train Epoch 5: 0/37 Loss: 1.019357\n",
      "2025-04-23 14:22: Train Epoch 5: 20/37 Loss: 0.282413\n",
      "2025-04-23 14:22: Train Epoch 5: 20/37 Loss: 0.282413\n",
      "2025-04-23 14:22: **********Train Epoch 5: averaged Loss: 0.325494\n",
      "2025-04-23 14:22: **********Train Epoch 5: averaged Loss: 0.325494\n",
      "2025-04-23 14:22: **********Val Epoch 5: average Loss: 0.192233\n",
      "2025-04-23 14:22: **********Val Epoch 5: average Loss: 0.192233\n",
      "2025-04-23 14:22: Train Epoch 6: 0/37 Loss: 0.430246\n",
      "2025-04-23 14:22: Train Epoch 6: 0/37 Loss: 0.430246\n",
      "2025-04-23 14:22: Train Epoch 6: 20/37 Loss: 0.164054\n",
      "2025-04-23 14:22: Train Epoch 6: 20/37 Loss: 0.164054\n",
      "2025-04-23 14:22: **********Train Epoch 6: averaged Loss: 0.230053\n",
      "2025-04-23 14:22: **********Train Epoch 6: averaged Loss: 0.230053\n",
      "2025-04-23 14:22: **********Val Epoch 6: average Loss: 0.417519\n",
      "2025-04-23 14:22: **********Val Epoch 6: average Loss: 0.417519\n",
      "2025-04-23 14:22: Train Epoch 7: 0/37 Loss: 1.120849\n",
      "2025-04-23 14:22: Train Epoch 7: 0/37 Loss: 1.120849\n",
      "2025-04-23 14:22: Train Epoch 7: 20/37 Loss: 0.399411\n",
      "2025-04-23 14:22: Train Epoch 7: 20/37 Loss: 0.399411\n",
      "2025-04-23 14:22: **********Train Epoch 7: averaged Loss: 0.375951\n",
      "2025-04-23 14:22: **********Train Epoch 7: averaged Loss: 0.375951\n",
      "2025-04-23 14:22: **********Val Epoch 7: average Loss: 0.334991\n",
      "2025-04-23 14:22: **********Val Epoch 7: average Loss: 0.334991\n",
      "2025-04-23 14:22: Train Epoch 8: 0/37 Loss: 1.059241\n",
      "2025-04-23 14:22: Train Epoch 8: 0/37 Loss: 1.059241\n",
      "2025-04-23 14:23: Train Epoch 8: 20/37 Loss: 0.183476\n",
      "2025-04-23 14:23: Train Epoch 8: 20/37 Loss: 0.183476\n",
      "2025-04-23 14:23: **********Train Epoch 8: averaged Loss: 0.322004\n",
      "2025-04-23 14:23: **********Train Epoch 8: averaged Loss: 0.322004\n",
      "2025-04-23 14:23: **********Val Epoch 8: average Loss: 0.355115\n",
      "2025-04-23 14:23: **********Val Epoch 8: average Loss: 0.355115\n",
      "2025-04-23 14:23: Train Epoch 9: 0/37 Loss: 0.816458\n",
      "2025-04-23 14:23: Train Epoch 9: 0/37 Loss: 0.816458\n",
      "2025-04-23 14:23: Train Epoch 9: 20/37 Loss: 0.134144\n",
      "2025-04-23 14:23: Train Epoch 9: 20/37 Loss: 0.134144\n",
      "2025-04-23 14:23: **********Train Epoch 9: averaged Loss: 0.288507\n",
      "2025-04-23 14:23: **********Train Epoch 9: averaged Loss: 0.288507\n",
      "2025-04-23 14:23: **********Val Epoch 9: average Loss: 0.326471\n",
      "2025-04-23 14:23: **********Val Epoch 9: average Loss: 0.326471\n",
      "2025-04-23 14:23: Train Epoch 10: 0/37 Loss: 0.864360\n",
      "2025-04-23 14:23: Train Epoch 10: 0/37 Loss: 0.864360\n",
      "2025-04-23 14:23: Train Epoch 10: 20/37 Loss: 0.105326\n",
      "2025-04-23 14:23: Train Epoch 10: 20/37 Loss: 0.105326\n",
      "2025-04-23 14:23: **********Train Epoch 10: averaged Loss: 0.218438\n",
      "2025-04-23 14:23: **********Train Epoch 10: averaged Loss: 0.218438\n",
      "2025-04-23 14:23: **********Val Epoch 10: average Loss: 0.097072\n",
      "2025-04-23 14:23: **********Val Epoch 10: average Loss: 0.097072\n",
      "2025-04-23 14:23: Train Epoch 11: 0/37 Loss: 0.588289\n",
      "2025-04-23 14:23: Train Epoch 11: 0/37 Loss: 0.588289\n",
      "2025-04-23 14:23: Train Epoch 11: 20/37 Loss: 0.228919\n",
      "2025-04-23 14:23: Train Epoch 11: 20/37 Loss: 0.228919\n",
      "2025-04-23 14:23: **********Train Epoch 11: averaged Loss: 0.227450\n",
      "2025-04-23 14:23: **********Train Epoch 11: averaged Loss: 0.227450\n",
      "2025-04-23 14:23: **********Val Epoch 11: average Loss: 0.313238\n",
      "2025-04-23 14:23: **********Val Epoch 11: average Loss: 0.313238\n",
      "2025-04-23 14:23: Train Epoch 12: 0/37 Loss: 0.880374\n",
      "2025-04-23 14:23: Train Epoch 12: 0/37 Loss: 0.880374\n",
      "2025-04-23 14:23: Train Epoch 12: 20/37 Loss: 0.071918\n",
      "2025-04-23 14:23: Train Epoch 12: 20/37 Loss: 0.071918\n",
      "2025-04-23 14:23: **********Train Epoch 12: averaged Loss: 0.245022\n",
      "2025-04-23 14:23: **********Train Epoch 12: averaged Loss: 0.245022\n",
      "2025-04-23 14:23: **********Val Epoch 12: average Loss: 0.316910\n",
      "2025-04-23 14:23: **********Val Epoch 12: average Loss: 0.316910\n",
      "2025-04-23 14:23: Train Epoch 13: 0/37 Loss: 0.898568\n",
      "2025-04-23 14:23: Train Epoch 13: 0/37 Loss: 0.898568\n",
      "2025-04-23 14:23: Train Epoch 13: 20/37 Loss: 0.010197\n",
      "2025-04-23 14:23: Train Epoch 13: 20/37 Loss: 0.010197\n",
      "2025-04-23 14:23: **********Train Epoch 13: averaged Loss: 0.253212\n",
      "2025-04-23 14:23: **********Train Epoch 13: averaged Loss: 0.253212\n",
      "2025-04-23 14:23: **********Val Epoch 13: average Loss: 0.283836\n",
      "2025-04-23 14:23: **********Val Epoch 13: average Loss: 0.283836\n",
      "2025-04-23 14:23: Train Epoch 14: 0/37 Loss: 0.882525\n",
      "2025-04-23 14:23: Train Epoch 14: 0/37 Loss: 0.882525\n",
      "2025-04-23 14:23: Train Epoch 14: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:23: Train Epoch 14: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:23: **********Train Epoch 14: averaged Loss: 0.282433\n",
      "2025-04-23 14:23: **********Train Epoch 14: averaged Loss: 0.282433\n",
      "2025-04-23 14:23: **********Val Epoch 14: average Loss: 0.110930\n",
      "2025-04-23 14:23: **********Val Epoch 14: average Loss: 0.110930\n",
      "2025-04-23 14:23: Train Epoch 15: 0/37 Loss: 0.581807\n",
      "2025-04-23 14:23: Train Epoch 15: 0/37 Loss: 0.581807\n",
      "2025-04-23 14:23: Train Epoch 15: 20/37 Loss: 0.047367\n",
      "2025-04-23 14:23: Train Epoch 15: 20/37 Loss: 0.047367\n",
      "2025-04-23 14:23: **********Train Epoch 15: averaged Loss: 0.245314\n",
      "2025-04-23 14:23: **********Train Epoch 15: averaged Loss: 0.245314\n",
      "2025-04-23 14:23: **********Val Epoch 15: average Loss: 0.088847\n",
      "2025-04-23 14:23: **********Val Epoch 15: average Loss: 0.088847\n",
      "2025-04-23 14:23: Train Epoch 16: 0/37 Loss: 0.598742\n",
      "2025-04-23 14:23: Train Epoch 16: 0/37 Loss: 0.598742\n",
      "2025-04-23 14:23: Train Epoch 16: 20/37 Loss: 0.076659\n",
      "2025-04-23 14:23: Train Epoch 16: 20/37 Loss: 0.076659\n",
      "2025-04-23 14:23: **********Train Epoch 16: averaged Loss: 0.194866\n",
      "2025-04-23 14:23: **********Train Epoch 16: averaged Loss: 0.194866\n",
      "2025-04-23 14:23: **********Val Epoch 16: average Loss: 0.165710\n",
      "2025-04-23 14:23: **********Val Epoch 16: average Loss: 0.165710\n",
      "2025-04-23 14:23: Train Epoch 17: 0/37 Loss: 0.704390\n",
      "2025-04-23 14:23: Train Epoch 17: 0/37 Loss: 0.704390\n",
      "2025-04-23 14:23: Train Epoch 17: 20/37 Loss: 0.058775\n",
      "2025-04-23 14:23: Train Epoch 17: 20/37 Loss: 0.058775\n",
      "2025-04-23 14:23: **********Train Epoch 17: averaged Loss: 0.234529\n",
      "2025-04-23 14:23: **********Train Epoch 17: averaged Loss: 0.234529\n",
      "2025-04-23 14:23: **********Val Epoch 17: average Loss: 0.032865\n",
      "2025-04-23 14:23: **********Val Epoch 17: average Loss: 0.032865\n",
      "2025-04-23 14:23: *********************************Current best model saved!\n",
      "2025-04-23 14:23: *********************************Current best model saved!\n",
      "2025-04-23 14:23: Train Epoch 18: 0/37 Loss: 0.556460\n",
      "2025-04-23 14:23: Train Epoch 18: 0/37 Loss: 0.556460\n",
      "2025-04-23 14:23: Train Epoch 18: 20/37 Loss: 0.054926\n",
      "2025-04-23 14:23: Train Epoch 18: 20/37 Loss: 0.054926\n",
      "2025-04-23 14:23: **********Train Epoch 18: averaged Loss: 0.214848\n",
      "2025-04-23 14:23: **********Train Epoch 18: averaged Loss: 0.214848\n",
      "2025-04-23 14:23: **********Val Epoch 18: average Loss: 0.056599\n",
      "2025-04-23 14:23: **********Val Epoch 18: average Loss: 0.056599\n",
      "2025-04-23 14:23: Train Epoch 19: 0/37 Loss: 0.536319\n",
      "2025-04-23 14:23: Train Epoch 19: 0/37 Loss: 0.536319\n",
      "2025-04-23 14:23: Train Epoch 19: 20/37 Loss: 0.008558\n",
      "2025-04-23 14:23: Train Epoch 19: 20/37 Loss: 0.008558\n",
      "2025-04-23 14:23: **********Train Epoch 19: averaged Loss: 0.206129\n",
      "2025-04-23 14:23: **********Train Epoch 19: averaged Loss: 0.206129\n",
      "2025-04-23 14:23: **********Val Epoch 19: average Loss: 0.038512\n",
      "2025-04-23 14:23: **********Val Epoch 19: average Loss: 0.038512\n",
      "2025-04-23 14:23: Train Epoch 20: 0/37 Loss: 0.518380\n",
      "2025-04-23 14:23: Train Epoch 20: 0/37 Loss: 0.518380\n",
      "2025-04-23 14:23: Train Epoch 20: 20/37 Loss: 0.089884\n",
      "2025-04-23 14:23: Train Epoch 20: 20/37 Loss: 0.089884\n",
      "2025-04-23 14:23: **********Train Epoch 20: averaged Loss: 0.181242\n",
      "2025-04-23 14:23: **********Train Epoch 20: averaged Loss: 0.181242\n",
      "2025-04-23 14:23: **********Val Epoch 20: average Loss: 0.107958\n",
      "2025-04-23 14:23: **********Val Epoch 20: average Loss: 0.107958\n",
      "2025-04-23 14:23: Train Epoch 21: 0/37 Loss: 0.530492\n",
      "2025-04-23 14:23: Train Epoch 21: 0/37 Loss: 0.530492\n",
      "2025-04-23 14:23: Train Epoch 21: 20/37 Loss: 0.038567\n",
      "2025-04-23 14:23: Train Epoch 21: 20/37 Loss: 0.038567\n",
      "2025-04-23 14:23: **********Train Epoch 21: averaged Loss: 0.170543\n",
      "2025-04-23 14:23: **********Train Epoch 21: averaged Loss: 0.170543\n",
      "2025-04-23 14:23: **********Val Epoch 21: average Loss: 0.250573\n",
      "2025-04-23 14:23: **********Val Epoch 21: average Loss: 0.250573\n",
      "2025-04-23 14:23: Train Epoch 22: 0/37 Loss: 0.599225\n",
      "2025-04-23 14:23: Train Epoch 22: 0/37 Loss: 0.599225\n",
      "2025-04-23 14:23: Train Epoch 22: 20/37 Loss: 0.054265\n",
      "2025-04-23 14:23: Train Epoch 22: 20/37 Loss: 0.054265\n",
      "2025-04-23 14:23: **********Train Epoch 22: averaged Loss: 0.202141\n",
      "2025-04-23 14:23: **********Train Epoch 22: averaged Loss: 0.202141\n",
      "2025-04-23 14:23: **********Val Epoch 22: average Loss: 0.050225\n",
      "2025-04-23 14:23: **********Val Epoch 22: average Loss: 0.050225\n",
      "2025-04-23 14:23: Train Epoch 23: 0/37 Loss: 0.391340\n",
      "2025-04-23 14:23: Train Epoch 23: 0/37 Loss: 0.391340\n",
      "2025-04-23 14:23: Train Epoch 23: 20/37 Loss: 0.032383\n",
      "2025-04-23 14:23: Train Epoch 23: 20/37 Loss: 0.032383\n",
      "2025-04-23 14:23: **********Train Epoch 23: averaged Loss: 0.125732\n",
      "2025-04-23 14:23: **********Train Epoch 23: averaged Loss: 0.125732\n",
      "2025-04-23 14:23: **********Val Epoch 23: average Loss: 0.109739\n",
      "2025-04-23 14:23: **********Val Epoch 23: average Loss: 0.109739\n",
      "2025-04-23 14:23: Train Epoch 24: 0/37 Loss: 0.365371\n",
      "2025-04-23 14:23: Train Epoch 24: 0/37 Loss: 0.365371\n",
      "2025-04-23 14:23: Train Epoch 24: 20/37 Loss: 0.009318\n",
      "2025-04-23 14:23: Train Epoch 24: 20/37 Loss: 0.009318\n",
      "2025-04-23 14:23: **********Train Epoch 24: averaged Loss: 0.110987\n",
      "2025-04-23 14:23: **********Train Epoch 24: averaged Loss: 0.110987\n",
      "2025-04-23 14:23: **********Val Epoch 24: average Loss: 0.157702\n",
      "2025-04-23 14:23: **********Val Epoch 24: average Loss: 0.157702\n",
      "2025-04-23 14:23: Train Epoch 25: 0/37 Loss: 0.477850\n",
      "2025-04-23 14:23: Train Epoch 25: 0/37 Loss: 0.477850\n",
      "2025-04-23 14:23: Train Epoch 25: 20/37 Loss: 0.083315\n",
      "2025-04-23 14:23: Train Epoch 25: 20/37 Loss: 0.083315\n",
      "2025-04-23 14:23: **********Train Epoch 25: averaged Loss: 0.144714\n",
      "2025-04-23 14:23: **********Train Epoch 25: averaged Loss: 0.144714\n",
      "2025-04-23 14:23: **********Val Epoch 25: average Loss: 0.237897\n",
      "2025-04-23 14:23: **********Val Epoch 25: average Loss: 0.237897\n",
      "2025-04-23 14:23: Train Epoch 26: 0/37 Loss: 0.471404\n",
      "2025-04-23 14:23: Train Epoch 26: 0/37 Loss: 0.471404\n",
      "2025-04-23 14:23: Train Epoch 26: 20/37 Loss: 0.079520\n",
      "2025-04-23 14:23: Train Epoch 26: 20/37 Loss: 0.079520\n",
      "2025-04-23 14:23: **********Train Epoch 26: averaged Loss: 0.161528\n",
      "2025-04-23 14:23: **********Train Epoch 26: averaged Loss: 0.161528\n",
      "2025-04-23 14:23: **********Val Epoch 26: average Loss: 0.116317\n",
      "2025-04-23 14:23: **********Val Epoch 26: average Loss: 0.116317\n",
      "2025-04-23 14:23: Train Epoch 27: 0/37 Loss: 0.284526\n",
      "2025-04-23 14:23: Train Epoch 27: 0/37 Loss: 0.284526\n",
      "2025-04-23 14:23: Train Epoch 27: 20/37 Loss: 0.077278\n",
      "2025-04-23 14:23: Train Epoch 27: 20/37 Loss: 0.077278\n",
      "2025-04-23 14:23: **********Train Epoch 27: averaged Loss: 0.093151\n",
      "2025-04-23 14:23: **********Train Epoch 27: averaged Loss: 0.093151\n",
      "2025-04-23 14:23: **********Val Epoch 27: average Loss: 0.099309\n",
      "2025-04-23 14:23: **********Val Epoch 27: average Loss: 0.099309\n",
      "2025-04-23 14:23: Train Epoch 28: 0/37 Loss: 0.024286\n",
      "2025-04-23 14:23: Train Epoch 28: 0/37 Loss: 0.024286\n",
      "2025-04-23 14:23: Train Epoch 28: 20/37 Loss: 0.051119\n",
      "2025-04-23 14:23: Train Epoch 28: 20/37 Loss: 0.051119\n",
      "2025-04-23 14:23: **********Train Epoch 28: averaged Loss: 0.035524\n",
      "2025-04-23 14:23: **********Train Epoch 28: averaged Loss: 0.035524\n",
      "2025-04-23 14:23: **********Val Epoch 28: average Loss: 0.094466\n",
      "2025-04-23 14:23: **********Val Epoch 28: average Loss: 0.094466\n",
      "2025-04-23 14:23: Train Epoch 29: 0/37 Loss: 0.024656\n",
      "2025-04-23 14:23: Train Epoch 29: 0/37 Loss: 0.024656\n",
      "2025-04-23 14:23: Train Epoch 29: 20/37 Loss: 0.010600\n",
      "2025-04-23 14:23: Train Epoch 29: 20/37 Loss: 0.010600\n",
      "2025-04-23 14:23: **********Train Epoch 29: averaged Loss: 0.038053\n",
      "2025-04-23 14:23: **********Train Epoch 29: averaged Loss: 0.038053\n",
      "2025-04-23 14:23: **********Val Epoch 29: average Loss: 0.235997\n",
      "2025-04-23 14:23: **********Val Epoch 29: average Loss: 0.235997\n",
      "2025-04-23 14:23: Train Epoch 30: 0/37 Loss: 0.126624\n",
      "2025-04-23 14:23: Train Epoch 30: 0/37 Loss: 0.126624\n",
      "2025-04-23 14:23: Train Epoch 30: 20/37 Loss: 0.062214\n",
      "2025-04-23 14:23: Train Epoch 30: 20/37 Loss: 0.062214\n",
      "2025-04-23 14:23: **********Train Epoch 30: averaged Loss: 0.116722\n",
      "2025-04-23 14:23: **********Train Epoch 30: averaged Loss: 0.116722\n",
      "2025-04-23 14:23: **********Val Epoch 30: average Loss: 0.155968\n",
      "2025-04-23 14:23: **********Val Epoch 30: average Loss: 0.155968\n",
      "2025-04-23 14:23: Train Epoch 31: 0/37 Loss: 0.184997\n",
      "2025-04-23 14:23: Train Epoch 31: 0/37 Loss: 0.184997\n",
      "2025-04-23 14:23: Train Epoch 31: 20/37 Loss: 0.022008\n",
      "2025-04-23 14:23: Train Epoch 31: 20/37 Loss: 0.022008\n",
      "2025-04-23 14:23: **********Train Epoch 31: averaged Loss: 0.059778\n",
      "2025-04-23 14:23: **********Train Epoch 31: averaged Loss: 0.059778\n",
      "2025-04-23 14:23: **********Val Epoch 31: average Loss: 0.175809\n",
      "2025-04-23 14:23: **********Val Epoch 31: average Loss: 0.175809\n",
      "2025-04-23 14:23: Train Epoch 32: 0/37 Loss: 0.105720\n",
      "2025-04-23 14:23: Train Epoch 32: 0/37 Loss: 0.105720\n",
      "2025-04-23 14:23: Train Epoch 32: 20/37 Loss: 0.026433\n",
      "2025-04-23 14:23: Train Epoch 32: 20/37 Loss: 0.026433\n",
      "2025-04-23 14:23: **********Train Epoch 32: averaged Loss: 0.039200\n",
      "2025-04-23 14:23: **********Train Epoch 32: averaged Loss: 0.039200\n",
      "2025-04-23 14:23: **********Val Epoch 32: average Loss: 0.015154\n",
      "2025-04-23 14:23: **********Val Epoch 32: average Loss: 0.015154\n",
      "2025-04-23 14:23: *********************************Current best model saved!\n",
      "2025-04-23 14:23: *********************************Current best model saved!\n",
      "2025-04-23 14:23: Train Epoch 33: 0/37 Loss: 0.114530\n",
      "2025-04-23 14:23: Train Epoch 33: 0/37 Loss: 0.114530\n",
      "2025-04-23 14:23: Train Epoch 33: 20/37 Loss: 0.016883\n",
      "2025-04-23 14:23: Train Epoch 33: 20/37 Loss: 0.016883\n",
      "2025-04-23 14:23: **********Train Epoch 33: averaged Loss: 0.066267\n",
      "2025-04-23 14:23: **********Train Epoch 33: averaged Loss: 0.066267\n",
      "2025-04-23 14:23: **********Val Epoch 33: average Loss: 0.085254\n",
      "2025-04-23 14:23: **********Val Epoch 33: average Loss: 0.085254\n",
      "2025-04-23 14:23: Train Epoch 34: 0/37 Loss: 0.365909\n",
      "2025-04-23 14:23: Train Epoch 34: 0/37 Loss: 0.365909\n",
      "2025-04-23 14:23: Train Epoch 34: 20/37 Loss: 0.090879\n",
      "2025-04-23 14:23: Train Epoch 34: 20/37 Loss: 0.090879\n",
      "2025-04-23 14:23: **********Train Epoch 34: averaged Loss: 0.100097\n",
      "2025-04-23 14:23: **********Train Epoch 34: averaged Loss: 0.100097\n",
      "2025-04-23 14:23: **********Val Epoch 34: average Loss: 0.028235\n",
      "2025-04-23 14:23: **********Val Epoch 34: average Loss: 0.028235\n",
      "2025-04-23 14:23: Train Epoch 35: 0/37 Loss: 0.127476\n",
      "2025-04-23 14:23: Train Epoch 35: 0/37 Loss: 0.127476\n",
      "2025-04-23 14:23: Train Epoch 35: 20/37 Loss: 0.058738\n",
      "2025-04-23 14:23: Train Epoch 35: 20/37 Loss: 0.058738\n",
      "2025-04-23 14:23: **********Train Epoch 35: averaged Loss: 0.073247\n",
      "2025-04-23 14:23: **********Train Epoch 35: averaged Loss: 0.073247\n",
      "2025-04-23 14:23: **********Val Epoch 35: average Loss: 0.150022\n",
      "2025-04-23 14:23: **********Val Epoch 35: average Loss: 0.150022\n",
      "2025-04-23 14:23: Train Epoch 36: 0/37 Loss: 0.478680\n",
      "2025-04-23 14:23: Train Epoch 36: 0/37 Loss: 0.478680\n",
      "2025-04-23 14:23: Train Epoch 36: 20/37 Loss: 0.046329\n",
      "2025-04-23 14:23: Train Epoch 36: 20/37 Loss: 0.046329\n",
      "2025-04-23 14:23: **********Train Epoch 36: averaged Loss: 0.153394\n",
      "2025-04-23 14:23: **********Train Epoch 36: averaged Loss: 0.153394\n",
      "2025-04-23 14:23: **********Val Epoch 36: average Loss: 0.035816\n",
      "2025-04-23 14:23: **********Val Epoch 36: average Loss: 0.035816\n",
      "2025-04-23 14:23: Train Epoch 37: 0/37 Loss: 0.283107\n",
      "2025-04-23 14:23: Train Epoch 37: 0/37 Loss: 0.283107\n",
      "2025-04-23 14:23: Train Epoch 37: 20/37 Loss: 0.035207\n",
      "2025-04-23 14:23: Train Epoch 37: 20/37 Loss: 0.035207\n",
      "2025-04-23 14:23: **********Train Epoch 37: averaged Loss: 0.079155\n",
      "2025-04-23 14:23: **********Train Epoch 37: averaged Loss: 0.079155\n",
      "2025-04-23 14:23: **********Val Epoch 37: average Loss: 0.071128\n",
      "2025-04-23 14:23: **********Val Epoch 37: average Loss: 0.071128\n",
      "2025-04-23 14:23: Train Epoch 38: 0/37 Loss: 0.198312\n",
      "2025-04-23 14:23: Train Epoch 38: 0/37 Loss: 0.198312\n",
      "2025-04-23 14:23: Train Epoch 38: 20/37 Loss: 0.041496\n",
      "2025-04-23 14:23: Train Epoch 38: 20/37 Loss: 0.041496\n",
      "2025-04-23 14:23: **********Train Epoch 38: averaged Loss: 0.072665\n",
      "2025-04-23 14:23: **********Train Epoch 38: averaged Loss: 0.072665\n",
      "2025-04-23 14:23: **********Val Epoch 38: average Loss: 0.137797\n",
      "2025-04-23 14:23: **********Val Epoch 38: average Loss: 0.137797\n",
      "2025-04-23 14:23: Train Epoch 39: 0/37 Loss: 0.298943\n",
      "2025-04-23 14:23: Train Epoch 39: 0/37 Loss: 0.298943\n",
      "2025-04-23 14:23: Train Epoch 39: 20/37 Loss: 0.048139\n",
      "2025-04-23 14:23: Train Epoch 39: 20/37 Loss: 0.048139\n",
      "2025-04-23 14:23: **********Train Epoch 39: averaged Loss: 0.075294\n",
      "2025-04-23 14:23: **********Train Epoch 39: averaged Loss: 0.075294\n",
      "2025-04-23 14:23: **********Val Epoch 39: average Loss: 0.039541\n",
      "2025-04-23 14:23: **********Val Epoch 39: average Loss: 0.039541\n",
      "2025-04-23 14:23: Train Epoch 40: 0/37 Loss: 0.175601\n",
      "2025-04-23 14:23: Train Epoch 40: 0/37 Loss: 0.175601\n",
      "2025-04-23 14:23: Train Epoch 40: 20/37 Loss: 0.016109\n",
      "2025-04-23 14:23: Train Epoch 40: 20/37 Loss: 0.016109\n",
      "2025-04-23 14:23: **********Train Epoch 40: averaged Loss: 0.062999\n",
      "2025-04-23 14:23: **********Train Epoch 40: averaged Loss: 0.062999\n",
      "2025-04-23 14:23: **********Val Epoch 40: average Loss: 0.146462\n",
      "2025-04-23 14:23: **********Val Epoch 40: average Loss: 0.146462\n",
      "2025-04-23 14:23: Train Epoch 41: 0/37 Loss: 0.298786\n",
      "2025-04-23 14:23: Train Epoch 41: 0/37 Loss: 0.298786\n",
      "2025-04-23 14:23: Train Epoch 41: 20/37 Loss: 0.038334\n",
      "2025-04-23 14:23: Train Epoch 41: 20/37 Loss: 0.038334\n",
      "2025-04-23 14:23: **********Train Epoch 41: averaged Loss: 0.085522\n",
      "2025-04-23 14:23: **********Train Epoch 41: averaged Loss: 0.085522\n",
      "2025-04-23 14:23: **********Val Epoch 41: average Loss: 0.032537\n",
      "2025-04-23 14:23: **********Val Epoch 41: average Loss: 0.032537\n",
      "2025-04-23 14:23: Train Epoch 42: 0/37 Loss: 0.174338\n",
      "2025-04-23 14:23: Train Epoch 42: 0/37 Loss: 0.174338\n",
      "2025-04-23 14:23: Train Epoch 42: 20/37 Loss: 0.023475\n",
      "2025-04-23 14:23: Train Epoch 42: 20/37 Loss: 0.023475\n",
      "2025-04-23 14:23: **********Train Epoch 42: averaged Loss: 0.067712\n",
      "2025-04-23 14:23: **********Train Epoch 42: averaged Loss: 0.067712\n",
      "2025-04-23 14:23: **********Val Epoch 42: average Loss: 0.115087\n",
      "2025-04-23 14:23: **********Val Epoch 42: average Loss: 0.115087\n",
      "2025-04-23 14:23: Train Epoch 43: 0/37 Loss: 0.350689\n",
      "2025-04-23 14:23: Train Epoch 43: 0/37 Loss: 0.350689\n",
      "2025-04-23 14:23: Train Epoch 43: 20/37 Loss: 0.014315\n",
      "2025-04-23 14:23: Train Epoch 43: 20/37 Loss: 0.014315\n",
      "2025-04-23 14:23: **********Train Epoch 43: averaged Loss: 0.108680\n",
      "2025-04-23 14:23: **********Train Epoch 43: averaged Loss: 0.108680\n",
      "2025-04-23 14:23: **********Val Epoch 43: average Loss: 0.118840\n",
      "2025-04-23 14:23: **********Val Epoch 43: average Loss: 0.118840\n",
      "2025-04-23 14:23: Train Epoch 44: 0/37 Loss: 0.094165\n",
      "2025-04-23 14:23: Train Epoch 44: 0/37 Loss: 0.094165\n",
      "2025-04-23 14:24: Train Epoch 44: 20/37 Loss: 0.042842\n",
      "2025-04-23 14:24: Train Epoch 44: 20/37 Loss: 0.042842\n",
      "2025-04-23 14:24: **********Train Epoch 44: averaged Loss: 0.056639\n",
      "2025-04-23 14:24: **********Train Epoch 44: averaged Loss: 0.056639\n",
      "2025-04-23 14:24: **********Val Epoch 44: average Loss: 0.057356\n",
      "2025-04-23 14:24: **********Val Epoch 44: average Loss: 0.057356\n",
      "2025-04-23 14:24: Train Epoch 45: 0/37 Loss: 0.219790\n",
      "2025-04-23 14:24: Train Epoch 45: 0/37 Loss: 0.219790\n",
      "2025-04-23 14:24: Train Epoch 45: 20/37 Loss: 0.013410\n",
      "2025-04-23 14:24: Train Epoch 45: 20/37 Loss: 0.013410\n",
      "2025-04-23 14:24: **********Train Epoch 45: averaged Loss: 0.077430\n",
      "2025-04-23 14:24: **********Train Epoch 45: averaged Loss: 0.077430\n",
      "2025-04-23 14:24: **********Val Epoch 45: average Loss: 0.030475\n",
      "2025-04-23 14:24: **********Val Epoch 45: average Loss: 0.030475\n",
      "2025-04-23 14:24: Train Epoch 46: 0/37 Loss: 0.050404\n",
      "2025-04-23 14:24: Train Epoch 46: 0/37 Loss: 0.050404\n",
      "2025-04-23 14:24: Train Epoch 46: 20/37 Loss: 0.026855\n",
      "2025-04-23 14:24: Train Epoch 46: 20/37 Loss: 0.026855\n",
      "2025-04-23 14:24: **********Train Epoch 46: averaged Loss: 0.072123\n",
      "2025-04-23 14:24: **********Train Epoch 46: averaged Loss: 0.072123\n",
      "2025-04-23 14:24: **********Val Epoch 46: average Loss: 0.249288\n",
      "2025-04-23 14:24: **********Val Epoch 46: average Loss: 0.249288\n",
      "2025-04-23 14:24: Train Epoch 47: 0/37 Loss: 0.150690\n",
      "2025-04-23 14:24: Train Epoch 47: 0/37 Loss: 0.150690\n",
      "2025-04-23 14:24: Train Epoch 47: 20/37 Loss: 0.096059\n",
      "2025-04-23 14:24: Train Epoch 47: 20/37 Loss: 0.096059\n",
      "2025-04-23 14:24: **********Train Epoch 47: averaged Loss: 0.059504\n",
      "2025-04-23 14:24: **********Train Epoch 47: averaged Loss: 0.059504\n",
      "2025-04-23 14:24: **********Val Epoch 47: average Loss: 0.077169\n",
      "2025-04-23 14:24: **********Val Epoch 47: average Loss: 0.077169\n",
      "2025-04-23 14:24: Train Epoch 48: 0/37 Loss: 0.269726\n",
      "2025-04-23 14:24: Train Epoch 48: 0/37 Loss: 0.269726\n",
      "2025-04-23 14:24: Train Epoch 48: 20/37 Loss: 0.034618\n",
      "2025-04-23 14:24: Train Epoch 48: 20/37 Loss: 0.034618\n",
      "2025-04-23 14:24: **********Train Epoch 48: averaged Loss: 0.077858\n",
      "2025-04-23 14:24: **********Train Epoch 48: averaged Loss: 0.077858\n",
      "2025-04-23 14:24: **********Val Epoch 48: average Loss: 0.074020\n",
      "2025-04-23 14:24: **********Val Epoch 48: average Loss: 0.074020\n",
      "2025-04-23 14:24: Train Epoch 49: 0/37 Loss: 0.186120\n",
      "2025-04-23 14:24: Train Epoch 49: 0/37 Loss: 0.186120\n",
      "2025-04-23 14:24: Train Epoch 49: 20/37 Loss: 0.016611\n",
      "2025-04-23 14:24: Train Epoch 49: 20/37 Loss: 0.016611\n",
      "2025-04-23 14:24: **********Train Epoch 49: averaged Loss: 0.058352\n",
      "2025-04-23 14:24: **********Train Epoch 49: averaged Loss: 0.058352\n",
      "2025-04-23 14:24: **********Val Epoch 49: average Loss: 0.094769\n",
      "2025-04-23 14:24: **********Val Epoch 49: average Loss: 0.094769\n",
      "2025-04-23 14:24: Train Epoch 50: 0/37 Loss: 0.298599\n",
      "2025-04-23 14:24: Train Epoch 50: 0/37 Loss: 0.298599\n",
      "2025-04-23 14:24: Train Epoch 50: 20/37 Loss: 0.049604\n",
      "2025-04-23 14:24: Train Epoch 50: 20/37 Loss: 0.049604\n",
      "2025-04-23 14:24: **********Train Epoch 50: averaged Loss: 0.096077\n",
      "2025-04-23 14:24: **********Train Epoch 50: averaged Loss: 0.096077\n",
      "2025-04-23 14:24: **********Val Epoch 50: average Loss: 0.091461\n",
      "2025-04-23 14:24: **********Val Epoch 50: average Loss: 0.091461\n",
      "2025-04-23 14:24: Train Epoch 51: 0/37 Loss: 0.295974\n",
      "2025-04-23 14:24: Train Epoch 51: 0/37 Loss: 0.295974\n",
      "2025-04-23 14:24: Train Epoch 51: 20/37 Loss: 0.015850\n",
      "2025-04-23 14:24: Train Epoch 51: 20/37 Loss: 0.015850\n",
      "2025-04-23 14:24: **********Train Epoch 51: averaged Loss: 0.091021\n",
      "2025-04-23 14:24: **********Train Epoch 51: averaged Loss: 0.091021\n",
      "2025-04-23 14:24: **********Val Epoch 51: average Loss: 0.079635\n",
      "2025-04-23 14:24: **********Val Epoch 51: average Loss: 0.079635\n",
      "2025-04-23 14:24: Train Epoch 52: 0/37 Loss: 0.218611\n",
      "2025-04-23 14:24: Train Epoch 52: 0/37 Loss: 0.218611\n",
      "2025-04-23 14:24: Train Epoch 52: 20/37 Loss: 0.036957\n",
      "2025-04-23 14:24: Train Epoch 52: 20/37 Loss: 0.036957\n",
      "2025-04-23 14:24: **********Train Epoch 52: averaged Loss: 0.100484\n",
      "2025-04-23 14:24: **********Train Epoch 52: averaged Loss: 0.100484\n",
      "2025-04-23 14:24: **********Val Epoch 52: average Loss: 0.094130\n",
      "2025-04-23 14:24: **********Val Epoch 52: average Loss: 0.094130\n",
      "2025-04-23 14:24: Train Epoch 53: 0/37 Loss: 0.048347\n",
      "2025-04-23 14:24: Train Epoch 53: 0/37 Loss: 0.048347\n",
      "2025-04-23 14:24: Train Epoch 53: 20/37 Loss: 0.037497\n",
      "2025-04-23 14:24: Train Epoch 53: 20/37 Loss: 0.037497\n",
      "2025-04-23 14:24: **********Train Epoch 53: averaged Loss: 0.055501\n",
      "2025-04-23 14:24: **********Train Epoch 53: averaged Loss: 0.055501\n",
      "2025-04-23 14:24: **********Val Epoch 53: average Loss: 0.098804\n",
      "2025-04-23 14:24: **********Val Epoch 53: average Loss: 0.098804\n",
      "2025-04-23 14:24: Train Epoch 54: 0/37 Loss: 0.173690\n",
      "2025-04-23 14:24: Train Epoch 54: 0/37 Loss: 0.173690\n",
      "2025-04-23 14:24: Train Epoch 54: 20/37 Loss: 0.011894\n",
      "2025-04-23 14:24: Train Epoch 54: 20/37 Loss: 0.011894\n",
      "2025-04-23 14:24: **********Train Epoch 54: averaged Loss: 0.058767\n",
      "2025-04-23 14:24: **********Train Epoch 54: averaged Loss: 0.058767\n",
      "2025-04-23 14:24: **********Val Epoch 54: average Loss: 0.094231\n",
      "2025-04-23 14:24: **********Val Epoch 54: average Loss: 0.094231\n",
      "2025-04-23 14:24: Train Epoch 55: 0/37 Loss: 0.231037\n",
      "2025-04-23 14:24: Train Epoch 55: 0/37 Loss: 0.231037\n",
      "2025-04-23 14:24: Train Epoch 55: 20/37 Loss: 0.011693\n",
      "2025-04-23 14:24: Train Epoch 55: 20/37 Loss: 0.011693\n",
      "2025-04-23 14:24: **********Train Epoch 55: averaged Loss: 0.062374\n",
      "2025-04-23 14:24: **********Train Epoch 55: averaged Loss: 0.062374\n",
      "2025-04-23 14:24: **********Val Epoch 55: average Loss: 0.100481\n",
      "2025-04-23 14:24: **********Val Epoch 55: average Loss: 0.100481\n",
      "2025-04-23 14:24: Train Epoch 56: 0/37 Loss: 0.209310\n",
      "2025-04-23 14:24: Train Epoch 56: 0/37 Loss: 0.209310\n",
      "2025-04-23 14:24: Train Epoch 56: 20/37 Loss: 0.015215\n",
      "2025-04-23 14:24: Train Epoch 56: 20/37 Loss: 0.015215\n",
      "2025-04-23 14:24: **********Train Epoch 56: averaged Loss: 0.079330\n",
      "2025-04-23 14:24: **********Train Epoch 56: averaged Loss: 0.079330\n",
      "2025-04-23 14:24: **********Val Epoch 56: average Loss: 0.096207\n",
      "2025-04-23 14:24: **********Val Epoch 56: average Loss: 0.096207\n",
      "2025-04-23 14:24: Train Epoch 57: 0/37 Loss: 0.114434\n",
      "2025-04-23 14:24: Train Epoch 57: 0/37 Loss: 0.114434\n",
      "2025-04-23 14:24: Train Epoch 57: 20/37 Loss: 0.018680\n",
      "2025-04-23 14:24: Train Epoch 57: 20/37 Loss: 0.018680\n",
      "2025-04-23 14:24: **********Train Epoch 57: averaged Loss: 0.050085\n",
      "2025-04-23 14:24: **********Train Epoch 57: averaged Loss: 0.050085\n",
      "2025-04-23 14:24: **********Val Epoch 57: average Loss: 0.106597\n",
      "2025-04-23 14:24: **********Val Epoch 57: average Loss: 0.106597\n",
      "2025-04-23 14:24: Train Epoch 58: 0/37 Loss: 0.197355\n",
      "2025-04-23 14:24: Train Epoch 58: 0/37 Loss: 0.197355\n",
      "2025-04-23 14:24: Train Epoch 58: 20/37 Loss: 0.025854\n",
      "2025-04-23 14:24: Train Epoch 58: 20/37 Loss: 0.025854\n",
      "2025-04-23 14:24: **********Train Epoch 58: averaged Loss: 0.064806\n",
      "2025-04-23 14:24: **********Train Epoch 58: averaged Loss: 0.064806\n",
      "2025-04-23 14:24: **********Val Epoch 58: average Loss: 0.067908\n",
      "2025-04-23 14:24: **********Val Epoch 58: average Loss: 0.067908\n",
      "2025-04-23 14:24: Train Epoch 59: 0/37 Loss: 0.275188\n",
      "2025-04-23 14:24: Train Epoch 59: 0/37 Loss: 0.275188\n",
      "2025-04-23 14:24: Train Epoch 59: 20/37 Loss: 0.066099\n",
      "2025-04-23 14:24: Train Epoch 59: 20/37 Loss: 0.066099\n",
      "2025-04-23 14:24: **********Train Epoch 59: averaged Loss: 0.096208\n",
      "2025-04-23 14:24: **********Train Epoch 59: averaged Loss: 0.096208\n",
      "2025-04-23 14:24: **********Val Epoch 59: average Loss: 0.069577\n",
      "2025-04-23 14:24: **********Val Epoch 59: average Loss: 0.069577\n",
      "2025-04-23 14:24: Train Epoch 60: 0/37 Loss: 0.244201\n",
      "2025-04-23 14:24: Train Epoch 60: 0/37 Loss: 0.244201\n",
      "2025-04-23 14:24: Train Epoch 60: 20/37 Loss: 0.066133\n",
      "2025-04-23 14:24: Train Epoch 60: 20/37 Loss: 0.066133\n",
      "2025-04-23 14:24: **********Train Epoch 60: averaged Loss: 0.082980\n",
      "2025-04-23 14:24: **********Train Epoch 60: averaged Loss: 0.082980\n",
      "2025-04-23 14:24: **********Val Epoch 60: average Loss: 0.087974\n",
      "2025-04-23 14:24: **********Val Epoch 60: average Loss: 0.087974\n",
      "2025-04-23 14:24: Train Epoch 61: 0/37 Loss: 0.350023\n",
      "2025-04-23 14:24: Train Epoch 61: 0/37 Loss: 0.350023\n",
      "2025-04-23 14:24: Train Epoch 61: 20/37 Loss: 0.145757\n",
      "2025-04-23 14:24: Train Epoch 61: 20/37 Loss: 0.145757\n",
      "2025-04-23 14:24: **********Train Epoch 61: averaged Loss: 0.124505\n",
      "2025-04-23 14:24: **********Train Epoch 61: averaged Loss: 0.124505\n",
      "2025-04-23 14:24: **********Val Epoch 61: average Loss: 0.019591\n",
      "2025-04-23 14:24: **********Val Epoch 61: average Loss: 0.019591\n",
      "2025-04-23 14:24: Train Epoch 62: 0/37 Loss: 0.262163\n",
      "2025-04-23 14:24: Train Epoch 62: 0/37 Loss: 0.262163\n",
      "2025-04-23 14:24: Train Epoch 62: 20/37 Loss: 0.110287\n",
      "2025-04-23 14:24: Train Epoch 62: 20/37 Loss: 0.110287\n",
      "2025-04-23 14:24: **********Train Epoch 62: averaged Loss: 0.091599\n",
      "2025-04-23 14:24: **********Train Epoch 62: averaged Loss: 0.091599\n",
      "2025-04-23 14:24: **********Val Epoch 62: average Loss: 0.089592\n",
      "2025-04-23 14:24: **********Val Epoch 62: average Loss: 0.089592\n",
      "2025-04-23 14:24: Train Epoch 63: 0/37 Loss: 0.241519\n",
      "2025-04-23 14:24: Train Epoch 63: 0/37 Loss: 0.241519\n",
      "2025-04-23 14:24: Train Epoch 63: 20/37 Loss: 0.027058\n",
      "2025-04-23 14:24: Train Epoch 63: 20/37 Loss: 0.027058\n",
      "2025-04-23 14:24: **********Train Epoch 63: averaged Loss: 0.079944\n",
      "2025-04-23 14:24: **********Train Epoch 63: averaged Loss: 0.079944\n",
      "2025-04-23 14:24: **********Val Epoch 63: average Loss: 0.087133\n",
      "2025-04-23 14:24: **********Val Epoch 63: average Loss: 0.087133\n",
      "2025-04-23 14:24: Train Epoch 64: 0/37 Loss: 0.124708\n",
      "2025-04-23 14:24: Train Epoch 64: 0/37 Loss: 0.124708\n",
      "2025-04-23 14:24: Train Epoch 64: 20/37 Loss: 0.007173\n",
      "2025-04-23 14:24: Train Epoch 64: 20/37 Loss: 0.007173\n",
      "2025-04-23 14:24: **********Train Epoch 64: averaged Loss: 0.055101\n",
      "2025-04-23 14:24: **********Train Epoch 64: averaged Loss: 0.055101\n",
      "2025-04-23 14:24: **********Val Epoch 64: average Loss: 0.144703\n",
      "2025-04-23 14:24: **********Val Epoch 64: average Loss: 0.144703\n",
      "2025-04-23 14:24: Train Epoch 65: 0/37 Loss: 0.050907\n",
      "2025-04-23 14:24: Train Epoch 65: 0/37 Loss: 0.050907\n",
      "2025-04-23 14:24: Train Epoch 65: 20/37 Loss: 0.039423\n",
      "2025-04-23 14:24: Train Epoch 65: 20/37 Loss: 0.039423\n",
      "2025-04-23 14:24: **********Train Epoch 65: averaged Loss: 0.029910\n",
      "2025-04-23 14:24: **********Train Epoch 65: averaged Loss: 0.029910\n",
      "2025-04-23 14:24: **********Val Epoch 65: average Loss: 0.052832\n",
      "2025-04-23 14:24: **********Val Epoch 65: average Loss: 0.052832\n",
      "2025-04-23 14:24: Train Epoch 66: 0/37 Loss: 0.180289\n",
      "2025-04-23 14:24: Train Epoch 66: 0/37 Loss: 0.180289\n",
      "2025-04-23 14:24: Train Epoch 66: 20/37 Loss: 0.020187\n",
      "2025-04-23 14:24: Train Epoch 66: 20/37 Loss: 0.020187\n",
      "2025-04-23 14:24: **********Train Epoch 66: averaged Loss: 0.052515\n",
      "2025-04-23 14:24: **********Train Epoch 66: averaged Loss: 0.052515\n",
      "2025-04-23 14:24: **********Val Epoch 66: average Loss: 0.040088\n",
      "2025-04-23 14:24: **********Val Epoch 66: average Loss: 0.040088\n",
      "2025-04-23 14:24: Train Epoch 67: 0/37 Loss: 0.137177\n",
      "2025-04-23 14:24: Train Epoch 67: 0/37 Loss: 0.137177\n",
      "2025-04-23 14:24: Train Epoch 67: 20/37 Loss: 0.007490\n",
      "2025-04-23 14:24: Train Epoch 67: 20/37 Loss: 0.007490\n",
      "2025-04-23 14:24: **********Train Epoch 67: averaged Loss: 0.052921\n",
      "2025-04-23 14:24: **********Train Epoch 67: averaged Loss: 0.052921\n",
      "2025-04-23 14:24: **********Val Epoch 67: average Loss: 0.088939\n",
      "2025-04-23 14:24: **********Val Epoch 67: average Loss: 0.088939\n",
      "2025-04-23 14:24: Train Epoch 68: 0/37 Loss: 0.174571\n",
      "2025-04-23 14:24: Train Epoch 68: 0/37 Loss: 0.174571\n",
      "2025-04-23 14:24: Train Epoch 68: 20/37 Loss: 0.051303\n",
      "2025-04-23 14:24: Train Epoch 68: 20/37 Loss: 0.051303\n",
      "2025-04-23 14:24: **********Train Epoch 68: averaged Loss: 0.056247\n",
      "2025-04-23 14:24: **********Train Epoch 68: averaged Loss: 0.056247\n",
      "2025-04-23 14:24: **********Val Epoch 68: average Loss: 0.041829\n",
      "2025-04-23 14:24: **********Val Epoch 68: average Loss: 0.041829\n",
      "2025-04-23 14:24: Train Epoch 69: 0/37 Loss: 0.075574\n",
      "2025-04-23 14:24: Train Epoch 69: 0/37 Loss: 0.075574\n",
      "2025-04-23 14:24: Train Epoch 69: 20/37 Loss: 0.012111\n",
      "2025-04-23 14:24: Train Epoch 69: 20/37 Loss: 0.012111\n",
      "2025-04-23 14:24: **********Train Epoch 69: averaged Loss: 0.050980\n",
      "2025-04-23 14:24: **********Train Epoch 69: averaged Loss: 0.050980\n",
      "2025-04-23 14:24: **********Val Epoch 69: average Loss: 0.046758\n",
      "2025-04-23 14:24: **********Val Epoch 69: average Loss: 0.046758\n",
      "2025-04-23 14:24: Train Epoch 70: 0/37 Loss: 0.068619\n",
      "2025-04-23 14:24: Train Epoch 70: 0/37 Loss: 0.068619\n",
      "2025-04-23 14:24: Train Epoch 70: 20/37 Loss: 0.038370\n",
      "2025-04-23 14:24: Train Epoch 70: 20/37 Loss: 0.038370\n",
      "2025-04-23 14:24: **********Train Epoch 70: averaged Loss: 0.028696\n",
      "2025-04-23 14:24: **********Train Epoch 70: averaged Loss: 0.028696\n",
      "2025-04-23 14:24: **********Val Epoch 70: average Loss: 0.056671\n",
      "2025-04-23 14:24: **********Val Epoch 70: average Loss: 0.056671\n",
      "2025-04-23 14:24: Train Epoch 71: 0/37 Loss: 0.042133\n",
      "2025-04-23 14:24: Train Epoch 71: 0/37 Loss: 0.042133\n",
      "2025-04-23 14:24: Train Epoch 71: 20/37 Loss: 0.020249\n",
      "2025-04-23 14:24: Train Epoch 71: 20/37 Loss: 0.020249\n",
      "2025-04-23 14:24: **********Train Epoch 71: averaged Loss: 0.026182\n",
      "2025-04-23 14:24: **********Train Epoch 71: averaged Loss: 0.026182\n",
      "2025-04-23 14:24: **********Val Epoch 71: average Loss: 0.075038\n",
      "2025-04-23 14:24: **********Val Epoch 71: average Loss: 0.075038\n",
      "2025-04-23 14:24: Train Epoch 72: 0/37 Loss: 0.103617\n",
      "2025-04-23 14:24: Train Epoch 72: 0/37 Loss: 0.103617\n",
      "2025-04-23 14:24: Train Epoch 72: 20/37 Loss: 0.006869\n",
      "2025-04-23 14:24: Train Epoch 72: 20/37 Loss: 0.006869\n",
      "2025-04-23 14:24: **********Train Epoch 72: averaged Loss: 0.035442\n",
      "2025-04-23 14:24: **********Train Epoch 72: averaged Loss: 0.035442\n",
      "2025-04-23 14:24: **********Val Epoch 72: average Loss: 0.066104\n",
      "2025-04-23 14:24: **********Val Epoch 72: average Loss: 0.066104\n",
      "2025-04-23 14:24: Train Epoch 73: 0/37 Loss: 0.055972\n",
      "2025-04-23 14:24: Train Epoch 73: 0/37 Loss: 0.055972\n",
      "2025-04-23 14:24: Train Epoch 73: 20/37 Loss: 0.008071\n",
      "2025-04-23 14:24: Train Epoch 73: 20/37 Loss: 0.008071\n",
      "2025-04-23 14:24: **********Train Epoch 73: averaged Loss: 0.050897\n",
      "2025-04-23 14:24: **********Train Epoch 73: averaged Loss: 0.050897\n",
      "2025-04-23 14:24: **********Val Epoch 73: average Loss: 0.106536\n",
      "2025-04-23 14:24: **********Val Epoch 73: average Loss: 0.106536\n",
      "2025-04-23 14:24: Train Epoch 74: 0/37 Loss: 0.069884\n",
      "2025-04-23 14:24: Train Epoch 74: 0/37 Loss: 0.069884\n",
      "2025-04-23 14:24: Train Epoch 74: 20/37 Loss: 0.013832\n",
      "2025-04-23 14:24: Train Epoch 74: 20/37 Loss: 0.013832\n",
      "2025-04-23 14:24: **********Train Epoch 74: averaged Loss: 0.030988\n",
      "2025-04-23 14:24: **********Train Epoch 74: averaged Loss: 0.030988\n",
      "2025-04-23 14:24: **********Val Epoch 74: average Loss: 0.113166\n",
      "2025-04-23 14:24: **********Val Epoch 74: average Loss: 0.113166\n",
      "2025-04-23 14:24: Train Epoch 75: 0/37 Loss: 0.151102\n",
      "2025-04-23 14:24: Train Epoch 75: 0/37 Loss: 0.151102\n",
      "2025-04-23 14:24: Train Epoch 75: 20/37 Loss: 0.045939\n",
      "2025-04-23 14:24: Train Epoch 75: 20/37 Loss: 0.045939\n",
      "2025-04-23 14:24: **********Train Epoch 75: averaged Loss: 0.046757\n",
      "2025-04-23 14:24: **********Train Epoch 75: averaged Loss: 0.046757\n",
      "2025-04-23 14:24: **********Val Epoch 75: average Loss: 0.101106\n",
      "2025-04-23 14:24: **********Val Epoch 75: average Loss: 0.101106\n",
      "2025-04-23 14:24: Train Epoch 76: 0/37 Loss: 0.004769\n",
      "2025-04-23 14:24: Train Epoch 76: 0/37 Loss: 0.004769\n",
      "2025-04-23 14:24: Train Epoch 76: 20/37 Loss: 0.016988\n",
      "2025-04-23 14:24: Train Epoch 76: 20/37 Loss: 0.016988\n",
      "2025-04-23 14:24: **********Train Epoch 76: averaged Loss: 0.047621\n",
      "2025-04-23 14:24: **********Train Epoch 76: averaged Loss: 0.047621\n",
      "2025-04-23 14:24: **********Val Epoch 76: average Loss: 0.241730\n",
      "2025-04-23 14:24: **********Val Epoch 76: average Loss: 0.241730\n",
      "2025-04-23 14:24: Train Epoch 77: 0/37 Loss: 0.101326\n",
      "2025-04-23 14:24: Train Epoch 77: 0/37 Loss: 0.101326\n",
      "2025-04-23 14:24: Train Epoch 77: 20/37 Loss: 0.072469\n",
      "2025-04-23 14:24: Train Epoch 77: 20/37 Loss: 0.072469\n",
      "2025-04-23 14:24: **********Train Epoch 77: averaged Loss: 0.048849\n",
      "2025-04-23 14:24: **********Train Epoch 77: averaged Loss: 0.048849\n",
      "2025-04-23 14:24: **********Val Epoch 77: average Loss: 0.051469\n",
      "2025-04-23 14:24: **********Val Epoch 77: average Loss: 0.051469\n",
      "2025-04-23 14:24: Train Epoch 78: 0/37 Loss: 0.380863\n",
      "2025-04-23 14:24: Train Epoch 78: 0/37 Loss: 0.380863\n",
      "2025-04-23 14:24: Train Epoch 78: 20/37 Loss: 0.088733\n",
      "2025-04-23 14:24: Train Epoch 78: 20/37 Loss: 0.088733\n",
      "2025-04-23 14:24: **********Train Epoch 78: averaged Loss: 0.144066\n",
      "2025-04-23 14:24: **********Train Epoch 78: averaged Loss: 0.144066\n",
      "2025-04-23 14:24: **********Val Epoch 78: average Loss: 0.018749\n",
      "2025-04-23 14:24: **********Val Epoch 78: average Loss: 0.018749\n",
      "2025-04-23 14:24: Train Epoch 79: 0/37 Loss: 0.051067\n",
      "2025-04-23 14:24: Train Epoch 79: 0/37 Loss: 0.051067\n",
      "2025-04-23 14:24: Train Epoch 79: 20/37 Loss: 0.027234\n",
      "2025-04-23 14:24: Train Epoch 79: 20/37 Loss: 0.027234\n",
      "2025-04-23 14:24: **********Train Epoch 79: averaged Loss: 0.031794\n",
      "2025-04-23 14:24: **********Train Epoch 79: averaged Loss: 0.031794\n",
      "2025-04-23 14:24: **********Val Epoch 79: average Loss: 0.061663\n",
      "2025-04-23 14:24: **********Val Epoch 79: average Loss: 0.061663\n",
      "2025-04-23 14:24: Train Epoch 80: 0/37 Loss: 0.053348\n",
      "2025-04-23 14:24: Train Epoch 80: 0/37 Loss: 0.053348\n",
      "2025-04-23 14:25: Train Epoch 80: 20/37 Loss: 0.024824\n",
      "2025-04-23 14:25: Train Epoch 80: 20/37 Loss: 0.024824\n",
      "2025-04-23 14:25: **********Train Epoch 80: averaged Loss: 0.024373\n",
      "2025-04-23 14:25: **********Train Epoch 80: averaged Loss: 0.024373\n",
      "2025-04-23 14:25: **********Val Epoch 80: average Loss: 0.087916\n",
      "2025-04-23 14:25: **********Val Epoch 80: average Loss: 0.087916\n",
      "2025-04-23 14:25: Train Epoch 81: 0/37 Loss: 0.028911\n",
      "2025-04-23 14:25: Train Epoch 81: 0/37 Loss: 0.028911\n",
      "2025-04-23 14:25: Train Epoch 81: 20/37 Loss: 0.010520\n",
      "2025-04-23 14:25: Train Epoch 81: 20/37 Loss: 0.010520\n",
      "2025-04-23 14:25: **********Train Epoch 81: averaged Loss: 0.046298\n",
      "2025-04-23 14:25: **********Train Epoch 81: averaged Loss: 0.046298\n",
      "2025-04-23 14:25: **********Val Epoch 81: average Loss: 0.176125\n",
      "2025-04-23 14:25: **********Val Epoch 81: average Loss: 0.176125\n",
      "2025-04-23 14:25: Train Epoch 82: 0/37 Loss: 0.143453\n",
      "2025-04-23 14:25: Train Epoch 82: 0/37 Loss: 0.143453\n",
      "2025-04-23 14:25: Train Epoch 82: 20/37 Loss: 0.079244\n",
      "2025-04-23 14:25: Train Epoch 82: 20/37 Loss: 0.079244\n",
      "2025-04-23 14:25: **********Train Epoch 82: averaged Loss: 0.055692\n",
      "2025-04-23 14:25: **********Train Epoch 82: averaged Loss: 0.055692\n",
      "2025-04-23 14:25: **********Val Epoch 82: average Loss: 0.097233\n",
      "2025-04-23 14:25: **********Val Epoch 82: average Loss: 0.097233\n",
      "2025-04-23 14:25: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:25: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:25: Total training time: 2.2890min, best loss: 0.015154\n",
      "2025-04-23 14:25: Total training time: 2.2890min, best loss: 0.015154\n",
      "2025-04-23 14:25: Average Horizon, MAE: 0.1600, MSE: 0.1607\n",
      "2025-04-23 14:25: Average Horizon, MAE: 0.1600, MSE: 0.1607\n",
      "2025-04-23 14:25: Average Horizon, MAE: 0.1630, MSE: 0.1637\n",
      "2025-04-23 14:25: Average Horizon, MAE: 0.1630, MSE: 0.1637\n",
      "2025-04-23 14:25: Experiment log path in: ./\n",
      "2025-04-23 14:25: Experiment log path in: ./\n",
      "2025-04-23 14:25: Experiment log path in: ./\n",
      "2025-04-23 14:25: Train Epoch 1: 0/37 Loss: 0.054694\n",
      "2025-04-23 14:25: Train Epoch 1: 0/37 Loss: 0.054694\n",
      "2025-04-23 14:25: Train Epoch 1: 0/37 Loss: 0.054694\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "graphsage.fc.weight torch.Size([1, 10]) True\n",
      "graphsage.fc.bias torch.Size([1]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240663\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:25: Train Epoch 1: 20/37 Loss: 0.023804\n",
      "2025-04-23 14:25: Train Epoch 1: 20/37 Loss: 0.023804\n",
      "2025-04-23 14:25: Train Epoch 1: 20/37 Loss: 0.023804\n",
      "2025-04-23 14:25: **********Train Epoch 1: averaged Loss: 0.098363\n",
      "2025-04-23 14:25: **********Train Epoch 1: averaged Loss: 0.098363\n",
      "2025-04-23 14:25: **********Train Epoch 1: averaged Loss: 0.098363\n",
      "2025-04-23 14:25: **********Val Epoch 1: average Loss: 0.088138\n",
      "2025-04-23 14:25: **********Val Epoch 1: average Loss: 0.088138\n",
      "2025-04-23 14:25: **********Val Epoch 1: average Loss: 0.088138\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: Train Epoch 2: 0/37 Loss: 0.765471\n",
      "2025-04-23 14:25: Train Epoch 2: 0/37 Loss: 0.765471\n",
      "2025-04-23 14:25: Train Epoch 2: 0/37 Loss: 0.765471\n",
      "2025-04-23 14:25: Train Epoch 2: 20/37 Loss: 0.025267\n",
      "2025-04-23 14:25: Train Epoch 2: 20/37 Loss: 0.025267\n",
      "2025-04-23 14:25: Train Epoch 2: 20/37 Loss: 0.025267\n",
      "2025-04-23 14:25: **********Train Epoch 2: averaged Loss: 0.189522\n",
      "2025-04-23 14:25: **********Train Epoch 2: averaged Loss: 0.189522\n",
      "2025-04-23 14:25: **********Train Epoch 2: averaged Loss: 0.189522\n",
      "2025-04-23 14:25: **********Val Epoch 2: average Loss: 0.106435\n",
      "2025-04-23 14:25: **********Val Epoch 2: average Loss: 0.106435\n",
      "2025-04-23 14:25: **********Val Epoch 2: average Loss: 0.106435\n",
      "2025-04-23 14:25: Train Epoch 3: 0/37 Loss: 0.687295\n",
      "2025-04-23 14:25: Train Epoch 3: 0/37 Loss: 0.687295\n",
      "2025-04-23 14:25: Train Epoch 3: 0/37 Loss: 0.687295\n",
      "2025-04-23 14:25: Train Epoch 3: 20/37 Loss: 0.211763\n",
      "2025-04-23 14:25: Train Epoch 3: 20/37 Loss: 0.211763\n",
      "2025-04-23 14:25: Train Epoch 3: 20/37 Loss: 0.211763\n",
      "2025-04-23 14:25: **********Train Epoch 3: averaged Loss: 0.289678\n",
      "2025-04-23 14:25: **********Train Epoch 3: averaged Loss: 0.289678\n",
      "2025-04-23 14:25: **********Train Epoch 3: averaged Loss: 0.289678\n",
      "2025-04-23 14:25: **********Val Epoch 3: average Loss: 0.042155\n",
      "2025-04-23 14:25: **********Val Epoch 3: average Loss: 0.042155\n",
      "2025-04-23 14:25: **********Val Epoch 3: average Loss: 0.042155\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: Train Epoch 4: 0/37 Loss: 0.427886\n",
      "2025-04-23 14:25: Train Epoch 4: 0/37 Loss: 0.427886\n",
      "2025-04-23 14:25: Train Epoch 4: 0/37 Loss: 0.427886\n",
      "2025-04-23 14:25: Train Epoch 4: 20/37 Loss: 0.420161\n",
      "2025-04-23 14:25: Train Epoch 4: 20/37 Loss: 0.420161\n",
      "2025-04-23 14:25: Train Epoch 4: 20/37 Loss: 0.420161\n",
      "2025-04-23 14:25: **********Train Epoch 4: averaged Loss: 0.309446\n",
      "2025-04-23 14:25: **********Train Epoch 4: averaged Loss: 0.309446\n",
      "2025-04-23 14:25: **********Train Epoch 4: averaged Loss: 0.309446\n",
      "2025-04-23 14:25: **********Val Epoch 4: average Loss: 0.141393\n",
      "2025-04-23 14:25: **********Val Epoch 4: average Loss: 0.141393\n",
      "2025-04-23 14:25: **********Val Epoch 4: average Loss: 0.141393\n",
      "2025-04-23 14:25: Train Epoch 5: 0/37 Loss: 0.482219\n",
      "2025-04-23 14:25: Train Epoch 5: 0/37 Loss: 0.482219\n",
      "2025-04-23 14:25: Train Epoch 5: 0/37 Loss: 0.482219\n",
      "2025-04-23 14:25: Train Epoch 5: 20/37 Loss: 0.296846\n",
      "2025-04-23 14:25: Train Epoch 5: 20/37 Loss: 0.296846\n",
      "2025-04-23 14:25: Train Epoch 5: 20/37 Loss: 0.296846\n",
      "2025-04-23 14:25: **********Train Epoch 5: averaged Loss: 0.320232\n",
      "2025-04-23 14:25: **********Train Epoch 5: averaged Loss: 0.320232\n",
      "2025-04-23 14:25: **********Train Epoch 5: averaged Loss: 0.320232\n",
      "2025-04-23 14:25: **********Val Epoch 5: average Loss: 0.245969\n",
      "2025-04-23 14:25: **********Val Epoch 5: average Loss: 0.245969\n",
      "2025-04-23 14:25: **********Val Epoch 5: average Loss: 0.245969\n",
      "2025-04-23 14:25: Train Epoch 6: 0/37 Loss: 0.426559\n",
      "2025-04-23 14:25: Train Epoch 6: 0/37 Loss: 0.426559\n",
      "2025-04-23 14:25: Train Epoch 6: 0/37 Loss: 0.426559\n",
      "2025-04-23 14:25: Train Epoch 6: 20/37 Loss: 0.172322\n",
      "2025-04-23 14:25: Train Epoch 6: 20/37 Loss: 0.172322\n",
      "2025-04-23 14:25: Train Epoch 6: 20/37 Loss: 0.172322\n",
      "2025-04-23 14:25: **********Train Epoch 6: averaged Loss: 0.262346\n",
      "2025-04-23 14:25: **********Train Epoch 6: averaged Loss: 0.262346\n",
      "2025-04-23 14:25: **********Train Epoch 6: averaged Loss: 0.262346\n",
      "2025-04-23 14:25: **********Val Epoch 6: average Loss: 0.210407\n",
      "2025-04-23 14:25: **********Val Epoch 6: average Loss: 0.210407\n",
      "2025-04-23 14:25: **********Val Epoch 6: average Loss: 0.210407\n",
      "2025-04-23 14:25: Train Epoch 7: 0/37 Loss: 0.342784\n",
      "2025-04-23 14:25: Train Epoch 7: 0/37 Loss: 0.342784\n",
      "2025-04-23 14:25: Train Epoch 7: 0/37 Loss: 0.342784\n",
      "2025-04-23 14:25: Train Epoch 7: 20/37 Loss: 0.108385\n",
      "2025-04-23 14:25: Train Epoch 7: 20/37 Loss: 0.108385\n",
      "2025-04-23 14:25: Train Epoch 7: 20/37 Loss: 0.108385\n",
      "2025-04-23 14:25: **********Train Epoch 7: averaged Loss: 0.231791\n",
      "2025-04-23 14:25: **********Train Epoch 7: averaged Loss: 0.231791\n",
      "2025-04-23 14:25: **********Train Epoch 7: averaged Loss: 0.231791\n",
      "2025-04-23 14:25: **********Val Epoch 7: average Loss: 0.216419\n",
      "2025-04-23 14:25: **********Val Epoch 7: average Loss: 0.216419\n",
      "2025-04-23 14:25: **********Val Epoch 7: average Loss: 0.216419\n",
      "2025-04-23 14:25: Train Epoch 8: 0/37 Loss: 0.377811\n",
      "2025-04-23 14:25: Train Epoch 8: 0/37 Loss: 0.377811\n",
      "2025-04-23 14:25: Train Epoch 8: 0/37 Loss: 0.377811\n",
      "2025-04-23 14:25: Train Epoch 8: 20/37 Loss: 0.128786\n",
      "2025-04-23 14:25: Train Epoch 8: 20/37 Loss: 0.128786\n",
      "2025-04-23 14:25: Train Epoch 8: 20/37 Loss: 0.128786\n",
      "2025-04-23 14:25: **********Train Epoch 8: averaged Loss: 0.254416\n",
      "2025-04-23 14:25: **********Train Epoch 8: averaged Loss: 0.254416\n",
      "2025-04-23 14:25: **********Train Epoch 8: averaged Loss: 0.254416\n",
      "2025-04-23 14:25: **********Val Epoch 8: average Loss: 0.241283\n",
      "2025-04-23 14:25: **********Val Epoch 8: average Loss: 0.241283\n",
      "2025-04-23 14:25: **********Val Epoch 8: average Loss: 0.241283\n",
      "2025-04-23 14:25: Train Epoch 9: 0/37 Loss: 0.367930\n",
      "2025-04-23 14:25: Train Epoch 9: 0/37 Loss: 0.367930\n",
      "2025-04-23 14:25: Train Epoch 9: 0/37 Loss: 0.367930\n",
      "2025-04-23 14:25: Train Epoch 9: 20/37 Loss: 0.100199\n",
      "2025-04-23 14:25: Train Epoch 9: 20/37 Loss: 0.100199\n",
      "2025-04-23 14:25: Train Epoch 9: 20/37 Loss: 0.100199\n",
      "2025-04-23 14:25: **********Train Epoch 9: averaged Loss: 0.179724\n",
      "2025-04-23 14:25: **********Train Epoch 9: averaged Loss: 0.179724\n",
      "2025-04-23 14:25: **********Train Epoch 9: averaged Loss: 0.179724\n",
      "2025-04-23 14:25: **********Val Epoch 9: average Loss: 0.046759\n",
      "2025-04-23 14:25: **********Val Epoch 9: average Loss: 0.046759\n",
      "2025-04-23 14:25: **********Val Epoch 9: average Loss: 0.046759\n",
      "2025-04-23 14:25: Train Epoch 10: 0/37 Loss: 0.597837\n",
      "2025-04-23 14:25: Train Epoch 10: 0/37 Loss: 0.597837\n",
      "2025-04-23 14:25: Train Epoch 10: 0/37 Loss: 0.597837\n",
      "2025-04-23 14:25: Train Epoch 10: 20/37 Loss: 0.216808\n",
      "2025-04-23 14:25: Train Epoch 10: 20/37 Loss: 0.216808\n",
      "2025-04-23 14:25: Train Epoch 10: 20/37 Loss: 0.216808\n",
      "2025-04-23 14:25: **********Train Epoch 10: averaged Loss: 0.252156\n",
      "2025-04-23 14:25: **********Train Epoch 10: averaged Loss: 0.252156\n",
      "2025-04-23 14:25: **********Train Epoch 10: averaged Loss: 0.252156\n",
      "2025-04-23 14:25: **********Val Epoch 10: average Loss: 0.140460\n",
      "2025-04-23 14:25: **********Val Epoch 10: average Loss: 0.140460\n",
      "2025-04-23 14:25: **********Val Epoch 10: average Loss: 0.140460\n",
      "2025-04-23 14:25: Train Epoch 11: 0/37 Loss: 0.261776\n",
      "2025-04-23 14:25: Train Epoch 11: 0/37 Loss: 0.261776\n",
      "2025-04-23 14:25: Train Epoch 11: 0/37 Loss: 0.261776\n",
      "2025-04-23 14:25: Train Epoch 11: 20/37 Loss: 0.078946\n",
      "2025-04-23 14:25: Train Epoch 11: 20/37 Loss: 0.078946\n",
      "2025-04-23 14:25: Train Epoch 11: 20/37 Loss: 0.078946\n",
      "2025-04-23 14:25: **********Train Epoch 11: averaged Loss: 0.127652\n",
      "2025-04-23 14:25: **********Train Epoch 11: averaged Loss: 0.127652\n",
      "2025-04-23 14:25: **********Train Epoch 11: averaged Loss: 0.127652\n",
      "2025-04-23 14:25: **********Val Epoch 11: average Loss: 0.124728\n",
      "2025-04-23 14:25: **********Val Epoch 11: average Loss: 0.124728\n",
      "2025-04-23 14:25: **********Val Epoch 11: average Loss: 0.124728\n",
      "2025-04-23 14:25: Train Epoch 12: 0/37 Loss: 0.195775\n",
      "2025-04-23 14:25: Train Epoch 12: 0/37 Loss: 0.195775\n",
      "2025-04-23 14:25: Train Epoch 12: 0/37 Loss: 0.195775\n",
      "2025-04-23 14:25: Train Epoch 12: 20/37 Loss: 0.112852\n",
      "2025-04-23 14:25: Train Epoch 12: 20/37 Loss: 0.112852\n",
      "2025-04-23 14:25: Train Epoch 12: 20/37 Loss: 0.112852\n",
      "2025-04-23 14:25: **********Train Epoch 12: averaged Loss: 0.097988\n",
      "2025-04-23 14:25: **********Train Epoch 12: averaged Loss: 0.097988\n",
      "2025-04-23 14:25: **********Train Epoch 12: averaged Loss: 0.097988\n",
      "2025-04-23 14:25: **********Val Epoch 12: average Loss: 0.127675\n",
      "2025-04-23 14:25: **********Val Epoch 12: average Loss: 0.127675\n",
      "2025-04-23 14:25: **********Val Epoch 12: average Loss: 0.127675\n",
      "2025-04-23 14:25: Train Epoch 13: 0/37 Loss: 0.435872\n",
      "2025-04-23 14:25: Train Epoch 13: 0/37 Loss: 0.435872\n",
      "2025-04-23 14:25: Train Epoch 13: 0/37 Loss: 0.435872\n",
      "2025-04-23 14:25: Train Epoch 13: 20/37 Loss: 0.135691\n",
      "2025-04-23 14:25: Train Epoch 13: 20/37 Loss: 0.135691\n",
      "2025-04-23 14:25: Train Epoch 13: 20/37 Loss: 0.135691\n",
      "2025-04-23 14:25: **********Train Epoch 13: averaged Loss: 0.165242\n",
      "2025-04-23 14:25: **********Train Epoch 13: averaged Loss: 0.165242\n",
      "2025-04-23 14:25: **********Train Epoch 13: averaged Loss: 0.165242\n",
      "2025-04-23 14:25: **********Val Epoch 13: average Loss: 0.092733\n",
      "2025-04-23 14:25: **********Val Epoch 13: average Loss: 0.092733\n",
      "2025-04-23 14:25: **********Val Epoch 13: average Loss: 0.092733\n",
      "2025-04-23 14:25: Train Epoch 14: 0/37 Loss: 0.103008\n",
      "2025-04-23 14:25: Train Epoch 14: 0/37 Loss: 0.103008\n",
      "2025-04-23 14:25: Train Epoch 14: 0/37 Loss: 0.103008\n",
      "2025-04-23 14:25: Train Epoch 14: 20/37 Loss: 0.060333\n",
      "2025-04-23 14:25: Train Epoch 14: 20/37 Loss: 0.060333\n",
      "2025-04-23 14:25: Train Epoch 14: 20/37 Loss: 0.060333\n",
      "2025-04-23 14:25: **********Train Epoch 14: averaged Loss: 0.094878\n",
      "2025-04-23 14:25: **********Train Epoch 14: averaged Loss: 0.094878\n",
      "2025-04-23 14:25: **********Train Epoch 14: averaged Loss: 0.094878\n",
      "2025-04-23 14:25: **********Val Epoch 14: average Loss: 0.058722\n",
      "2025-04-23 14:25: **********Val Epoch 14: average Loss: 0.058722\n",
      "2025-04-23 14:25: **********Val Epoch 14: average Loss: 0.058722\n",
      "2025-04-23 14:25: Train Epoch 15: 0/37 Loss: 0.004414\n",
      "2025-04-23 14:25: Train Epoch 15: 0/37 Loss: 0.004414\n",
      "2025-04-23 14:25: Train Epoch 15: 0/37 Loss: 0.004414\n",
      "2025-04-23 14:25: Train Epoch 15: 20/37 Loss: 0.017657\n",
      "2025-04-23 14:25: Train Epoch 15: 20/37 Loss: 0.017657\n",
      "2025-04-23 14:25: Train Epoch 15: 20/37 Loss: 0.017657\n",
      "2025-04-23 14:25: **********Train Epoch 15: averaged Loss: 0.041498\n",
      "2025-04-23 14:25: **********Train Epoch 15: averaged Loss: 0.041498\n",
      "2025-04-23 14:25: **********Train Epoch 15: averaged Loss: 0.041498\n",
      "2025-04-23 14:25: **********Val Epoch 15: average Loss: 0.063650\n",
      "2025-04-23 14:25: **********Val Epoch 15: average Loss: 0.063650\n",
      "2025-04-23 14:25: **********Val Epoch 15: average Loss: 0.063650\n",
      "2025-04-23 14:25: Train Epoch 16: 0/37 Loss: 0.110674\n",
      "2025-04-23 14:25: Train Epoch 16: 0/37 Loss: 0.110674\n",
      "2025-04-23 14:25: Train Epoch 16: 0/37 Loss: 0.110674\n",
      "2025-04-23 14:25: Train Epoch 16: 20/37 Loss: 0.039970\n",
      "2025-04-23 14:25: Train Epoch 16: 20/37 Loss: 0.039970\n",
      "2025-04-23 14:25: Train Epoch 16: 20/37 Loss: 0.039970\n",
      "2025-04-23 14:25: **********Train Epoch 16: averaged Loss: 0.082938\n",
      "2025-04-23 14:25: **********Train Epoch 16: averaged Loss: 0.082938\n",
      "2025-04-23 14:25: **********Train Epoch 16: averaged Loss: 0.082938\n",
      "2025-04-23 14:25: **********Val Epoch 16: average Loss: 0.079906\n",
      "2025-04-23 14:25: **********Val Epoch 16: average Loss: 0.079906\n",
      "2025-04-23 14:25: **********Val Epoch 16: average Loss: 0.079906\n",
      "2025-04-23 14:25: Train Epoch 17: 0/37 Loss: 0.130744\n",
      "2025-04-23 14:25: Train Epoch 17: 0/37 Loss: 0.130744\n",
      "2025-04-23 14:25: Train Epoch 17: 0/37 Loss: 0.130744\n",
      "2025-04-23 14:25: Train Epoch 17: 20/37 Loss: 0.016417\n",
      "2025-04-23 14:25: Train Epoch 17: 20/37 Loss: 0.016417\n",
      "2025-04-23 14:25: Train Epoch 17: 20/37 Loss: 0.016417\n",
      "2025-04-23 14:25: **********Train Epoch 17: averaged Loss: 0.102158\n",
      "2025-04-23 14:25: **********Train Epoch 17: averaged Loss: 0.102158\n",
      "2025-04-23 14:25: **********Train Epoch 17: averaged Loss: 0.102158\n",
      "2025-04-23 14:25: **********Val Epoch 17: average Loss: 0.287834\n",
      "2025-04-23 14:25: **********Val Epoch 17: average Loss: 0.287834\n",
      "2025-04-23 14:25: **********Val Epoch 17: average Loss: 0.287834\n",
      "2025-04-23 14:25: Train Epoch 18: 0/37 Loss: 0.091678\n",
      "2025-04-23 14:25: Train Epoch 18: 0/37 Loss: 0.091678\n",
      "2025-04-23 14:25: Train Epoch 18: 0/37 Loss: 0.091678\n",
      "2025-04-23 14:25: Train Epoch 18: 20/37 Loss: 0.073845\n",
      "2025-04-23 14:25: Train Epoch 18: 20/37 Loss: 0.073845\n",
      "2025-04-23 14:25: Train Epoch 18: 20/37 Loss: 0.073845\n",
      "2025-04-23 14:25: **********Train Epoch 18: averaged Loss: 0.053713\n",
      "2025-04-23 14:25: **********Train Epoch 18: averaged Loss: 0.053713\n",
      "2025-04-23 14:25: **********Train Epoch 18: averaged Loss: 0.053713\n",
      "2025-04-23 14:25: **********Val Epoch 18: average Loss: 0.148108\n",
      "2025-04-23 14:25: **********Val Epoch 18: average Loss: 0.148108\n",
      "2025-04-23 14:25: **********Val Epoch 18: average Loss: 0.148108\n",
      "2025-04-23 14:25: Train Epoch 19: 0/37 Loss: 0.456502\n",
      "2025-04-23 14:25: Train Epoch 19: 0/37 Loss: 0.456502\n",
      "2025-04-23 14:25: Train Epoch 19: 0/37 Loss: 0.456502\n",
      "2025-04-23 14:25: Train Epoch 19: 20/37 Loss: 0.205057\n",
      "2025-04-23 14:25: Train Epoch 19: 20/37 Loss: 0.205057\n",
      "2025-04-23 14:25: Train Epoch 19: 20/37 Loss: 0.205057\n",
      "2025-04-23 14:25: **********Train Epoch 19: averaged Loss: 0.177417\n",
      "2025-04-23 14:25: **********Train Epoch 19: averaged Loss: 0.177417\n",
      "2025-04-23 14:25: **********Train Epoch 19: averaged Loss: 0.177417\n",
      "2025-04-23 14:25: **********Val Epoch 19: average Loss: 0.101633\n",
      "2025-04-23 14:25: **********Val Epoch 19: average Loss: 0.101633\n",
      "2025-04-23 14:25: **********Val Epoch 19: average Loss: 0.101633\n",
      "2025-04-23 14:25: Train Epoch 20: 0/37 Loss: 0.174335\n",
      "2025-04-23 14:25: Train Epoch 20: 0/37 Loss: 0.174335\n",
      "2025-04-23 14:25: Train Epoch 20: 0/37 Loss: 0.174335\n",
      "2025-04-23 14:25: Train Epoch 20: 20/37 Loss: 0.100514\n",
      "2025-04-23 14:25: Train Epoch 20: 20/37 Loss: 0.100514\n",
      "2025-04-23 14:25: Train Epoch 20: 20/37 Loss: 0.100514\n",
      "2025-04-23 14:25: **********Train Epoch 20: averaged Loss: 0.080800\n",
      "2025-04-23 14:25: **********Train Epoch 20: averaged Loss: 0.080800\n",
      "2025-04-23 14:25: **********Train Epoch 20: averaged Loss: 0.080800\n",
      "2025-04-23 14:25: **********Val Epoch 20: average Loss: 0.124386\n",
      "2025-04-23 14:25: **********Val Epoch 20: average Loss: 0.124386\n",
      "2025-04-23 14:25: **********Val Epoch 20: average Loss: 0.124386\n",
      "2025-04-23 14:25: Train Epoch 21: 0/37 Loss: 0.175203\n",
      "2025-04-23 14:25: Train Epoch 21: 0/37 Loss: 0.175203\n",
      "2025-04-23 14:25: Train Epoch 21: 0/37 Loss: 0.175203\n",
      "2025-04-23 14:25: Train Epoch 21: 20/37 Loss: 0.027301\n",
      "2025-04-23 14:25: Train Epoch 21: 20/37 Loss: 0.027301\n",
      "2025-04-23 14:25: Train Epoch 21: 20/37 Loss: 0.027301\n",
      "2025-04-23 14:25: **********Train Epoch 21: averaged Loss: 0.069218\n",
      "2025-04-23 14:25: **********Train Epoch 21: averaged Loss: 0.069218\n",
      "2025-04-23 14:25: **********Train Epoch 21: averaged Loss: 0.069218\n",
      "2025-04-23 14:25: **********Val Epoch 21: average Loss: 0.094495\n",
      "2025-04-23 14:25: **********Val Epoch 21: average Loss: 0.094495\n",
      "2025-04-23 14:25: **********Val Epoch 21: average Loss: 0.094495\n",
      "2025-04-23 14:25: Train Epoch 22: 0/37 Loss: 0.147032\n",
      "2025-04-23 14:25: Train Epoch 22: 0/37 Loss: 0.147032\n",
      "2025-04-23 14:25: Train Epoch 22: 0/37 Loss: 0.147032\n",
      "2025-04-23 14:25: Train Epoch 22: 20/37 Loss: 0.009894\n",
      "2025-04-23 14:25: Train Epoch 22: 20/37 Loss: 0.009894\n",
      "2025-04-23 14:25: Train Epoch 22: 20/37 Loss: 0.009894\n",
      "2025-04-23 14:25: **********Train Epoch 22: averaged Loss: 0.063909\n",
      "2025-04-23 14:25: **********Train Epoch 22: averaged Loss: 0.063909\n",
      "2025-04-23 14:25: **********Train Epoch 22: averaged Loss: 0.063909\n",
      "2025-04-23 14:25: **********Val Epoch 22: average Loss: 0.046038\n",
      "2025-04-23 14:25: **********Val Epoch 22: average Loss: 0.046038\n",
      "2025-04-23 14:25: **********Val Epoch 22: average Loss: 0.046038\n",
      "2025-04-23 14:25: Train Epoch 23: 0/37 Loss: 0.053691\n",
      "2025-04-23 14:25: Train Epoch 23: 0/37 Loss: 0.053691\n",
      "2025-04-23 14:25: Train Epoch 23: 0/37 Loss: 0.053691\n",
      "2025-04-23 14:25: Train Epoch 23: 20/37 Loss: 0.100114\n",
      "2025-04-23 14:25: Train Epoch 23: 20/37 Loss: 0.100114\n",
      "2025-04-23 14:25: Train Epoch 23: 20/37 Loss: 0.100114\n",
      "2025-04-23 14:25: **********Train Epoch 23: averaged Loss: 0.088285\n",
      "2025-04-23 14:25: **********Train Epoch 23: averaged Loss: 0.088285\n",
      "2025-04-23 14:25: **********Train Epoch 23: averaged Loss: 0.088285\n",
      "2025-04-23 14:25: **********Val Epoch 23: average Loss: 0.294486\n",
      "2025-04-23 14:25: **********Val Epoch 23: average Loss: 0.294486\n",
      "2025-04-23 14:25: **********Val Epoch 23: average Loss: 0.294486\n",
      "2025-04-23 14:25: Train Epoch 24: 0/37 Loss: 0.030724\n",
      "2025-04-23 14:25: Train Epoch 24: 0/37 Loss: 0.030724\n",
      "2025-04-23 14:25: Train Epoch 24: 0/37 Loss: 0.030724\n",
      "2025-04-23 14:25: Train Epoch 24: 20/37 Loss: 0.111193\n",
      "2025-04-23 14:25: Train Epoch 24: 20/37 Loss: 0.111193\n",
      "2025-04-23 14:25: Train Epoch 24: 20/37 Loss: 0.111193\n",
      "2025-04-23 14:25: **********Train Epoch 24: averaged Loss: 0.083404\n",
      "2025-04-23 14:25: **********Train Epoch 24: averaged Loss: 0.083404\n",
      "2025-04-23 14:25: **********Train Epoch 24: averaged Loss: 0.083404\n",
      "2025-04-23 14:25: **********Val Epoch 24: average Loss: 0.118970\n",
      "2025-04-23 14:25: **********Val Epoch 24: average Loss: 0.118970\n",
      "2025-04-23 14:25: **********Val Epoch 24: average Loss: 0.118970\n",
      "2025-04-23 14:25: Train Epoch 25: 0/37 Loss: 0.269701\n",
      "2025-04-23 14:25: Train Epoch 25: 0/37 Loss: 0.269701\n",
      "2025-04-23 14:25: Train Epoch 25: 0/37 Loss: 0.269701\n",
      "2025-04-23 14:25: Train Epoch 25: 20/37 Loss: 0.132364\n",
      "2025-04-23 14:25: Train Epoch 25: 20/37 Loss: 0.132364\n",
      "2025-04-23 14:25: Train Epoch 25: 20/37 Loss: 0.132364\n",
      "2025-04-23 14:25: **********Train Epoch 25: averaged Loss: 0.134822\n",
      "2025-04-23 14:25: **********Train Epoch 25: averaged Loss: 0.134822\n",
      "2025-04-23 14:25: **********Train Epoch 25: averaged Loss: 0.134822\n",
      "2025-04-23 14:25: **********Val Epoch 25: average Loss: 0.060127\n",
      "2025-04-23 14:25: **********Val Epoch 25: average Loss: 0.060127\n",
      "2025-04-23 14:25: **********Val Epoch 25: average Loss: 0.060127\n",
      "2025-04-23 14:25: Train Epoch 26: 0/37 Loss: 0.156860\n",
      "2025-04-23 14:25: Train Epoch 26: 0/37 Loss: 0.156860\n",
      "2025-04-23 14:25: Train Epoch 26: 0/37 Loss: 0.156860\n",
      "2025-04-23 14:25: Train Epoch 26: 20/37 Loss: 0.099590\n",
      "2025-04-23 14:25: Train Epoch 26: 20/37 Loss: 0.099590\n",
      "2025-04-23 14:25: Train Epoch 26: 20/37 Loss: 0.099590\n",
      "2025-04-23 14:25: **********Train Epoch 26: averaged Loss: 0.087257\n",
      "2025-04-23 14:25: **********Train Epoch 26: averaged Loss: 0.087257\n",
      "2025-04-23 14:25: **********Train Epoch 26: averaged Loss: 0.087257\n",
      "2025-04-23 14:25: **********Val Epoch 26: average Loss: 0.063242\n",
      "2025-04-23 14:25: **********Val Epoch 26: average Loss: 0.063242\n",
      "2025-04-23 14:25: **********Val Epoch 26: average Loss: 0.063242\n",
      "2025-04-23 14:25: Train Epoch 27: 0/37 Loss: 0.251204\n",
      "2025-04-23 14:25: Train Epoch 27: 0/37 Loss: 0.251204\n",
      "2025-04-23 14:25: Train Epoch 27: 0/37 Loss: 0.251204\n",
      "2025-04-23 14:25: Train Epoch 27: 20/37 Loss: 0.086035\n",
      "2025-04-23 14:25: Train Epoch 27: 20/37 Loss: 0.086035\n",
      "2025-04-23 14:25: Train Epoch 27: 20/37 Loss: 0.086035\n",
      "2025-04-23 14:25: **********Train Epoch 27: averaged Loss: 0.094107\n",
      "2025-04-23 14:25: **********Train Epoch 27: averaged Loss: 0.094107\n",
      "2025-04-23 14:25: **********Train Epoch 27: averaged Loss: 0.094107\n",
      "2025-04-23 14:25: **********Val Epoch 27: average Loss: 0.059961\n",
      "2025-04-23 14:25: **********Val Epoch 27: average Loss: 0.059961\n",
      "2025-04-23 14:25: **********Val Epoch 27: average Loss: 0.059961\n",
      "2025-04-23 14:25: Train Epoch 28: 0/37 Loss: 0.025157\n",
      "2025-04-23 14:25: Train Epoch 28: 0/37 Loss: 0.025157\n",
      "2025-04-23 14:25: Train Epoch 28: 0/37 Loss: 0.025157\n",
      "2025-04-23 14:25: Train Epoch 28: 20/37 Loss: 0.016765\n",
      "2025-04-23 14:25: Train Epoch 28: 20/37 Loss: 0.016765\n",
      "2025-04-23 14:25: Train Epoch 28: 20/37 Loss: 0.016765\n",
      "2025-04-23 14:25: **********Train Epoch 28: averaged Loss: 0.032991\n",
      "2025-04-23 14:25: **********Train Epoch 28: averaged Loss: 0.032991\n",
      "2025-04-23 14:25: **********Train Epoch 28: averaged Loss: 0.032991\n",
      "2025-04-23 14:25: **********Val Epoch 28: average Loss: 0.063491\n",
      "2025-04-23 14:25: **********Val Epoch 28: average Loss: 0.063491\n",
      "2025-04-23 14:25: **********Val Epoch 28: average Loss: 0.063491\n",
      "2025-04-23 14:25: Train Epoch 29: 0/37 Loss: 0.091724\n",
      "2025-04-23 14:25: Train Epoch 29: 0/37 Loss: 0.091724\n",
      "2025-04-23 14:25: Train Epoch 29: 0/37 Loss: 0.091724\n",
      "2025-04-23 14:25: Train Epoch 29: 20/37 Loss: 0.035861\n",
      "2025-04-23 14:25: Train Epoch 29: 20/37 Loss: 0.035861\n",
      "2025-04-23 14:25: Train Epoch 29: 20/37 Loss: 0.035861\n",
      "2025-04-23 14:25: **********Train Epoch 29: averaged Loss: 0.051553\n",
      "2025-04-23 14:25: **********Train Epoch 29: averaged Loss: 0.051553\n",
      "2025-04-23 14:25: **********Train Epoch 29: averaged Loss: 0.051553\n",
      "2025-04-23 14:25: **********Val Epoch 29: average Loss: 0.078013\n",
      "2025-04-23 14:25: **********Val Epoch 29: average Loss: 0.078013\n",
      "2025-04-23 14:25: **********Val Epoch 29: average Loss: 0.078013\n",
      "2025-04-23 14:25: Train Epoch 30: 0/37 Loss: 0.003753\n",
      "2025-04-23 14:25: Train Epoch 30: 0/37 Loss: 0.003753\n",
      "2025-04-23 14:25: Train Epoch 30: 0/37 Loss: 0.003753\n",
      "2025-04-23 14:25: Train Epoch 30: 20/37 Loss: 0.027138\n",
      "2025-04-23 14:25: Train Epoch 30: 20/37 Loss: 0.027138\n",
      "2025-04-23 14:25: Train Epoch 30: 20/37 Loss: 0.027138\n",
      "2025-04-23 14:25: **********Train Epoch 30: averaged Loss: 0.040277\n",
      "2025-04-23 14:25: **********Train Epoch 30: averaged Loss: 0.040277\n",
      "2025-04-23 14:25: **********Train Epoch 30: averaged Loss: 0.040277\n",
      "2025-04-23 14:25: **********Val Epoch 30: average Loss: 0.071413\n",
      "2025-04-23 14:25: **********Val Epoch 30: average Loss: 0.071413\n",
      "2025-04-23 14:25: **********Val Epoch 30: average Loss: 0.071413\n",
      "2025-04-23 14:25: Train Epoch 31: 0/37 Loss: 0.016746\n",
      "2025-04-23 14:25: Train Epoch 31: 0/37 Loss: 0.016746\n",
      "2025-04-23 14:25: Train Epoch 31: 0/37 Loss: 0.016746\n",
      "2025-04-23 14:25: Train Epoch 31: 20/37 Loss: 0.021650\n",
      "2025-04-23 14:25: Train Epoch 31: 20/37 Loss: 0.021650\n",
      "2025-04-23 14:25: Train Epoch 31: 20/37 Loss: 0.021650\n",
      "2025-04-23 14:25: **********Train Epoch 31: averaged Loss: 0.053181\n",
      "2025-04-23 14:25: **********Train Epoch 31: averaged Loss: 0.053181\n",
      "2025-04-23 14:25: **********Train Epoch 31: averaged Loss: 0.053181\n",
      "2025-04-23 14:25: **********Val Epoch 31: average Loss: 0.021750\n",
      "2025-04-23 14:25: **********Val Epoch 31: average Loss: 0.021750\n",
      "2025-04-23 14:25: **********Val Epoch 31: average Loss: 0.021750\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: *********************************Current best model saved!\n",
      "2025-04-23 14:25: Train Epoch 32: 0/37 Loss: 0.114861\n",
      "2025-04-23 14:25: Train Epoch 32: 0/37 Loss: 0.114861\n",
      "2025-04-23 14:25: Train Epoch 32: 0/37 Loss: 0.114861\n",
      "2025-04-23 14:25: Train Epoch 32: 20/37 Loss: 0.012572\n",
      "2025-04-23 14:25: Train Epoch 32: 20/37 Loss: 0.012572\n",
      "2025-04-23 14:25: Train Epoch 32: 20/37 Loss: 0.012572\n",
      "2025-04-23 14:25: **********Train Epoch 32: averaged Loss: 0.055138\n",
      "2025-04-23 14:25: **********Train Epoch 32: averaged Loss: 0.055138\n",
      "2025-04-23 14:25: **********Train Epoch 32: averaged Loss: 0.055138\n",
      "2025-04-23 14:25: **********Val Epoch 32: average Loss: 0.077388\n",
      "2025-04-23 14:25: **********Val Epoch 32: average Loss: 0.077388\n",
      "2025-04-23 14:25: **********Val Epoch 32: average Loss: 0.077388\n",
      "2025-04-23 14:25: Train Epoch 33: 0/37 Loss: 0.082784\n",
      "2025-04-23 14:25: Train Epoch 33: 0/37 Loss: 0.082784\n",
      "2025-04-23 14:25: Train Epoch 33: 0/37 Loss: 0.082784\n",
      "2025-04-23 14:25: Train Epoch 33: 20/37 Loss: 0.034364\n",
      "2025-04-23 14:25: Train Epoch 33: 20/37 Loss: 0.034364\n",
      "2025-04-23 14:25: Train Epoch 33: 20/37 Loss: 0.034364\n",
      "2025-04-23 14:26: **********Train Epoch 33: averaged Loss: 0.066234\n",
      "2025-04-23 14:26: **********Train Epoch 33: averaged Loss: 0.066234\n",
      "2025-04-23 14:26: **********Train Epoch 33: averaged Loss: 0.066234\n",
      "2025-04-23 14:26: **********Val Epoch 33: average Loss: 0.169793\n",
      "2025-04-23 14:26: **********Val Epoch 33: average Loss: 0.169793\n",
      "2025-04-23 14:26: **********Val Epoch 33: average Loss: 0.169793\n",
      "2025-04-23 14:26: Train Epoch 34: 0/37 Loss: 0.053676\n",
      "2025-04-23 14:26: Train Epoch 34: 0/37 Loss: 0.053676\n",
      "2025-04-23 14:26: Train Epoch 34: 0/37 Loss: 0.053676\n",
      "2025-04-23 14:26: Train Epoch 34: 20/37 Loss: 0.073636\n",
      "2025-04-23 14:26: Train Epoch 34: 20/37 Loss: 0.073636\n",
      "2025-04-23 14:26: Train Epoch 34: 20/37 Loss: 0.073636\n",
      "2025-04-23 14:26: **********Train Epoch 34: averaged Loss: 0.053058\n",
      "2025-04-23 14:26: **********Train Epoch 34: averaged Loss: 0.053058\n",
      "2025-04-23 14:26: **********Train Epoch 34: averaged Loss: 0.053058\n",
      "2025-04-23 14:26: **********Val Epoch 34: average Loss: 0.149051\n",
      "2025-04-23 14:26: **********Val Epoch 34: average Loss: 0.149051\n",
      "2025-04-23 14:26: **********Val Epoch 34: average Loss: 0.149051\n",
      "2025-04-23 14:26: Train Epoch 35: 0/37 Loss: 0.270327\n",
      "2025-04-23 14:26: Train Epoch 35: 0/37 Loss: 0.270327\n",
      "2025-04-23 14:26: Train Epoch 35: 0/37 Loss: 0.270327\n",
      "2025-04-23 14:26: Train Epoch 35: 20/37 Loss: 0.074436\n",
      "2025-04-23 14:26: Train Epoch 35: 20/37 Loss: 0.074436\n",
      "2025-04-23 14:26: Train Epoch 35: 20/37 Loss: 0.074436\n",
      "2025-04-23 14:26: **********Train Epoch 35: averaged Loss: 0.100672\n",
      "2025-04-23 14:26: **********Train Epoch 35: averaged Loss: 0.100672\n",
      "2025-04-23 14:26: **********Train Epoch 35: averaged Loss: 0.100672\n",
      "2025-04-23 14:26: **********Val Epoch 35: average Loss: 0.057441\n",
      "2025-04-23 14:26: **********Val Epoch 35: average Loss: 0.057441\n",
      "2025-04-23 14:26: **********Val Epoch 35: average Loss: 0.057441\n",
      "2025-04-23 14:26: Train Epoch 36: 0/37 Loss: 0.062340\n",
      "2025-04-23 14:26: Train Epoch 36: 0/37 Loss: 0.062340\n",
      "2025-04-23 14:26: Train Epoch 36: 0/37 Loss: 0.062340\n",
      "2025-04-23 14:26: Train Epoch 36: 20/37 Loss: 0.007137\n",
      "2025-04-23 14:26: Train Epoch 36: 20/37 Loss: 0.007137\n",
      "2025-04-23 14:26: Train Epoch 36: 20/37 Loss: 0.007137\n",
      "2025-04-23 14:26: **********Train Epoch 36: averaged Loss: 0.035582\n",
      "2025-04-23 14:26: **********Train Epoch 36: averaged Loss: 0.035582\n",
      "2025-04-23 14:26: **********Train Epoch 36: averaged Loss: 0.035582\n",
      "2025-04-23 14:26: **********Val Epoch 36: average Loss: 0.034645\n",
      "2025-04-23 14:26: **********Val Epoch 36: average Loss: 0.034645\n",
      "2025-04-23 14:26: **********Val Epoch 36: average Loss: 0.034645\n",
      "2025-04-23 14:26: Train Epoch 37: 0/37 Loss: 0.016211\n",
      "2025-04-23 14:26: Train Epoch 37: 0/37 Loss: 0.016211\n",
      "2025-04-23 14:26: Train Epoch 37: 0/37 Loss: 0.016211\n",
      "2025-04-23 14:26: Train Epoch 37: 20/37 Loss: 0.023277\n",
      "2025-04-23 14:26: Train Epoch 37: 20/37 Loss: 0.023277\n",
      "2025-04-23 14:26: Train Epoch 37: 20/37 Loss: 0.023277\n",
      "2025-04-23 14:26: **********Train Epoch 37: averaged Loss: 0.024794\n",
      "2025-04-23 14:26: **********Train Epoch 37: averaged Loss: 0.024794\n",
      "2025-04-23 14:26: **********Train Epoch 37: averaged Loss: 0.024794\n",
      "2025-04-23 14:26: **********Val Epoch 37: average Loss: 0.022116\n",
      "2025-04-23 14:26: **********Val Epoch 37: average Loss: 0.022116\n",
      "2025-04-23 14:26: **********Val Epoch 37: average Loss: 0.022116\n",
      "2025-04-23 14:26: Train Epoch 38: 0/37 Loss: 0.081138\n",
      "2025-04-23 14:26: Train Epoch 38: 0/37 Loss: 0.081138\n",
      "2025-04-23 14:26: Train Epoch 38: 0/37 Loss: 0.081138\n",
      "2025-04-23 14:26: Train Epoch 38: 20/37 Loss: 0.031090\n",
      "2025-04-23 14:26: Train Epoch 38: 20/37 Loss: 0.031090\n",
      "2025-04-23 14:26: Train Epoch 38: 20/37 Loss: 0.031090\n",
      "2025-04-23 14:26: **********Train Epoch 38: averaged Loss: 0.045328\n",
      "2025-04-23 14:26: **********Train Epoch 38: averaged Loss: 0.045328\n",
      "2025-04-23 14:26: **********Train Epoch 38: averaged Loss: 0.045328\n",
      "2025-04-23 14:26: **********Val Epoch 38: average Loss: 0.143548\n",
      "2025-04-23 14:26: **********Val Epoch 38: average Loss: 0.143548\n",
      "2025-04-23 14:26: **********Val Epoch 38: average Loss: 0.143548\n",
      "2025-04-23 14:26: Train Epoch 39: 0/37 Loss: 0.078097\n",
      "2025-04-23 14:26: Train Epoch 39: 0/37 Loss: 0.078097\n",
      "2025-04-23 14:26: Train Epoch 39: 0/37 Loss: 0.078097\n",
      "2025-04-23 14:26: Train Epoch 39: 20/37 Loss: 0.100675\n",
      "2025-04-23 14:26: Train Epoch 39: 20/37 Loss: 0.100675\n",
      "2025-04-23 14:26: Train Epoch 39: 20/37 Loss: 0.100675\n",
      "2025-04-23 14:26: **********Train Epoch 39: averaged Loss: 0.047892\n",
      "2025-04-23 14:26: **********Train Epoch 39: averaged Loss: 0.047892\n",
      "2025-04-23 14:26: **********Train Epoch 39: averaged Loss: 0.047892\n",
      "2025-04-23 14:26: **********Val Epoch 39: average Loss: 0.134774\n",
      "2025-04-23 14:26: **********Val Epoch 39: average Loss: 0.134774\n",
      "2025-04-23 14:26: **********Val Epoch 39: average Loss: 0.134774\n",
      "2025-04-23 14:26: Train Epoch 40: 0/37 Loss: 0.085298\n",
      "2025-04-23 14:26: Train Epoch 40: 0/37 Loss: 0.085298\n",
      "2025-04-23 14:26: Train Epoch 40: 0/37 Loss: 0.085298\n",
      "2025-04-23 14:26: Train Epoch 40: 20/37 Loss: 0.024988\n",
      "2025-04-23 14:26: Train Epoch 40: 20/37 Loss: 0.024988\n",
      "2025-04-23 14:26: Train Epoch 40: 20/37 Loss: 0.024988\n",
      "2025-04-23 14:26: **********Train Epoch 40: averaged Loss: 0.066935\n",
      "2025-04-23 14:26: **********Train Epoch 40: averaged Loss: 0.066935\n",
      "2025-04-23 14:26: **********Train Epoch 40: averaged Loss: 0.066935\n",
      "2025-04-23 14:26: **********Val Epoch 40: average Loss: 0.020752\n",
      "2025-04-23 14:26: **********Val Epoch 40: average Loss: 0.020752\n",
      "2025-04-23 14:26: **********Val Epoch 40: average Loss: 0.020752\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: Train Epoch 41: 0/37 Loss: 0.036198\n",
      "2025-04-23 14:26: Train Epoch 41: 0/37 Loss: 0.036198\n",
      "2025-04-23 14:26: Train Epoch 41: 0/37 Loss: 0.036198\n",
      "2025-04-23 14:26: Train Epoch 41: 20/37 Loss: 0.012773\n",
      "2025-04-23 14:26: Train Epoch 41: 20/37 Loss: 0.012773\n",
      "2025-04-23 14:26: Train Epoch 41: 20/37 Loss: 0.012773\n",
      "2025-04-23 14:26: **********Train Epoch 41: averaged Loss: 0.030969\n",
      "2025-04-23 14:26: **********Train Epoch 41: averaged Loss: 0.030969\n",
      "2025-04-23 14:26: **********Train Epoch 41: averaged Loss: 0.030969\n",
      "2025-04-23 14:26: **********Val Epoch 41: average Loss: 0.032609\n",
      "2025-04-23 14:26: **********Val Epoch 41: average Loss: 0.032609\n",
      "2025-04-23 14:26: **********Val Epoch 41: average Loss: 0.032609\n",
      "2025-04-23 14:26: Train Epoch 42: 0/37 Loss: 0.091465\n",
      "2025-04-23 14:26: Train Epoch 42: 0/37 Loss: 0.091465\n",
      "2025-04-23 14:26: Train Epoch 42: 0/37 Loss: 0.091465\n",
      "2025-04-23 14:26: Train Epoch 42: 20/37 Loss: 0.025566\n",
      "2025-04-23 14:26: Train Epoch 42: 20/37 Loss: 0.025566\n",
      "2025-04-23 14:26: Train Epoch 42: 20/37 Loss: 0.025566\n",
      "2025-04-23 14:26: **********Train Epoch 42: averaged Loss: 0.055808\n",
      "2025-04-23 14:26: **********Train Epoch 42: averaged Loss: 0.055808\n",
      "2025-04-23 14:26: **********Train Epoch 42: averaged Loss: 0.055808\n",
      "2025-04-23 14:26: **********Val Epoch 42: average Loss: 0.030427\n",
      "2025-04-23 14:26: **********Val Epoch 42: average Loss: 0.030427\n",
      "2025-04-23 14:26: **********Val Epoch 42: average Loss: 0.030427\n",
      "2025-04-23 14:26: Train Epoch 43: 0/37 Loss: 0.017077\n",
      "2025-04-23 14:26: Train Epoch 43: 0/37 Loss: 0.017077\n",
      "2025-04-23 14:26: Train Epoch 43: 0/37 Loss: 0.017077\n",
      "2025-04-23 14:26: Train Epoch 43: 20/37 Loss: 0.026400\n",
      "2025-04-23 14:26: Train Epoch 43: 20/37 Loss: 0.026400\n",
      "2025-04-23 14:26: Train Epoch 43: 20/37 Loss: 0.026400\n",
      "2025-04-23 14:26: **********Train Epoch 43: averaged Loss: 0.041630\n",
      "2025-04-23 14:26: **********Train Epoch 43: averaged Loss: 0.041630\n",
      "2025-04-23 14:26: **********Train Epoch 43: averaged Loss: 0.041630\n",
      "2025-04-23 14:26: **********Val Epoch 43: average Loss: 0.036437\n",
      "2025-04-23 14:26: **********Val Epoch 43: average Loss: 0.036437\n",
      "2025-04-23 14:26: **********Val Epoch 43: average Loss: 0.036437\n",
      "2025-04-23 14:26: Train Epoch 44: 0/37 Loss: 0.089595\n",
      "2025-04-23 14:26: Train Epoch 44: 0/37 Loss: 0.089595\n",
      "2025-04-23 14:26: Train Epoch 44: 0/37 Loss: 0.089595\n",
      "2025-04-23 14:26: Train Epoch 44: 20/37 Loss: 0.006333\n",
      "2025-04-23 14:26: Train Epoch 44: 20/37 Loss: 0.006333\n",
      "2025-04-23 14:26: Train Epoch 44: 20/37 Loss: 0.006333\n",
      "2025-04-23 14:26: **********Train Epoch 44: averaged Loss: 0.039229\n",
      "2025-04-23 14:26: **********Train Epoch 44: averaged Loss: 0.039229\n",
      "2025-04-23 14:26: **********Train Epoch 44: averaged Loss: 0.039229\n",
      "2025-04-23 14:26: **********Val Epoch 44: average Loss: 0.025907\n",
      "2025-04-23 14:26: **********Val Epoch 44: average Loss: 0.025907\n",
      "2025-04-23 14:26: **********Val Epoch 44: average Loss: 0.025907\n",
      "2025-04-23 14:26: Train Epoch 45: 0/37 Loss: 0.053556\n",
      "2025-04-23 14:26: Train Epoch 45: 0/37 Loss: 0.053556\n",
      "2025-04-23 14:26: Train Epoch 45: 0/37 Loss: 0.053556\n",
      "2025-04-23 14:26: Train Epoch 45: 20/37 Loss: 0.043886\n",
      "2025-04-23 14:26: Train Epoch 45: 20/37 Loss: 0.043886\n",
      "2025-04-23 14:26: Train Epoch 45: 20/37 Loss: 0.043886\n",
      "2025-04-23 14:26: **********Train Epoch 45: averaged Loss: 0.043101\n",
      "2025-04-23 14:26: **********Train Epoch 45: averaged Loss: 0.043101\n",
      "2025-04-23 14:26: **********Train Epoch 45: averaged Loss: 0.043101\n",
      "2025-04-23 14:26: **********Val Epoch 45: average Loss: 0.049138\n",
      "2025-04-23 14:26: **********Val Epoch 45: average Loss: 0.049138\n",
      "2025-04-23 14:26: **********Val Epoch 45: average Loss: 0.049138\n",
      "2025-04-23 14:26: Train Epoch 46: 0/37 Loss: 0.014066\n",
      "2025-04-23 14:26: Train Epoch 46: 0/37 Loss: 0.014066\n",
      "2025-04-23 14:26: Train Epoch 46: 0/37 Loss: 0.014066\n",
      "2025-04-23 14:26: Train Epoch 46: 20/37 Loss: 0.026342\n",
      "2025-04-23 14:26: Train Epoch 46: 20/37 Loss: 0.026342\n",
      "2025-04-23 14:26: Train Epoch 46: 20/37 Loss: 0.026342\n",
      "2025-04-23 14:26: **********Train Epoch 46: averaged Loss: 0.036405\n",
      "2025-04-23 14:26: **********Train Epoch 46: averaged Loss: 0.036405\n",
      "2025-04-23 14:26: **********Train Epoch 46: averaged Loss: 0.036405\n",
      "2025-04-23 14:26: **********Val Epoch 46: average Loss: 0.033087\n",
      "2025-04-23 14:26: **********Val Epoch 46: average Loss: 0.033087\n",
      "2025-04-23 14:26: **********Val Epoch 46: average Loss: 0.033087\n",
      "2025-04-23 14:26: Train Epoch 47: 0/37 Loss: 0.114302\n",
      "2025-04-23 14:26: Train Epoch 47: 0/37 Loss: 0.114302\n",
      "2025-04-23 14:26: Train Epoch 47: 0/37 Loss: 0.114302\n",
      "2025-04-23 14:26: Train Epoch 47: 20/37 Loss: 0.035722\n",
      "2025-04-23 14:26: Train Epoch 47: 20/37 Loss: 0.035722\n",
      "2025-04-23 14:26: Train Epoch 47: 20/37 Loss: 0.035722\n",
      "2025-04-23 14:26: **********Train Epoch 47: averaged Loss: 0.055087\n",
      "2025-04-23 14:26: **********Train Epoch 47: averaged Loss: 0.055087\n",
      "2025-04-23 14:26: **********Train Epoch 47: averaged Loss: 0.055087\n",
      "2025-04-23 14:26: **********Val Epoch 47: average Loss: 0.020221\n",
      "2025-04-23 14:26: **********Val Epoch 47: average Loss: 0.020221\n",
      "2025-04-23 14:26: **********Val Epoch 47: average Loss: 0.020221\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: Train Epoch 48: 0/37 Loss: 0.032850\n",
      "2025-04-23 14:26: Train Epoch 48: 0/37 Loss: 0.032850\n",
      "2025-04-23 14:26: Train Epoch 48: 0/37 Loss: 0.032850\n",
      "2025-04-23 14:26: Train Epoch 48: 20/37 Loss: 0.006727\n",
      "2025-04-23 14:26: Train Epoch 48: 20/37 Loss: 0.006727\n",
      "2025-04-23 14:26: Train Epoch 48: 20/37 Loss: 0.006727\n",
      "2025-04-23 14:26: **********Train Epoch 48: averaged Loss: 0.028523\n",
      "2025-04-23 14:26: **********Train Epoch 48: averaged Loss: 0.028523\n",
      "2025-04-23 14:26: **********Train Epoch 48: averaged Loss: 0.028523\n",
      "2025-04-23 14:26: **********Val Epoch 48: average Loss: 0.018359\n",
      "2025-04-23 14:26: **********Val Epoch 48: average Loss: 0.018359\n",
      "2025-04-23 14:26: **********Val Epoch 48: average Loss: 0.018359\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: Train Epoch 49: 0/37 Loss: 0.017098\n",
      "2025-04-23 14:26: Train Epoch 49: 0/37 Loss: 0.017098\n",
      "2025-04-23 14:26: Train Epoch 49: 0/37 Loss: 0.017098\n",
      "2025-04-23 14:26: Train Epoch 49: 20/37 Loss: 0.007647\n",
      "2025-04-23 14:26: Train Epoch 49: 20/37 Loss: 0.007647\n",
      "2025-04-23 14:26: Train Epoch 49: 20/37 Loss: 0.007647\n",
      "2025-04-23 14:26: **********Train Epoch 49: averaged Loss: 0.021291\n",
      "2025-04-23 14:26: **********Train Epoch 49: averaged Loss: 0.021291\n",
      "2025-04-23 14:26: **********Train Epoch 49: averaged Loss: 0.021291\n",
      "2025-04-23 14:26: **********Val Epoch 49: average Loss: 0.128426\n",
      "2025-04-23 14:26: **********Val Epoch 49: average Loss: 0.128426\n",
      "2025-04-23 14:26: **********Val Epoch 49: average Loss: 0.128426\n",
      "2025-04-23 14:26: Train Epoch 50: 0/37 Loss: 0.005451\n",
      "2025-04-23 14:26: Train Epoch 50: 0/37 Loss: 0.005451\n",
      "2025-04-23 14:26: Train Epoch 50: 0/37 Loss: 0.005451\n",
      "2025-04-23 14:26: Train Epoch 50: 20/37 Loss: 0.040176\n",
      "2025-04-23 14:26: Train Epoch 50: 20/37 Loss: 0.040176\n",
      "2025-04-23 14:26: Train Epoch 50: 20/37 Loss: 0.040176\n",
      "2025-04-23 14:26: **********Train Epoch 50: averaged Loss: 0.037436\n",
      "2025-04-23 14:26: **********Train Epoch 50: averaged Loss: 0.037436\n",
      "2025-04-23 14:26: **********Train Epoch 50: averaged Loss: 0.037436\n",
      "2025-04-23 14:26: **********Val Epoch 50: average Loss: 0.037007\n",
      "2025-04-23 14:26: **********Val Epoch 50: average Loss: 0.037007\n",
      "2025-04-23 14:26: **********Val Epoch 50: average Loss: 0.037007\n",
      "2025-04-23 14:26: Train Epoch 51: 0/37 Loss: 0.005786\n",
      "2025-04-23 14:26: Train Epoch 51: 0/37 Loss: 0.005786\n",
      "2025-04-23 14:26: Train Epoch 51: 0/37 Loss: 0.005786\n",
      "2025-04-23 14:26: Train Epoch 51: 20/37 Loss: 0.040121\n",
      "2025-04-23 14:26: Train Epoch 51: 20/37 Loss: 0.040121\n",
      "2025-04-23 14:26: Train Epoch 51: 20/37 Loss: 0.040121\n",
      "2025-04-23 14:26: **********Train Epoch 51: averaged Loss: 0.022772\n",
      "2025-04-23 14:26: **********Train Epoch 51: averaged Loss: 0.022772\n",
      "2025-04-23 14:26: **********Train Epoch 51: averaged Loss: 0.022772\n",
      "2025-04-23 14:26: **********Val Epoch 51: average Loss: 0.098046\n",
      "2025-04-23 14:26: **********Val Epoch 51: average Loss: 0.098046\n",
      "2025-04-23 14:26: **********Val Epoch 51: average Loss: 0.098046\n",
      "2025-04-23 14:26: Train Epoch 52: 0/37 Loss: 0.008264\n",
      "2025-04-23 14:26: Train Epoch 52: 0/37 Loss: 0.008264\n",
      "2025-04-23 14:26: Train Epoch 52: 0/37 Loss: 0.008264\n",
      "2025-04-23 14:26: Train Epoch 52: 20/37 Loss: 0.014885\n",
      "2025-04-23 14:26: Train Epoch 52: 20/37 Loss: 0.014885\n",
      "2025-04-23 14:26: Train Epoch 52: 20/37 Loss: 0.014885\n",
      "2025-04-23 14:26: **********Train Epoch 52: averaged Loss: 0.018275\n",
      "2025-04-23 14:26: **********Train Epoch 52: averaged Loss: 0.018275\n",
      "2025-04-23 14:26: **********Train Epoch 52: averaged Loss: 0.018275\n",
      "2025-04-23 14:26: **********Val Epoch 52: average Loss: 0.036347\n",
      "2025-04-23 14:26: **********Val Epoch 52: average Loss: 0.036347\n",
      "2025-04-23 14:26: **********Val Epoch 52: average Loss: 0.036347\n",
      "2025-04-23 14:26: Train Epoch 53: 0/37 Loss: 0.004009\n",
      "2025-04-23 14:26: Train Epoch 53: 0/37 Loss: 0.004009\n",
      "2025-04-23 14:26: Train Epoch 53: 0/37 Loss: 0.004009\n",
      "2025-04-23 14:26: Train Epoch 53: 20/37 Loss: 0.027418\n",
      "2025-04-23 14:26: Train Epoch 53: 20/37 Loss: 0.027418\n",
      "2025-04-23 14:26: Train Epoch 53: 20/37 Loss: 0.027418\n",
      "2025-04-23 14:26: **********Train Epoch 53: averaged Loss: 0.034488\n",
      "2025-04-23 14:26: **********Train Epoch 53: averaged Loss: 0.034488\n",
      "2025-04-23 14:26: **********Train Epoch 53: averaged Loss: 0.034488\n",
      "2025-04-23 14:26: **********Val Epoch 53: average Loss: 0.084561\n",
      "2025-04-23 14:26: **********Val Epoch 53: average Loss: 0.084561\n",
      "2025-04-23 14:26: **********Val Epoch 53: average Loss: 0.084561\n",
      "2025-04-23 14:26: Train Epoch 54: 0/37 Loss: 0.031066\n",
      "2025-04-23 14:26: Train Epoch 54: 0/37 Loss: 0.031066\n",
      "2025-04-23 14:26: Train Epoch 54: 0/37 Loss: 0.031066\n",
      "2025-04-23 14:26: Train Epoch 54: 20/37 Loss: 0.076184\n",
      "2025-04-23 14:26: Train Epoch 54: 20/37 Loss: 0.076184\n",
      "2025-04-23 14:26: Train Epoch 54: 20/37 Loss: 0.076184\n",
      "2025-04-23 14:26: **********Train Epoch 54: averaged Loss: 0.052571\n",
      "2025-04-23 14:26: **********Train Epoch 54: averaged Loss: 0.052571\n",
      "2025-04-23 14:26: **********Train Epoch 54: averaged Loss: 0.052571\n",
      "2025-04-23 14:26: **********Val Epoch 54: average Loss: 0.082301\n",
      "2025-04-23 14:26: **********Val Epoch 54: average Loss: 0.082301\n",
      "2025-04-23 14:26: **********Val Epoch 54: average Loss: 0.082301\n",
      "2025-04-23 14:26: Train Epoch 55: 0/37 Loss: 0.029548\n",
      "2025-04-23 14:26: Train Epoch 55: 0/37 Loss: 0.029548\n",
      "2025-04-23 14:26: Train Epoch 55: 0/37 Loss: 0.029548\n",
      "2025-04-23 14:26: Train Epoch 55: 20/37 Loss: 0.012380\n",
      "2025-04-23 14:26: Train Epoch 55: 20/37 Loss: 0.012380\n",
      "2025-04-23 14:26: Train Epoch 55: 20/37 Loss: 0.012380\n",
      "2025-04-23 14:26: **********Train Epoch 55: averaged Loss: 0.040463\n",
      "2025-04-23 14:26: **********Train Epoch 55: averaged Loss: 0.040463\n",
      "2025-04-23 14:26: **********Train Epoch 55: averaged Loss: 0.040463\n",
      "2025-04-23 14:26: **********Val Epoch 55: average Loss: 0.016459\n",
      "2025-04-23 14:26: **********Val Epoch 55: average Loss: 0.016459\n",
      "2025-04-23 14:26: **********Val Epoch 55: average Loss: 0.016459\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: Train Epoch 56: 0/37 Loss: 0.109370\n",
      "2025-04-23 14:26: Train Epoch 56: 0/37 Loss: 0.109370\n",
      "2025-04-23 14:26: Train Epoch 56: 0/37 Loss: 0.109370\n",
      "2025-04-23 14:26: Train Epoch 56: 20/37 Loss: 0.006058\n",
      "2025-04-23 14:26: Train Epoch 56: 20/37 Loss: 0.006058\n",
      "2025-04-23 14:26: Train Epoch 56: 20/37 Loss: 0.006058\n",
      "2025-04-23 14:26: **********Train Epoch 56: averaged Loss: 0.050788\n",
      "2025-04-23 14:26: **********Train Epoch 56: averaged Loss: 0.050788\n",
      "2025-04-23 14:26: **********Train Epoch 56: averaged Loss: 0.050788\n",
      "2025-04-23 14:26: **********Val Epoch 56: average Loss: 0.089250\n",
      "2025-04-23 14:26: **********Val Epoch 56: average Loss: 0.089250\n",
      "2025-04-23 14:26: **********Val Epoch 56: average Loss: 0.089250\n",
      "2025-04-23 14:26: Train Epoch 57: 0/37 Loss: 0.008747\n",
      "2025-04-23 14:26: Train Epoch 57: 0/37 Loss: 0.008747\n",
      "2025-04-23 14:26: Train Epoch 57: 0/37 Loss: 0.008747\n",
      "2025-04-23 14:26: Train Epoch 57: 20/37 Loss: 0.021774\n",
      "2025-04-23 14:26: Train Epoch 57: 20/37 Loss: 0.021774\n",
      "2025-04-23 14:26: Train Epoch 57: 20/37 Loss: 0.021774\n",
      "2025-04-23 14:26: **********Train Epoch 57: averaged Loss: 0.033899\n",
      "2025-04-23 14:26: **********Train Epoch 57: averaged Loss: 0.033899\n",
      "2025-04-23 14:26: **********Train Epoch 57: averaged Loss: 0.033899\n",
      "2025-04-23 14:26: **********Val Epoch 57: average Loss: 0.012730\n",
      "2025-04-23 14:26: **********Val Epoch 57: average Loss: 0.012730\n",
      "2025-04-23 14:26: **********Val Epoch 57: average Loss: 0.012730\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: *********************************Current best model saved!\n",
      "2025-04-23 14:26: Train Epoch 58: 0/37 Loss: 0.067511\n",
      "2025-04-23 14:26: Train Epoch 58: 0/37 Loss: 0.067511\n",
      "2025-04-23 14:26: Train Epoch 58: 0/37 Loss: 0.067511\n",
      "2025-04-23 14:26: Train Epoch 58: 20/37 Loss: 0.037271\n",
      "2025-04-23 14:26: Train Epoch 58: 20/37 Loss: 0.037271\n",
      "2025-04-23 14:26: Train Epoch 58: 20/37 Loss: 0.037271\n",
      "2025-04-23 14:26: **********Train Epoch 58: averaged Loss: 0.037925\n",
      "2025-04-23 14:26: **********Train Epoch 58: averaged Loss: 0.037925\n",
      "2025-04-23 14:26: **********Train Epoch 58: averaged Loss: 0.037925\n",
      "2025-04-23 14:26: **********Val Epoch 58: average Loss: 0.030767\n",
      "2025-04-23 14:26: **********Val Epoch 58: average Loss: 0.030767\n",
      "2025-04-23 14:26: **********Val Epoch 58: average Loss: 0.030767\n",
      "2025-04-23 14:26: Train Epoch 59: 0/37 Loss: 0.008465\n",
      "2025-04-23 14:26: Train Epoch 59: 0/37 Loss: 0.008465\n",
      "2025-04-23 14:26: Train Epoch 59: 0/37 Loss: 0.008465\n",
      "2025-04-23 14:26: Train Epoch 59: 20/37 Loss: 0.032378\n",
      "2025-04-23 14:26: Train Epoch 59: 20/37 Loss: 0.032378\n",
      "2025-04-23 14:26: Train Epoch 59: 20/37 Loss: 0.032378\n",
      "2025-04-23 14:26: **********Train Epoch 59: averaged Loss: 0.037865\n",
      "2025-04-23 14:26: **********Train Epoch 59: averaged Loss: 0.037865\n",
      "2025-04-23 14:26: **********Train Epoch 59: averaged Loss: 0.037865\n",
      "2025-04-23 14:26: **********Val Epoch 59: average Loss: 0.032773\n",
      "2025-04-23 14:26: **********Val Epoch 59: average Loss: 0.032773\n",
      "2025-04-23 14:26: **********Val Epoch 59: average Loss: 0.032773\n",
      "2025-04-23 14:26: Train Epoch 60: 0/37 Loss: 0.095778\n",
      "2025-04-23 14:26: Train Epoch 60: 0/37 Loss: 0.095778\n",
      "2025-04-23 14:26: Train Epoch 60: 0/37 Loss: 0.095778\n",
      "2025-04-23 14:26: Train Epoch 60: 20/37 Loss: 0.034463\n",
      "2025-04-23 14:26: Train Epoch 60: 20/37 Loss: 0.034463\n",
      "2025-04-23 14:26: Train Epoch 60: 20/37 Loss: 0.034463\n",
      "2025-04-23 14:26: **********Train Epoch 60: averaged Loss: 0.042629\n",
      "2025-04-23 14:26: **********Train Epoch 60: averaged Loss: 0.042629\n",
      "2025-04-23 14:26: **********Train Epoch 60: averaged Loss: 0.042629\n",
      "2025-04-23 14:26: **********Val Epoch 60: average Loss: 0.036451\n",
      "2025-04-23 14:26: **********Val Epoch 60: average Loss: 0.036451\n",
      "2025-04-23 14:26: **********Val Epoch 60: average Loss: 0.036451\n",
      "2025-04-23 14:26: Train Epoch 61: 0/37 Loss: 0.055005\n",
      "2025-04-23 14:26: Train Epoch 61: 0/37 Loss: 0.055005\n",
      "2025-04-23 14:26: Train Epoch 61: 0/37 Loss: 0.055005\n",
      "2025-04-23 14:26: Train Epoch 61: 20/37 Loss: 0.013923\n",
      "2025-04-23 14:26: Train Epoch 61: 20/37 Loss: 0.013923\n",
      "2025-04-23 14:26: Train Epoch 61: 20/37 Loss: 0.013923\n",
      "2025-04-23 14:26: **********Train Epoch 61: averaged Loss: 0.032126\n",
      "2025-04-23 14:26: **********Train Epoch 61: averaged Loss: 0.032126\n",
      "2025-04-23 14:26: **********Train Epoch 61: averaged Loss: 0.032126\n",
      "2025-04-23 14:26: **********Val Epoch 61: average Loss: 0.073481\n",
      "2025-04-23 14:26: **********Val Epoch 61: average Loss: 0.073481\n",
      "2025-04-23 14:26: **********Val Epoch 61: average Loss: 0.073481\n",
      "2025-04-23 14:26: Train Epoch 62: 0/37 Loss: 0.041583\n",
      "2025-04-23 14:26: Train Epoch 62: 0/37 Loss: 0.041583\n",
      "2025-04-23 14:26: Train Epoch 62: 0/37 Loss: 0.041583\n",
      "2025-04-23 14:26: Train Epoch 62: 20/37 Loss: 0.014101\n",
      "2025-04-23 14:26: Train Epoch 62: 20/37 Loss: 0.014101\n",
      "2025-04-23 14:26: Train Epoch 62: 20/37 Loss: 0.014101\n",
      "2025-04-23 14:26: **********Train Epoch 62: averaged Loss: 0.024130\n",
      "2025-04-23 14:26: **********Train Epoch 62: averaged Loss: 0.024130\n",
      "2025-04-23 14:26: **********Train Epoch 62: averaged Loss: 0.024130\n",
      "2025-04-23 14:26: **********Val Epoch 62: average Loss: 0.103374\n",
      "2025-04-23 14:26: **********Val Epoch 62: average Loss: 0.103374\n",
      "2025-04-23 14:26: **********Val Epoch 62: average Loss: 0.103374\n",
      "2025-04-23 14:26: Train Epoch 63: 0/37 Loss: 0.020931\n",
      "2025-04-23 14:26: Train Epoch 63: 0/37 Loss: 0.020931\n",
      "2025-04-23 14:26: Train Epoch 63: 0/37 Loss: 0.020931\n",
      "2025-04-23 14:26: Train Epoch 63: 20/37 Loss: 0.025209\n",
      "2025-04-23 14:26: Train Epoch 63: 20/37 Loss: 0.025209\n",
      "2025-04-23 14:26: Train Epoch 63: 20/37 Loss: 0.025209\n",
      "2025-04-23 14:26: **********Train Epoch 63: averaged Loss: 0.046637\n",
      "2025-04-23 14:26: **********Train Epoch 63: averaged Loss: 0.046637\n",
      "2025-04-23 14:26: **********Train Epoch 63: averaged Loss: 0.046637\n",
      "2025-04-23 14:26: **********Val Epoch 63: average Loss: 0.075437\n",
      "2025-04-23 14:26: **********Val Epoch 63: average Loss: 0.075437\n",
      "2025-04-23 14:26: **********Val Epoch 63: average Loss: 0.075437\n",
      "2025-04-23 14:26: Train Epoch 64: 0/37 Loss: 0.055914\n",
      "2025-04-23 14:26: Train Epoch 64: 0/37 Loss: 0.055914\n",
      "2025-04-23 14:26: Train Epoch 64: 0/37 Loss: 0.055914\n",
      "2025-04-23 14:26: Train Epoch 64: 20/37 Loss: 0.021312\n",
      "2025-04-23 14:26: Train Epoch 64: 20/37 Loss: 0.021312\n",
      "2025-04-23 14:26: Train Epoch 64: 20/37 Loss: 0.021312\n",
      "2025-04-23 14:26: **********Train Epoch 64: averaged Loss: 0.036345\n",
      "2025-04-23 14:26: **********Train Epoch 64: averaged Loss: 0.036345\n",
      "2025-04-23 14:26: **********Train Epoch 64: averaged Loss: 0.036345\n",
      "2025-04-23 14:26: **********Val Epoch 64: average Loss: 0.061390\n",
      "2025-04-23 14:26: **********Val Epoch 64: average Loss: 0.061390\n",
      "2025-04-23 14:26: **********Val Epoch 64: average Loss: 0.061390\n",
      "2025-04-23 14:26: Train Epoch 65: 0/37 Loss: 0.120658\n",
      "2025-04-23 14:26: Train Epoch 65: 0/37 Loss: 0.120658\n",
      "2025-04-23 14:26: Train Epoch 65: 0/37 Loss: 0.120658\n",
      "2025-04-23 14:26: Train Epoch 65: 20/37 Loss: 0.023275\n",
      "2025-04-23 14:26: Train Epoch 65: 20/37 Loss: 0.023275\n",
      "2025-04-23 14:26: Train Epoch 65: 20/37 Loss: 0.023275\n",
      "2025-04-23 14:26: **********Train Epoch 65: averaged Loss: 0.046641\n",
      "2025-04-23 14:26: **********Train Epoch 65: averaged Loss: 0.046641\n",
      "2025-04-23 14:26: **********Train Epoch 65: averaged Loss: 0.046641\n",
      "2025-04-23 14:26: **********Val Epoch 65: average Loss: 0.054645\n",
      "2025-04-23 14:26: **********Val Epoch 65: average Loss: 0.054645\n",
      "2025-04-23 14:26: **********Val Epoch 65: average Loss: 0.054645\n",
      "2025-04-23 14:26: Train Epoch 66: 0/37 Loss: 0.010937\n",
      "2025-04-23 14:26: Train Epoch 66: 0/37 Loss: 0.010937\n",
      "2025-04-23 14:26: Train Epoch 66: 0/37 Loss: 0.010937\n",
      "2025-04-23 14:26: Train Epoch 66: 20/37 Loss: 0.006402\n",
      "2025-04-23 14:26: Train Epoch 66: 20/37 Loss: 0.006402\n",
      "2025-04-23 14:26: Train Epoch 66: 20/37 Loss: 0.006402\n",
      "2025-04-23 14:26: **********Train Epoch 66: averaged Loss: 0.026687\n",
      "2025-04-23 14:26: **********Train Epoch 66: averaged Loss: 0.026687\n",
      "2025-04-23 14:26: **********Train Epoch 66: averaged Loss: 0.026687\n",
      "2025-04-23 14:26: **********Val Epoch 66: average Loss: 0.052276\n",
      "2025-04-23 14:26: **********Val Epoch 66: average Loss: 0.052276\n",
      "2025-04-23 14:26: **********Val Epoch 66: average Loss: 0.052276\n",
      "2025-04-23 14:26: Train Epoch 67: 0/37 Loss: 0.017089\n",
      "2025-04-23 14:26: Train Epoch 67: 0/37 Loss: 0.017089\n",
      "2025-04-23 14:26: Train Epoch 67: 0/37 Loss: 0.017089\n",
      "2025-04-23 14:26: Train Epoch 67: 20/37 Loss: 0.055147\n",
      "2025-04-23 14:26: Train Epoch 67: 20/37 Loss: 0.055147\n",
      "2025-04-23 14:26: Train Epoch 67: 20/37 Loss: 0.055147\n",
      "2025-04-23 14:26: **********Train Epoch 67: averaged Loss: 0.029581\n",
      "2025-04-23 14:26: **********Train Epoch 67: averaged Loss: 0.029581\n",
      "2025-04-23 14:26: **********Train Epoch 67: averaged Loss: 0.029581\n",
      "2025-04-23 14:26: **********Val Epoch 67: average Loss: 0.072803\n",
      "2025-04-23 14:26: **********Val Epoch 67: average Loss: 0.072803\n",
      "2025-04-23 14:26: **********Val Epoch 67: average Loss: 0.072803\n",
      "2025-04-23 14:26: Train Epoch 68: 0/37 Loss: 0.088912\n",
      "2025-04-23 14:26: Train Epoch 68: 0/37 Loss: 0.088912\n",
      "2025-04-23 14:26: Train Epoch 68: 0/37 Loss: 0.088912\n",
      "2025-04-23 14:26: Train Epoch 68: 20/37 Loss: 0.011455\n",
      "2025-04-23 14:26: Train Epoch 68: 20/37 Loss: 0.011455\n",
      "2025-04-23 14:26: Train Epoch 68: 20/37 Loss: 0.011455\n",
      "2025-04-23 14:26: **********Train Epoch 68: averaged Loss: 0.047292\n",
      "2025-04-23 14:26: **********Train Epoch 68: averaged Loss: 0.047292\n",
      "2025-04-23 14:26: **********Train Epoch 68: averaged Loss: 0.047292\n",
      "2025-04-23 14:26: **********Val Epoch 68: average Loss: 0.080235\n",
      "2025-04-23 14:26: **********Val Epoch 68: average Loss: 0.080235\n",
      "2025-04-23 14:26: **********Val Epoch 68: average Loss: 0.080235\n",
      "2025-04-23 14:26: Train Epoch 69: 0/37 Loss: 0.078153\n",
      "2025-04-23 14:26: Train Epoch 69: 0/37 Loss: 0.078153\n",
      "2025-04-23 14:26: Train Epoch 69: 0/37 Loss: 0.078153\n",
      "2025-04-23 14:27: Train Epoch 69: 20/37 Loss: 0.051263\n",
      "2025-04-23 14:27: Train Epoch 69: 20/37 Loss: 0.051263\n",
      "2025-04-23 14:27: Train Epoch 69: 20/37 Loss: 0.051263\n",
      "2025-04-23 14:27: **********Train Epoch 69: averaged Loss: 0.037303\n",
      "2025-04-23 14:27: **********Train Epoch 69: averaged Loss: 0.037303\n",
      "2025-04-23 14:27: **********Train Epoch 69: averaged Loss: 0.037303\n",
      "2025-04-23 14:27: **********Val Epoch 69: average Loss: 0.060393\n",
      "2025-04-23 14:27: **********Val Epoch 69: average Loss: 0.060393\n",
      "2025-04-23 14:27: **********Val Epoch 69: average Loss: 0.060393\n",
      "2025-04-23 14:27: Train Epoch 70: 0/37 Loss: 0.143478\n",
      "2025-04-23 14:27: Train Epoch 70: 0/37 Loss: 0.143478\n",
      "2025-04-23 14:27: Train Epoch 70: 0/37 Loss: 0.143478\n",
      "2025-04-23 14:27: Train Epoch 70: 20/37 Loss: 0.013402\n",
      "2025-04-23 14:27: Train Epoch 70: 20/37 Loss: 0.013402\n",
      "2025-04-23 14:27: Train Epoch 70: 20/37 Loss: 0.013402\n",
      "2025-04-23 14:27: **********Train Epoch 70: averaged Loss: 0.048448\n",
      "2025-04-23 14:27: **********Train Epoch 70: averaged Loss: 0.048448\n",
      "2025-04-23 14:27: **********Train Epoch 70: averaged Loss: 0.048448\n",
      "2025-04-23 14:27: **********Val Epoch 70: average Loss: 0.023902\n",
      "2025-04-23 14:27: **********Val Epoch 70: average Loss: 0.023902\n",
      "2025-04-23 14:27: **********Val Epoch 70: average Loss: 0.023902\n",
      "2025-04-23 14:27: Train Epoch 71: 0/37 Loss: 0.031855\n",
      "2025-04-23 14:27: Train Epoch 71: 0/37 Loss: 0.031855\n",
      "2025-04-23 14:27: Train Epoch 71: 0/37 Loss: 0.031855\n",
      "2025-04-23 14:27: Train Epoch 71: 20/37 Loss: 0.013476\n",
      "2025-04-23 14:27: Train Epoch 71: 20/37 Loss: 0.013476\n",
      "2025-04-23 14:27: Train Epoch 71: 20/37 Loss: 0.013476\n",
      "2025-04-23 14:27: **********Train Epoch 71: averaged Loss: 0.032235\n",
      "2025-04-23 14:27: **********Train Epoch 71: averaged Loss: 0.032235\n",
      "2025-04-23 14:27: **********Train Epoch 71: averaged Loss: 0.032235\n",
      "2025-04-23 14:27: **********Val Epoch 71: average Loss: 0.148185\n",
      "2025-04-23 14:27: **********Val Epoch 71: average Loss: 0.148185\n",
      "2025-04-23 14:27: **********Val Epoch 71: average Loss: 0.148185\n",
      "2025-04-23 14:27: Train Epoch 72: 0/37 Loss: 0.030522\n",
      "2025-04-23 14:27: Train Epoch 72: 0/37 Loss: 0.030522\n",
      "2025-04-23 14:27: Train Epoch 72: 0/37 Loss: 0.030522\n",
      "2025-04-23 14:27: Train Epoch 72: 20/37 Loss: 0.049959\n",
      "2025-04-23 14:27: Train Epoch 72: 20/37 Loss: 0.049959\n",
      "2025-04-23 14:27: Train Epoch 72: 20/37 Loss: 0.049959\n",
      "2025-04-23 14:27: **********Train Epoch 72: averaged Loss: 0.047804\n",
      "2025-04-23 14:27: **********Train Epoch 72: averaged Loss: 0.047804\n",
      "2025-04-23 14:27: **********Train Epoch 72: averaged Loss: 0.047804\n",
      "2025-04-23 14:27: **********Val Epoch 72: average Loss: 0.111374\n",
      "2025-04-23 14:27: **********Val Epoch 72: average Loss: 0.111374\n",
      "2025-04-23 14:27: **********Val Epoch 72: average Loss: 0.111374\n",
      "2025-04-23 14:27: Train Epoch 73: 0/37 Loss: 0.006251\n",
      "2025-04-23 14:27: Train Epoch 73: 0/37 Loss: 0.006251\n",
      "2025-04-23 14:27: Train Epoch 73: 0/37 Loss: 0.006251\n",
      "2025-04-23 14:27: Train Epoch 73: 20/37 Loss: 0.112184\n",
      "2025-04-23 14:27: Train Epoch 73: 20/37 Loss: 0.112184\n",
      "2025-04-23 14:27: Train Epoch 73: 20/37 Loss: 0.112184\n",
      "2025-04-23 14:27: **********Train Epoch 73: averaged Loss: 0.069948\n",
      "2025-04-23 14:27: **********Train Epoch 73: averaged Loss: 0.069948\n",
      "2025-04-23 14:27: **********Train Epoch 73: averaged Loss: 0.069948\n",
      "2025-04-23 14:27: **********Val Epoch 73: average Loss: 0.119967\n",
      "2025-04-23 14:27: **********Val Epoch 73: average Loss: 0.119967\n",
      "2025-04-23 14:27: **********Val Epoch 73: average Loss: 0.119967\n",
      "2025-04-23 14:27: Train Epoch 74: 0/37 Loss: 0.087816\n",
      "2025-04-23 14:27: Train Epoch 74: 0/37 Loss: 0.087816\n",
      "2025-04-23 14:27: Train Epoch 74: 0/37 Loss: 0.087816\n",
      "2025-04-23 14:27: Train Epoch 74: 20/37 Loss: 0.085922\n",
      "2025-04-23 14:27: Train Epoch 74: 20/37 Loss: 0.085922\n",
      "2025-04-23 14:27: Train Epoch 74: 20/37 Loss: 0.085922\n",
      "2025-04-23 14:27: **********Train Epoch 74: averaged Loss: 0.051028\n",
      "2025-04-23 14:27: **********Train Epoch 74: averaged Loss: 0.051028\n",
      "2025-04-23 14:27: **********Train Epoch 74: averaged Loss: 0.051028\n",
      "2025-04-23 14:27: **********Val Epoch 74: average Loss: 0.025060\n",
      "2025-04-23 14:27: **********Val Epoch 74: average Loss: 0.025060\n",
      "2025-04-23 14:27: **********Val Epoch 74: average Loss: 0.025060\n",
      "2025-04-23 14:27: Train Epoch 75: 0/37 Loss: 0.056670\n",
      "2025-04-23 14:27: Train Epoch 75: 0/37 Loss: 0.056670\n",
      "2025-04-23 14:27: Train Epoch 75: 0/37 Loss: 0.056670\n",
      "2025-04-23 14:27: Train Epoch 75: 20/37 Loss: 0.195146\n",
      "2025-04-23 14:27: Train Epoch 75: 20/37 Loss: 0.195146\n",
      "2025-04-23 14:27: Train Epoch 75: 20/37 Loss: 0.195146\n",
      "2025-04-23 14:27: **********Train Epoch 75: averaged Loss: 0.106359\n",
      "2025-04-23 14:27: **********Train Epoch 75: averaged Loss: 0.106359\n",
      "2025-04-23 14:27: **********Train Epoch 75: averaged Loss: 0.106359\n",
      "2025-04-23 14:27: **********Val Epoch 75: average Loss: 0.043472\n",
      "2025-04-23 14:27: **********Val Epoch 75: average Loss: 0.043472\n",
      "2025-04-23 14:27: **********Val Epoch 75: average Loss: 0.043472\n",
      "2025-04-23 14:27: Train Epoch 76: 0/37 Loss: 0.077760\n",
      "2025-04-23 14:27: Train Epoch 76: 0/37 Loss: 0.077760\n",
      "2025-04-23 14:27: Train Epoch 76: 0/37 Loss: 0.077760\n",
      "2025-04-23 14:27: Train Epoch 76: 20/37 Loss: 0.146198\n",
      "2025-04-23 14:27: Train Epoch 76: 20/37 Loss: 0.146198\n",
      "2025-04-23 14:27: Train Epoch 76: 20/37 Loss: 0.146198\n",
      "2025-04-23 14:27: **********Train Epoch 76: averaged Loss: 0.126635\n",
      "2025-04-23 14:27: **********Train Epoch 76: averaged Loss: 0.126635\n",
      "2025-04-23 14:27: **********Train Epoch 76: averaged Loss: 0.126635\n",
      "2025-04-23 14:27: **********Val Epoch 76: average Loss: 0.119408\n",
      "2025-04-23 14:27: **********Val Epoch 76: average Loss: 0.119408\n",
      "2025-04-23 14:27: **********Val Epoch 76: average Loss: 0.119408\n",
      "2025-04-23 14:27: Train Epoch 77: 0/37 Loss: 0.090865\n",
      "2025-04-23 14:27: Train Epoch 77: 0/37 Loss: 0.090865\n",
      "2025-04-23 14:27: Train Epoch 77: 0/37 Loss: 0.090865\n",
      "2025-04-23 14:27: Train Epoch 77: 20/37 Loss: 0.037853\n",
      "2025-04-23 14:27: Train Epoch 77: 20/37 Loss: 0.037853\n",
      "2025-04-23 14:27: Train Epoch 77: 20/37 Loss: 0.037853\n",
      "2025-04-23 14:27: **********Train Epoch 77: averaged Loss: 0.035077\n",
      "2025-04-23 14:27: **********Train Epoch 77: averaged Loss: 0.035077\n",
      "2025-04-23 14:27: **********Train Epoch 77: averaged Loss: 0.035077\n",
      "2025-04-23 14:27: **********Val Epoch 77: average Loss: 0.043644\n",
      "2025-04-23 14:27: **********Val Epoch 77: average Loss: 0.043644\n",
      "2025-04-23 14:27: **********Val Epoch 77: average Loss: 0.043644\n",
      "2025-04-23 14:27: Train Epoch 78: 0/37 Loss: 0.094584\n",
      "2025-04-23 14:27: Train Epoch 78: 0/37 Loss: 0.094584\n",
      "2025-04-23 14:27: Train Epoch 78: 0/37 Loss: 0.094584\n",
      "2025-04-23 14:27: Train Epoch 78: 20/37 Loss: 0.014139\n",
      "2025-04-23 14:27: Train Epoch 78: 20/37 Loss: 0.014139\n",
      "2025-04-23 14:27: Train Epoch 78: 20/37 Loss: 0.014139\n",
      "2025-04-23 14:27: **********Train Epoch 78: averaged Loss: 0.044379\n",
      "2025-04-23 14:27: **********Train Epoch 78: averaged Loss: 0.044379\n",
      "2025-04-23 14:27: **********Train Epoch 78: averaged Loss: 0.044379\n",
      "2025-04-23 14:27: **********Val Epoch 78: average Loss: 0.027187\n",
      "2025-04-23 14:27: **********Val Epoch 78: average Loss: 0.027187\n",
      "2025-04-23 14:27: **********Val Epoch 78: average Loss: 0.027187\n",
      "2025-04-23 14:27: Train Epoch 79: 0/37 Loss: 0.100404\n",
      "2025-04-23 14:27: Train Epoch 79: 0/37 Loss: 0.100404\n",
      "2025-04-23 14:27: Train Epoch 79: 0/37 Loss: 0.100404\n",
      "2025-04-23 14:27: Train Epoch 79: 20/37 Loss: 0.006002\n",
      "2025-04-23 14:27: Train Epoch 79: 20/37 Loss: 0.006002\n",
      "2025-04-23 14:27: Train Epoch 79: 20/37 Loss: 0.006002\n",
      "2025-04-23 14:27: **********Train Epoch 79: averaged Loss: 0.047932\n",
      "2025-04-23 14:27: **********Train Epoch 79: averaged Loss: 0.047932\n",
      "2025-04-23 14:27: **********Train Epoch 79: averaged Loss: 0.047932\n",
      "2025-04-23 14:27: **********Val Epoch 79: average Loss: 0.071350\n",
      "2025-04-23 14:27: **********Val Epoch 79: average Loss: 0.071350\n",
      "2025-04-23 14:27: **********Val Epoch 79: average Loss: 0.071350\n",
      "2025-04-23 14:27: Train Epoch 80: 0/37 Loss: 0.095937\n",
      "2025-04-23 14:27: Train Epoch 80: 0/37 Loss: 0.095937\n",
      "2025-04-23 14:27: Train Epoch 80: 0/37 Loss: 0.095937\n",
      "2025-04-23 14:27: Train Epoch 80: 20/37 Loss: 0.007663\n",
      "2025-04-23 14:27: Train Epoch 80: 20/37 Loss: 0.007663\n",
      "2025-04-23 14:27: Train Epoch 80: 20/37 Loss: 0.007663\n",
      "2025-04-23 14:27: **********Train Epoch 80: averaged Loss: 0.031283\n",
      "2025-04-23 14:27: **********Train Epoch 80: averaged Loss: 0.031283\n",
      "2025-04-23 14:27: **********Train Epoch 80: averaged Loss: 0.031283\n",
      "2025-04-23 14:27: **********Val Epoch 80: average Loss: 0.038587\n",
      "2025-04-23 14:27: **********Val Epoch 80: average Loss: 0.038587\n",
      "2025-04-23 14:27: **********Val Epoch 80: average Loss: 0.038587\n",
      "2025-04-23 14:27: Train Epoch 81: 0/37 Loss: 0.098287\n",
      "2025-04-23 14:27: Train Epoch 81: 0/37 Loss: 0.098287\n",
      "2025-04-23 14:27: Train Epoch 81: 0/37 Loss: 0.098287\n",
      "2025-04-23 14:27: Train Epoch 81: 20/37 Loss: 0.021471\n",
      "2025-04-23 14:27: Train Epoch 81: 20/37 Loss: 0.021471\n",
      "2025-04-23 14:27: Train Epoch 81: 20/37 Loss: 0.021471\n",
      "2025-04-23 14:27: **********Train Epoch 81: averaged Loss: 0.040282\n",
      "2025-04-23 14:27: **********Train Epoch 81: averaged Loss: 0.040282\n",
      "2025-04-23 14:27: **********Train Epoch 81: averaged Loss: 0.040282\n",
      "2025-04-23 14:27: **********Val Epoch 81: average Loss: 0.017848\n",
      "2025-04-23 14:27: **********Val Epoch 81: average Loss: 0.017848\n",
      "2025-04-23 14:27: **********Val Epoch 81: average Loss: 0.017848\n",
      "2025-04-23 14:27: Train Epoch 82: 0/37 Loss: 0.069310\n",
      "2025-04-23 14:27: Train Epoch 82: 0/37 Loss: 0.069310\n",
      "2025-04-23 14:27: Train Epoch 82: 0/37 Loss: 0.069310\n",
      "2025-04-23 14:27: Train Epoch 82: 20/37 Loss: 0.018675\n",
      "2025-04-23 14:27: Train Epoch 82: 20/37 Loss: 0.018675\n",
      "2025-04-23 14:27: Train Epoch 82: 20/37 Loss: 0.018675\n",
      "2025-04-23 14:27: **********Train Epoch 82: averaged Loss: 0.025437\n",
      "2025-04-23 14:27: **********Train Epoch 82: averaged Loss: 0.025437\n",
      "2025-04-23 14:27: **********Train Epoch 82: averaged Loss: 0.025437\n",
      "2025-04-23 14:27: **********Val Epoch 82: average Loss: 0.054762\n",
      "2025-04-23 14:27: **********Val Epoch 82: average Loss: 0.054762\n",
      "2025-04-23 14:27: **********Val Epoch 82: average Loss: 0.054762\n",
      "2025-04-23 14:27: Train Epoch 83: 0/37 Loss: 0.041981\n",
      "2025-04-23 14:27: Train Epoch 83: 0/37 Loss: 0.041981\n",
      "2025-04-23 14:27: Train Epoch 83: 0/37 Loss: 0.041981\n",
      "2025-04-23 14:27: Train Epoch 83: 20/37 Loss: 0.062793\n",
      "2025-04-23 14:27: Train Epoch 83: 20/37 Loss: 0.062793\n",
      "2025-04-23 14:27: Train Epoch 83: 20/37 Loss: 0.062793\n",
      "2025-04-23 14:27: **********Train Epoch 83: averaged Loss: 0.057579\n",
      "2025-04-23 14:27: **********Train Epoch 83: averaged Loss: 0.057579\n",
      "2025-04-23 14:27: **********Train Epoch 83: averaged Loss: 0.057579\n",
      "2025-04-23 14:27: **********Val Epoch 83: average Loss: 0.109474\n",
      "2025-04-23 14:27: **********Val Epoch 83: average Loss: 0.109474\n",
      "2025-04-23 14:27: **********Val Epoch 83: average Loss: 0.109474\n",
      "2025-04-23 14:27: Train Epoch 84: 0/37 Loss: 0.058779\n",
      "2025-04-23 14:27: Train Epoch 84: 0/37 Loss: 0.058779\n",
      "2025-04-23 14:27: Train Epoch 84: 0/37 Loss: 0.058779\n",
      "2025-04-23 14:27: Train Epoch 84: 20/37 Loss: 0.057637\n",
      "2025-04-23 14:27: Train Epoch 84: 20/37 Loss: 0.057637\n",
      "2025-04-23 14:27: Train Epoch 84: 20/37 Loss: 0.057637\n",
      "2025-04-23 14:27: **********Train Epoch 84: averaged Loss: 0.036867\n",
      "2025-04-23 14:27: **********Train Epoch 84: averaged Loss: 0.036867\n",
      "2025-04-23 14:27: **********Train Epoch 84: averaged Loss: 0.036867\n",
      "2025-04-23 14:27: **********Val Epoch 84: average Loss: 0.054127\n",
      "2025-04-23 14:27: **********Val Epoch 84: average Loss: 0.054127\n",
      "2025-04-23 14:27: **********Val Epoch 84: average Loss: 0.054127\n",
      "2025-04-23 14:27: Train Epoch 85: 0/37 Loss: 0.185021\n",
      "2025-04-23 14:27: Train Epoch 85: 0/37 Loss: 0.185021\n",
      "2025-04-23 14:27: Train Epoch 85: 0/37 Loss: 0.185021\n",
      "2025-04-23 14:27: Train Epoch 85: 20/37 Loss: 0.044308\n",
      "2025-04-23 14:27: Train Epoch 85: 20/37 Loss: 0.044308\n",
      "2025-04-23 14:27: Train Epoch 85: 20/37 Loss: 0.044308\n",
      "2025-04-23 14:27: **********Train Epoch 85: averaged Loss: 0.066804\n",
      "2025-04-23 14:27: **********Train Epoch 85: averaged Loss: 0.066804\n",
      "2025-04-23 14:27: **********Train Epoch 85: averaged Loss: 0.066804\n",
      "2025-04-23 14:27: **********Val Epoch 85: average Loss: 0.039737\n",
      "2025-04-23 14:27: **********Val Epoch 85: average Loss: 0.039737\n",
      "2025-04-23 14:27: **********Val Epoch 85: average Loss: 0.039737\n",
      "2025-04-23 14:27: Train Epoch 86: 0/37 Loss: 0.060105\n",
      "2025-04-23 14:27: Train Epoch 86: 0/37 Loss: 0.060105\n",
      "2025-04-23 14:27: Train Epoch 86: 0/37 Loss: 0.060105\n",
      "2025-04-23 14:27: Train Epoch 86: 20/37 Loss: 0.007912\n",
      "2025-04-23 14:27: Train Epoch 86: 20/37 Loss: 0.007912\n",
      "2025-04-23 14:27: Train Epoch 86: 20/37 Loss: 0.007912\n",
      "2025-04-23 14:27: **********Train Epoch 86: averaged Loss: 0.022385\n",
      "2025-04-23 14:27: **********Train Epoch 86: averaged Loss: 0.022385\n",
      "2025-04-23 14:27: **********Train Epoch 86: averaged Loss: 0.022385\n",
      "2025-04-23 14:27: **********Val Epoch 86: average Loss: 0.067654\n",
      "2025-04-23 14:27: **********Val Epoch 86: average Loss: 0.067654\n",
      "2025-04-23 14:27: **********Val Epoch 86: average Loss: 0.067654\n",
      "2025-04-23 14:27: Train Epoch 87: 0/37 Loss: 0.040030\n",
      "2025-04-23 14:27: Train Epoch 87: 0/37 Loss: 0.040030\n",
      "2025-04-23 14:27: Train Epoch 87: 0/37 Loss: 0.040030\n",
      "2025-04-23 14:27: Train Epoch 87: 20/37 Loss: 0.013970\n",
      "2025-04-23 14:27: Train Epoch 87: 20/37 Loss: 0.013970\n",
      "2025-04-23 14:27: Train Epoch 87: 20/37 Loss: 0.013970\n",
      "2025-04-23 14:27: **********Train Epoch 87: averaged Loss: 0.032915\n",
      "2025-04-23 14:27: **********Train Epoch 87: averaged Loss: 0.032915\n",
      "2025-04-23 14:27: **********Train Epoch 87: averaged Loss: 0.032915\n",
      "2025-04-23 14:27: **********Val Epoch 87: average Loss: 0.065484\n",
      "2025-04-23 14:27: **********Val Epoch 87: average Loss: 0.065484\n",
      "2025-04-23 14:27: **********Val Epoch 87: average Loss: 0.065484\n",
      "2025-04-23 14:27: Train Epoch 88: 0/37 Loss: 0.053989\n",
      "2025-04-23 14:27: Train Epoch 88: 0/37 Loss: 0.053989\n",
      "2025-04-23 14:27: Train Epoch 88: 0/37 Loss: 0.053989\n",
      "2025-04-23 14:27: Train Epoch 88: 20/37 Loss: 0.007404\n",
      "2025-04-23 14:27: Train Epoch 88: 20/37 Loss: 0.007404\n",
      "2025-04-23 14:27: Train Epoch 88: 20/37 Loss: 0.007404\n",
      "2025-04-23 14:27: **********Train Epoch 88: averaged Loss: 0.023535\n",
      "2025-04-23 14:27: **********Train Epoch 88: averaged Loss: 0.023535\n",
      "2025-04-23 14:27: **********Train Epoch 88: averaged Loss: 0.023535\n",
      "2025-04-23 14:27: **********Val Epoch 88: average Loss: 0.090429\n",
      "2025-04-23 14:27: **********Val Epoch 88: average Loss: 0.090429\n",
      "2025-04-23 14:27: **********Val Epoch 88: average Loss: 0.090429\n",
      "2025-04-23 14:27: Train Epoch 89: 0/37 Loss: 0.076904\n",
      "2025-04-23 14:27: Train Epoch 89: 0/37 Loss: 0.076904\n",
      "2025-04-23 14:27: Train Epoch 89: 0/37 Loss: 0.076904\n",
      "2025-04-23 14:27: Train Epoch 89: 20/37 Loss: 0.014241\n",
      "2025-04-23 14:27: Train Epoch 89: 20/37 Loss: 0.014241\n",
      "2025-04-23 14:27: Train Epoch 89: 20/37 Loss: 0.014241\n",
      "2025-04-23 14:27: **********Train Epoch 89: averaged Loss: 0.047881\n",
      "2025-04-23 14:27: **********Train Epoch 89: averaged Loss: 0.047881\n",
      "2025-04-23 14:27: **********Train Epoch 89: averaged Loss: 0.047881\n",
      "2025-04-23 14:27: **********Val Epoch 89: average Loss: 0.073194\n",
      "2025-04-23 14:27: **********Val Epoch 89: average Loss: 0.073194\n",
      "2025-04-23 14:27: **********Val Epoch 89: average Loss: 0.073194\n",
      "2025-04-23 14:27: Train Epoch 90: 0/37 Loss: 0.054480\n",
      "2025-04-23 14:27: Train Epoch 90: 0/37 Loss: 0.054480\n",
      "2025-04-23 14:27: Train Epoch 90: 0/37 Loss: 0.054480\n",
      "2025-04-23 14:27: Train Epoch 90: 20/37 Loss: 0.019724\n",
      "2025-04-23 14:27: Train Epoch 90: 20/37 Loss: 0.019724\n",
      "2025-04-23 14:27: Train Epoch 90: 20/37 Loss: 0.019724\n",
      "2025-04-23 14:27: **********Train Epoch 90: averaged Loss: 0.026775\n",
      "2025-04-23 14:27: **********Train Epoch 90: averaged Loss: 0.026775\n",
      "2025-04-23 14:27: **********Train Epoch 90: averaged Loss: 0.026775\n",
      "2025-04-23 14:27: **********Val Epoch 90: average Loss: 0.042954\n",
      "2025-04-23 14:27: **********Val Epoch 90: average Loss: 0.042954\n",
      "2025-04-23 14:27: **********Val Epoch 90: average Loss: 0.042954\n",
      "2025-04-23 14:27: Train Epoch 91: 0/37 Loss: 0.122902\n",
      "2025-04-23 14:27: Train Epoch 91: 0/37 Loss: 0.122902\n",
      "2025-04-23 14:27: Train Epoch 91: 0/37 Loss: 0.122902\n",
      "2025-04-23 14:27: Train Epoch 91: 20/37 Loss: 0.008617\n",
      "2025-04-23 14:27: Train Epoch 91: 20/37 Loss: 0.008617\n",
      "2025-04-23 14:27: Train Epoch 91: 20/37 Loss: 0.008617\n",
      "2025-04-23 14:27: **********Train Epoch 91: averaged Loss: 0.047170\n",
      "2025-04-23 14:27: **********Train Epoch 91: averaged Loss: 0.047170\n",
      "2025-04-23 14:27: **********Train Epoch 91: averaged Loss: 0.047170\n",
      "2025-04-23 14:27: **********Val Epoch 91: average Loss: 0.086424\n",
      "2025-04-23 14:27: **********Val Epoch 91: average Loss: 0.086424\n",
      "2025-04-23 14:27: **********Val Epoch 91: average Loss: 0.086424\n",
      "2025-04-23 14:27: Train Epoch 92: 0/37 Loss: 0.018576\n",
      "2025-04-23 14:27: Train Epoch 92: 0/37 Loss: 0.018576\n",
      "2025-04-23 14:27: Train Epoch 92: 0/37 Loss: 0.018576\n",
      "2025-04-23 14:27: Train Epoch 92: 20/37 Loss: 0.010078\n",
      "2025-04-23 14:27: Train Epoch 92: 20/37 Loss: 0.010078\n",
      "2025-04-23 14:27: Train Epoch 92: 20/37 Loss: 0.010078\n",
      "2025-04-23 14:27: **********Train Epoch 92: averaged Loss: 0.019965\n",
      "2025-04-23 14:27: **********Train Epoch 92: averaged Loss: 0.019965\n",
      "2025-04-23 14:27: **********Train Epoch 92: averaged Loss: 0.019965\n",
      "2025-04-23 14:27: **********Val Epoch 92: average Loss: 0.028312\n",
      "2025-04-23 14:27: **********Val Epoch 92: average Loss: 0.028312\n",
      "2025-04-23 14:27: **********Val Epoch 92: average Loss: 0.028312\n",
      "2025-04-23 14:27: Train Epoch 93: 0/37 Loss: 0.005740\n",
      "2025-04-23 14:27: Train Epoch 93: 0/37 Loss: 0.005740\n",
      "2025-04-23 14:27: Train Epoch 93: 0/37 Loss: 0.005740\n",
      "2025-04-23 14:27: Train Epoch 93: 20/37 Loss: 0.030310\n",
      "2025-04-23 14:27: Train Epoch 93: 20/37 Loss: 0.030310\n",
      "2025-04-23 14:27: Train Epoch 93: 20/37 Loss: 0.030310\n",
      "2025-04-23 14:27: **********Train Epoch 93: averaged Loss: 0.045676\n",
      "2025-04-23 14:27: **********Train Epoch 93: averaged Loss: 0.045676\n",
      "2025-04-23 14:27: **********Train Epoch 93: averaged Loss: 0.045676\n",
      "2025-04-23 14:27: **********Val Epoch 93: average Loss: 0.044879\n",
      "2025-04-23 14:27: **********Val Epoch 93: average Loss: 0.044879\n",
      "2025-04-23 14:27: **********Val Epoch 93: average Loss: 0.044879\n",
      "2025-04-23 14:27: Train Epoch 94: 0/37 Loss: 0.087548\n",
      "2025-04-23 14:27: Train Epoch 94: 0/37 Loss: 0.087548\n",
      "2025-04-23 14:27: Train Epoch 94: 0/37 Loss: 0.087548\n",
      "2025-04-23 14:27: Train Epoch 94: 20/37 Loss: 0.038899\n",
      "2025-04-23 14:27: Train Epoch 94: 20/37 Loss: 0.038899\n",
      "2025-04-23 14:27: Train Epoch 94: 20/37 Loss: 0.038899\n",
      "2025-04-23 14:27: **********Train Epoch 94: averaged Loss: 0.057367\n",
      "2025-04-23 14:27: **********Train Epoch 94: averaged Loss: 0.057367\n",
      "2025-04-23 14:27: **********Train Epoch 94: averaged Loss: 0.057367\n",
      "2025-04-23 14:27: **********Val Epoch 94: average Loss: 0.029913\n",
      "2025-04-23 14:27: **********Val Epoch 94: average Loss: 0.029913\n",
      "2025-04-23 14:27: **********Val Epoch 94: average Loss: 0.029913\n",
      "2025-04-23 14:27: Train Epoch 95: 0/37 Loss: 0.090027\n",
      "2025-04-23 14:27: Train Epoch 95: 0/37 Loss: 0.090027\n",
      "2025-04-23 14:27: Train Epoch 95: 0/37 Loss: 0.090027\n",
      "2025-04-23 14:27: Train Epoch 95: 20/37 Loss: 0.126313\n",
      "2025-04-23 14:27: Train Epoch 95: 20/37 Loss: 0.126313\n",
      "2025-04-23 14:27: Train Epoch 95: 20/37 Loss: 0.126313\n",
      "2025-04-23 14:27: **********Train Epoch 95: averaged Loss: 0.058089\n",
      "2025-04-23 14:27: **********Train Epoch 95: averaged Loss: 0.058089\n",
      "2025-04-23 14:27: **********Train Epoch 95: averaged Loss: 0.058089\n",
      "2025-04-23 14:27: **********Val Epoch 95: average Loss: 0.045557\n",
      "2025-04-23 14:27: **********Val Epoch 95: average Loss: 0.045557\n",
      "2025-04-23 14:27: **********Val Epoch 95: average Loss: 0.045557\n",
      "2025-04-23 14:27: Train Epoch 96: 0/37 Loss: 0.089551\n",
      "2025-04-23 14:27: Train Epoch 96: 0/37 Loss: 0.089551\n",
      "2025-04-23 14:27: Train Epoch 96: 0/37 Loss: 0.089551\n",
      "2025-04-23 14:27: Train Epoch 96: 20/37 Loss: 0.025629\n",
      "2025-04-23 14:27: Train Epoch 96: 20/37 Loss: 0.025629\n",
      "2025-04-23 14:27: Train Epoch 96: 20/37 Loss: 0.025629\n",
      "2025-04-23 14:27: **********Train Epoch 96: averaged Loss: 0.035729\n",
      "2025-04-23 14:27: **********Train Epoch 96: averaged Loss: 0.035729\n",
      "2025-04-23 14:27: **********Train Epoch 96: averaged Loss: 0.035729\n",
      "2025-04-23 14:27: **********Val Epoch 96: average Loss: 0.010701\n",
      "2025-04-23 14:27: **********Val Epoch 96: average Loss: 0.010701\n",
      "2025-04-23 14:27: **********Val Epoch 96: average Loss: 0.010701\n",
      "2025-04-23 14:27: *********************************Current best model saved!\n",
      "2025-04-23 14:27: *********************************Current best model saved!\n",
      "2025-04-23 14:27: *********************************Current best model saved!\n",
      "2025-04-23 14:27: Train Epoch 97: 0/37 Loss: 0.042613\n",
      "2025-04-23 14:27: Train Epoch 97: 0/37 Loss: 0.042613\n",
      "2025-04-23 14:27: Train Epoch 97: 0/37 Loss: 0.042613\n",
      "2025-04-23 14:27: Train Epoch 97: 20/37 Loss: 0.025393\n",
      "2025-04-23 14:27: Train Epoch 97: 20/37 Loss: 0.025393\n",
      "2025-04-23 14:27: Train Epoch 97: 20/37 Loss: 0.025393\n",
      "2025-04-23 14:27: **********Train Epoch 97: averaged Loss: 0.043114\n",
      "2025-04-23 14:27: **********Train Epoch 97: averaged Loss: 0.043114\n",
      "2025-04-23 14:27: **********Train Epoch 97: averaged Loss: 0.043114\n",
      "2025-04-23 14:27: **********Val Epoch 97: average Loss: 0.031323\n",
      "2025-04-23 14:27: **********Val Epoch 97: average Loss: 0.031323\n",
      "2025-04-23 14:27: **********Val Epoch 97: average Loss: 0.031323\n",
      "2025-04-23 14:27: Train Epoch 98: 0/37 Loss: 0.062720\n",
      "2025-04-23 14:27: Train Epoch 98: 0/37 Loss: 0.062720\n",
      "2025-04-23 14:27: Train Epoch 98: 0/37 Loss: 0.062720\n",
      "2025-04-23 14:27: Train Epoch 98: 20/37 Loss: 0.012065\n",
      "2025-04-23 14:27: Train Epoch 98: 20/37 Loss: 0.012065\n",
      "2025-04-23 14:27: Train Epoch 98: 20/37 Loss: 0.012065\n",
      "2025-04-23 14:27: **********Train Epoch 98: averaged Loss: 0.040544\n",
      "2025-04-23 14:27: **********Train Epoch 98: averaged Loss: 0.040544\n",
      "2025-04-23 14:27: **********Train Epoch 98: averaged Loss: 0.040544\n",
      "2025-04-23 14:27: **********Val Epoch 98: average Loss: 0.029463\n",
      "2025-04-23 14:27: **********Val Epoch 98: average Loss: 0.029463\n",
      "2025-04-23 14:27: **********Val Epoch 98: average Loss: 0.029463\n",
      "2025-04-23 14:27: Train Epoch 99: 0/37 Loss: 0.075591\n",
      "2025-04-23 14:27: Train Epoch 99: 0/37 Loss: 0.075591\n",
      "2025-04-23 14:27: Train Epoch 99: 0/37 Loss: 0.075591\n",
      "2025-04-23 14:27: Train Epoch 99: 20/37 Loss: 0.043218\n",
      "2025-04-23 14:27: Train Epoch 99: 20/37 Loss: 0.043218\n",
      "2025-04-23 14:27: Train Epoch 99: 20/37 Loss: 0.043218\n",
      "2025-04-23 14:27: **********Train Epoch 99: averaged Loss: 0.038431\n",
      "2025-04-23 14:27: **********Train Epoch 99: averaged Loss: 0.038431\n",
      "2025-04-23 14:27: **********Train Epoch 99: averaged Loss: 0.038431\n",
      "2025-04-23 14:27: **********Val Epoch 99: average Loss: 0.030654\n",
      "2025-04-23 14:27: **********Val Epoch 99: average Loss: 0.030654\n",
      "2025-04-23 14:27: **********Val Epoch 99: average Loss: 0.030654\n",
      "2025-04-23 14:27: Train Epoch 100: 0/37 Loss: 0.087273\n",
      "2025-04-23 14:27: Train Epoch 100: 0/37 Loss: 0.087273\n",
      "2025-04-23 14:27: Train Epoch 100: 0/37 Loss: 0.087273\n",
      "2025-04-23 14:27: Train Epoch 100: 20/37 Loss: 0.019027\n",
      "2025-04-23 14:27: Train Epoch 100: 20/37 Loss: 0.019027\n",
      "2025-04-23 14:27: Train Epoch 100: 20/37 Loss: 0.019027\n",
      "2025-04-23 14:27: **********Train Epoch 100: averaged Loss: 0.032992\n",
      "2025-04-23 14:27: **********Train Epoch 100: averaged Loss: 0.032992\n",
      "2025-04-23 14:27: **********Train Epoch 100: averaged Loss: 0.032992\n",
      "2025-04-23 14:27: **********Val Epoch 100: average Loss: 0.086916\n",
      "2025-04-23 14:27: **********Val Epoch 100: average Loss: 0.086916\n",
      "2025-04-23 14:27: **********Val Epoch 100: average Loss: 0.086916\n",
      "2025-04-23 14:27: Train Epoch 101: 0/37 Loss: 0.049755\n",
      "2025-04-23 14:27: Train Epoch 101: 0/37 Loss: 0.049755\n",
      "2025-04-23 14:27: Train Epoch 101: 0/37 Loss: 0.049755\n",
      "2025-04-23 14:27: Train Epoch 101: 20/37 Loss: 0.020072\n",
      "2025-04-23 14:27: Train Epoch 101: 20/37 Loss: 0.020072\n",
      "2025-04-23 14:27: Train Epoch 101: 20/37 Loss: 0.020072\n",
      "2025-04-23 14:27: **********Train Epoch 101: averaged Loss: 0.032523\n",
      "2025-04-23 14:27: **********Train Epoch 101: averaged Loss: 0.032523\n",
      "2025-04-23 14:27: **********Train Epoch 101: averaged Loss: 0.032523\n",
      "2025-04-23 14:27: **********Val Epoch 101: average Loss: 0.016206\n",
      "2025-04-23 14:27: **********Val Epoch 101: average Loss: 0.016206\n",
      "2025-04-23 14:27: **********Val Epoch 101: average Loss: 0.016206\n",
      "2025-04-23 14:27: Train Epoch 102: 0/37 Loss: 0.076352\n",
      "2025-04-23 14:27: Train Epoch 102: 0/37 Loss: 0.076352\n",
      "2025-04-23 14:27: Train Epoch 102: 0/37 Loss: 0.076352\n",
      "2025-04-23 14:27: Train Epoch 102: 20/37 Loss: 0.030874\n",
      "2025-04-23 14:27: Train Epoch 102: 20/37 Loss: 0.030874\n",
      "2025-04-23 14:27: Train Epoch 102: 20/37 Loss: 0.030874\n",
      "2025-04-23 14:27: **********Train Epoch 102: averaged Loss: 0.031839\n",
      "2025-04-23 14:27: **********Train Epoch 102: averaged Loss: 0.031839\n",
      "2025-04-23 14:27: **********Train Epoch 102: averaged Loss: 0.031839\n",
      "2025-04-23 14:27: **********Val Epoch 102: average Loss: 0.066515\n",
      "2025-04-23 14:27: **********Val Epoch 102: average Loss: 0.066515\n",
      "2025-04-23 14:27: **********Val Epoch 102: average Loss: 0.066515\n",
      "2025-04-23 14:27: Train Epoch 103: 0/37 Loss: 0.037235\n",
      "2025-04-23 14:27: Train Epoch 103: 0/37 Loss: 0.037235\n",
      "2025-04-23 14:27: Train Epoch 103: 0/37 Loss: 0.037235\n",
      "2025-04-23 14:27: Train Epoch 103: 20/37 Loss: 0.014555\n",
      "2025-04-23 14:27: Train Epoch 103: 20/37 Loss: 0.014555\n",
      "2025-04-23 14:27: Train Epoch 103: 20/37 Loss: 0.014555\n",
      "2025-04-23 14:27: **********Train Epoch 103: averaged Loss: 0.029219\n",
      "2025-04-23 14:27: **********Train Epoch 103: averaged Loss: 0.029219\n",
      "2025-04-23 14:27: **********Train Epoch 103: averaged Loss: 0.029219\n",
      "2025-04-23 14:27: **********Val Epoch 103: average Loss: 0.087473\n",
      "2025-04-23 14:27: **********Val Epoch 103: average Loss: 0.087473\n",
      "2025-04-23 14:27: **********Val Epoch 103: average Loss: 0.087473\n",
      "2025-04-23 14:27: Train Epoch 104: 0/37 Loss: 0.069952\n",
      "2025-04-23 14:27: Train Epoch 104: 0/37 Loss: 0.069952\n",
      "2025-04-23 14:27: Train Epoch 104: 0/37 Loss: 0.069952\n",
      "2025-04-23 14:27: Train Epoch 104: 20/37 Loss: 0.009952\n",
      "2025-04-23 14:27: Train Epoch 104: 20/37 Loss: 0.009952\n",
      "2025-04-23 14:27: Train Epoch 104: 20/37 Loss: 0.009952\n",
      "2025-04-23 14:27: **********Train Epoch 104: averaged Loss: 0.042331\n",
      "2025-04-23 14:27: **********Train Epoch 104: averaged Loss: 0.042331\n",
      "2025-04-23 14:27: **********Train Epoch 104: averaged Loss: 0.042331\n",
      "2025-04-23 14:27: **********Val Epoch 104: average Loss: 0.077924\n",
      "2025-04-23 14:27: **********Val Epoch 104: average Loss: 0.077924\n",
      "2025-04-23 14:27: **********Val Epoch 104: average Loss: 0.077924\n",
      "2025-04-23 14:27: Train Epoch 105: 0/37 Loss: 0.040238\n",
      "2025-04-23 14:27: Train Epoch 105: 0/37 Loss: 0.040238\n",
      "2025-04-23 14:27: Train Epoch 105: 0/37 Loss: 0.040238\n",
      "2025-04-23 14:28: Train Epoch 105: 20/37 Loss: 0.017880\n",
      "2025-04-23 14:28: Train Epoch 105: 20/37 Loss: 0.017880\n",
      "2025-04-23 14:28: Train Epoch 105: 20/37 Loss: 0.017880\n",
      "2025-04-23 14:28: **********Train Epoch 105: averaged Loss: 0.033804\n",
      "2025-04-23 14:28: **********Train Epoch 105: averaged Loss: 0.033804\n",
      "2025-04-23 14:28: **********Train Epoch 105: averaged Loss: 0.033804\n",
      "2025-04-23 14:28: **********Val Epoch 105: average Loss: 0.080292\n",
      "2025-04-23 14:28: **********Val Epoch 105: average Loss: 0.080292\n",
      "2025-04-23 14:28: **********Val Epoch 105: average Loss: 0.080292\n",
      "2025-04-23 14:28: Train Epoch 106: 0/37 Loss: 0.069841\n",
      "2025-04-23 14:28: Train Epoch 106: 0/37 Loss: 0.069841\n",
      "2025-04-23 14:28: Train Epoch 106: 0/37 Loss: 0.069841\n",
      "2025-04-23 14:28: Train Epoch 106: 20/37 Loss: 0.009056\n",
      "2025-04-23 14:28: Train Epoch 106: 20/37 Loss: 0.009056\n",
      "2025-04-23 14:28: Train Epoch 106: 20/37 Loss: 0.009056\n",
      "2025-04-23 14:28: **********Train Epoch 106: averaged Loss: 0.034335\n",
      "2025-04-23 14:28: **********Train Epoch 106: averaged Loss: 0.034335\n",
      "2025-04-23 14:28: **********Train Epoch 106: averaged Loss: 0.034335\n",
      "2025-04-23 14:28: **********Val Epoch 106: average Loss: 0.043343\n",
      "2025-04-23 14:28: **********Val Epoch 106: average Loss: 0.043343\n",
      "2025-04-23 14:28: **********Val Epoch 106: average Loss: 0.043343\n",
      "2025-04-23 14:28: Train Epoch 107: 0/37 Loss: 0.029190\n",
      "2025-04-23 14:28: Train Epoch 107: 0/37 Loss: 0.029190\n",
      "2025-04-23 14:28: Train Epoch 107: 0/37 Loss: 0.029190\n",
      "2025-04-23 14:28: Train Epoch 107: 20/37 Loss: 0.076531\n",
      "2025-04-23 14:28: Train Epoch 107: 20/37 Loss: 0.076531\n",
      "2025-04-23 14:28: Train Epoch 107: 20/37 Loss: 0.076531\n",
      "2025-04-23 14:28: **********Train Epoch 107: averaged Loss: 0.057291\n",
      "2025-04-23 14:28: **********Train Epoch 107: averaged Loss: 0.057291\n",
      "2025-04-23 14:28: **********Train Epoch 107: averaged Loss: 0.057291\n",
      "2025-04-23 14:28: **********Val Epoch 107: average Loss: 0.125662\n",
      "2025-04-23 14:28: **********Val Epoch 107: average Loss: 0.125662\n",
      "2025-04-23 14:28: **********Val Epoch 107: average Loss: 0.125662\n",
      "2025-04-23 14:28: Train Epoch 108: 0/37 Loss: 0.049861\n",
      "2025-04-23 14:28: Train Epoch 108: 0/37 Loss: 0.049861\n",
      "2025-04-23 14:28: Train Epoch 108: 0/37 Loss: 0.049861\n",
      "2025-04-23 14:28: Train Epoch 108: 20/37 Loss: 0.099570\n",
      "2025-04-23 14:28: Train Epoch 108: 20/37 Loss: 0.099570\n",
      "2025-04-23 14:28: Train Epoch 108: 20/37 Loss: 0.099570\n",
      "2025-04-23 14:28: **********Train Epoch 108: averaged Loss: 0.055533\n",
      "2025-04-23 14:28: **********Train Epoch 108: averaged Loss: 0.055533\n",
      "2025-04-23 14:28: **********Train Epoch 108: averaged Loss: 0.055533\n",
      "2025-04-23 14:28: **********Val Epoch 108: average Loss: 0.064550\n",
      "2025-04-23 14:28: **********Val Epoch 108: average Loss: 0.064550\n",
      "2025-04-23 14:28: **********Val Epoch 108: average Loss: 0.064550\n",
      "2025-04-23 14:28: Train Epoch 109: 0/37 Loss: 0.072016\n",
      "2025-04-23 14:28: Train Epoch 109: 0/37 Loss: 0.072016\n",
      "2025-04-23 14:28: Train Epoch 109: 0/37 Loss: 0.072016\n",
      "2025-04-23 14:28: Train Epoch 109: 20/37 Loss: 0.007242\n",
      "2025-04-23 14:28: Train Epoch 109: 20/37 Loss: 0.007242\n",
      "2025-04-23 14:28: Train Epoch 109: 20/37 Loss: 0.007242\n",
      "2025-04-23 14:28: **********Train Epoch 109: averaged Loss: 0.028540\n",
      "2025-04-23 14:28: **********Train Epoch 109: averaged Loss: 0.028540\n",
      "2025-04-23 14:28: **********Train Epoch 109: averaged Loss: 0.028540\n",
      "2025-04-23 14:28: **********Val Epoch 109: average Loss: 0.009039\n",
      "2025-04-23 14:28: **********Val Epoch 109: average Loss: 0.009039\n",
      "2025-04-23 14:28: **********Val Epoch 109: average Loss: 0.009039\n",
      "2025-04-23 14:28: *********************************Current best model saved!\n",
      "2025-04-23 14:28: *********************************Current best model saved!\n",
      "2025-04-23 14:28: *********************************Current best model saved!\n",
      "2025-04-23 14:28: Train Epoch 110: 0/37 Loss: 0.019135\n",
      "2025-04-23 14:28: Train Epoch 110: 0/37 Loss: 0.019135\n",
      "2025-04-23 14:28: Train Epoch 110: 0/37 Loss: 0.019135\n",
      "2025-04-23 14:28: Train Epoch 110: 20/37 Loss: 0.006445\n",
      "2025-04-23 14:28: Train Epoch 110: 20/37 Loss: 0.006445\n",
      "2025-04-23 14:28: Train Epoch 110: 20/37 Loss: 0.006445\n",
      "2025-04-23 14:28: **********Train Epoch 110: averaged Loss: 0.020890\n",
      "2025-04-23 14:28: **********Train Epoch 110: averaged Loss: 0.020890\n",
      "2025-04-23 14:28: **********Train Epoch 110: averaged Loss: 0.020890\n",
      "2025-04-23 14:28: **********Val Epoch 110: average Loss: 0.025863\n",
      "2025-04-23 14:28: **********Val Epoch 110: average Loss: 0.025863\n",
      "2025-04-23 14:28: **********Val Epoch 110: average Loss: 0.025863\n",
      "2025-04-23 14:28: Train Epoch 111: 0/37 Loss: 0.013750\n",
      "2025-04-23 14:28: Train Epoch 111: 0/37 Loss: 0.013750\n",
      "2025-04-23 14:28: Train Epoch 111: 0/37 Loss: 0.013750\n",
      "2025-04-23 14:28: Train Epoch 111: 20/37 Loss: 0.025410\n",
      "2025-04-23 14:28: Train Epoch 111: 20/37 Loss: 0.025410\n",
      "2025-04-23 14:28: Train Epoch 111: 20/37 Loss: 0.025410\n",
      "2025-04-23 14:28: **********Train Epoch 111: averaged Loss: 0.031801\n",
      "2025-04-23 14:28: **********Train Epoch 111: averaged Loss: 0.031801\n",
      "2025-04-23 14:28: **********Train Epoch 111: averaged Loss: 0.031801\n",
      "2025-04-23 14:28: **********Val Epoch 111: average Loss: 0.075092\n",
      "2025-04-23 14:28: **********Val Epoch 111: average Loss: 0.075092\n",
      "2025-04-23 14:28: **********Val Epoch 111: average Loss: 0.075092\n",
      "2025-04-23 14:28: Train Epoch 112: 0/37 Loss: 0.073750\n",
      "2025-04-23 14:28: Train Epoch 112: 0/37 Loss: 0.073750\n",
      "2025-04-23 14:28: Train Epoch 112: 0/37 Loss: 0.073750\n",
      "2025-04-23 14:28: Train Epoch 112: 20/37 Loss: 0.012214\n",
      "2025-04-23 14:28: Train Epoch 112: 20/37 Loss: 0.012214\n",
      "2025-04-23 14:28: Train Epoch 112: 20/37 Loss: 0.012214\n",
      "2025-04-23 14:28: **********Train Epoch 112: averaged Loss: 0.049632\n",
      "2025-04-23 14:28: **********Train Epoch 112: averaged Loss: 0.049632\n",
      "2025-04-23 14:28: **********Train Epoch 112: averaged Loss: 0.049632\n",
      "2025-04-23 14:28: **********Val Epoch 112: average Loss: 0.056066\n",
      "2025-04-23 14:28: **********Val Epoch 112: average Loss: 0.056066\n",
      "2025-04-23 14:28: **********Val Epoch 112: average Loss: 0.056066\n",
      "2025-04-23 14:28: Train Epoch 113: 0/37 Loss: 0.064040\n",
      "2025-04-23 14:28: Train Epoch 113: 0/37 Loss: 0.064040\n",
      "2025-04-23 14:28: Train Epoch 113: 0/37 Loss: 0.064040\n",
      "2025-04-23 14:28: Train Epoch 113: 20/37 Loss: 0.064784\n",
      "2025-04-23 14:28: Train Epoch 113: 20/37 Loss: 0.064784\n",
      "2025-04-23 14:28: Train Epoch 113: 20/37 Loss: 0.064784\n",
      "2025-04-23 14:28: **********Train Epoch 113: averaged Loss: 0.041287\n",
      "2025-04-23 14:28: **********Train Epoch 113: averaged Loss: 0.041287\n",
      "2025-04-23 14:28: **********Train Epoch 113: averaged Loss: 0.041287\n",
      "2025-04-23 14:28: **********Val Epoch 113: average Loss: 0.063374\n",
      "2025-04-23 14:28: **********Val Epoch 113: average Loss: 0.063374\n",
      "2025-04-23 14:28: **********Val Epoch 113: average Loss: 0.063374\n",
      "2025-04-23 14:28: Train Epoch 114: 0/37 Loss: 0.081940\n",
      "2025-04-23 14:28: Train Epoch 114: 0/37 Loss: 0.081940\n",
      "2025-04-23 14:28: Train Epoch 114: 0/37 Loss: 0.081940\n",
      "2025-04-23 14:28: Train Epoch 114: 20/37 Loss: 0.015639\n",
      "2025-04-23 14:28: Train Epoch 114: 20/37 Loss: 0.015639\n",
      "2025-04-23 14:28: Train Epoch 114: 20/37 Loss: 0.015639\n",
      "2025-04-23 14:28: **********Train Epoch 114: averaged Loss: 0.033690\n",
      "2025-04-23 14:28: **********Train Epoch 114: averaged Loss: 0.033690\n",
      "2025-04-23 14:28: **********Train Epoch 114: averaged Loss: 0.033690\n",
      "2025-04-23 14:28: **********Val Epoch 114: average Loss: 0.074671\n",
      "2025-04-23 14:28: **********Val Epoch 114: average Loss: 0.074671\n",
      "2025-04-23 14:28: **********Val Epoch 114: average Loss: 0.074671\n",
      "2025-04-23 14:28: Train Epoch 115: 0/37 Loss: 0.022428\n",
      "2025-04-23 14:28: Train Epoch 115: 0/37 Loss: 0.022428\n",
      "2025-04-23 14:28: Train Epoch 115: 0/37 Loss: 0.022428\n",
      "2025-04-23 14:28: Train Epoch 115: 20/37 Loss: 0.027347\n",
      "2025-04-23 14:28: Train Epoch 115: 20/37 Loss: 0.027347\n",
      "2025-04-23 14:28: Train Epoch 115: 20/37 Loss: 0.027347\n",
      "2025-04-23 14:28: **********Train Epoch 115: averaged Loss: 0.031296\n",
      "2025-04-23 14:28: **********Train Epoch 115: averaged Loss: 0.031296\n",
      "2025-04-23 14:28: **********Train Epoch 115: averaged Loss: 0.031296\n",
      "2025-04-23 14:28: **********Val Epoch 115: average Loss: 0.013006\n",
      "2025-04-23 14:28: **********Val Epoch 115: average Loss: 0.013006\n",
      "2025-04-23 14:28: **********Val Epoch 115: average Loss: 0.013006\n",
      "2025-04-23 14:28: Train Epoch 116: 0/37 Loss: 0.068765\n",
      "2025-04-23 14:28: Train Epoch 116: 0/37 Loss: 0.068765\n",
      "2025-04-23 14:28: Train Epoch 116: 0/37 Loss: 0.068765\n",
      "2025-04-23 14:28: Train Epoch 116: 20/37 Loss: 0.018331\n",
      "2025-04-23 14:28: Train Epoch 116: 20/37 Loss: 0.018331\n",
      "2025-04-23 14:28: Train Epoch 116: 20/37 Loss: 0.018331\n",
      "2025-04-23 14:28: **********Train Epoch 116: averaged Loss: 0.028363\n",
      "2025-04-23 14:28: **********Train Epoch 116: averaged Loss: 0.028363\n",
      "2025-04-23 14:28: **********Train Epoch 116: averaged Loss: 0.028363\n",
      "2025-04-23 14:28: **********Val Epoch 116: average Loss: 0.029363\n",
      "2025-04-23 14:28: **********Val Epoch 116: average Loss: 0.029363\n",
      "2025-04-23 14:28: **********Val Epoch 116: average Loss: 0.029363\n",
      "2025-04-23 14:28: Train Epoch 117: 0/37 Loss: 0.007855\n",
      "2025-04-23 14:28: Train Epoch 117: 0/37 Loss: 0.007855\n",
      "2025-04-23 14:28: Train Epoch 117: 0/37 Loss: 0.007855\n",
      "2025-04-23 14:28: Train Epoch 117: 20/37 Loss: 0.039468\n",
      "2025-04-23 14:28: Train Epoch 117: 20/37 Loss: 0.039468\n",
      "2025-04-23 14:28: Train Epoch 117: 20/37 Loss: 0.039468\n",
      "2025-04-23 14:28: **********Train Epoch 117: averaged Loss: 0.040483\n",
      "2025-04-23 14:28: **********Train Epoch 117: averaged Loss: 0.040483\n",
      "2025-04-23 14:28: **********Train Epoch 117: averaged Loss: 0.040483\n",
      "2025-04-23 14:28: **********Val Epoch 117: average Loss: 0.017002\n",
      "2025-04-23 14:28: **********Val Epoch 117: average Loss: 0.017002\n",
      "2025-04-23 14:28: **********Val Epoch 117: average Loss: 0.017002\n",
      "2025-04-23 14:28: Train Epoch 118: 0/37 Loss: 0.082091\n",
      "2025-04-23 14:28: Train Epoch 118: 0/37 Loss: 0.082091\n",
      "2025-04-23 14:28: Train Epoch 118: 0/37 Loss: 0.082091\n",
      "2025-04-23 14:28: Train Epoch 118: 20/37 Loss: 0.014193\n",
      "2025-04-23 14:28: Train Epoch 118: 20/37 Loss: 0.014193\n",
      "2025-04-23 14:28: Train Epoch 118: 20/37 Loss: 0.014193\n",
      "2025-04-23 14:28: **********Train Epoch 118: averaged Loss: 0.033476\n",
      "2025-04-23 14:28: **********Train Epoch 118: averaged Loss: 0.033476\n",
      "2025-04-23 14:28: **********Train Epoch 118: averaged Loss: 0.033476\n",
      "2025-04-23 14:28: **********Val Epoch 118: average Loss: 0.043108\n",
      "2025-04-23 14:28: **********Val Epoch 118: average Loss: 0.043108\n",
      "2025-04-23 14:28: **********Val Epoch 118: average Loss: 0.043108\n",
      "2025-04-23 14:28: Train Epoch 119: 0/37 Loss: 0.045227\n",
      "2025-04-23 14:28: Train Epoch 119: 0/37 Loss: 0.045227\n",
      "2025-04-23 14:28: Train Epoch 119: 0/37 Loss: 0.045227\n",
      "2025-04-23 14:28: Train Epoch 119: 20/37 Loss: 0.006458\n",
      "2025-04-23 14:28: Train Epoch 119: 20/37 Loss: 0.006458\n",
      "2025-04-23 14:28: Train Epoch 119: 20/37 Loss: 0.006458\n",
      "2025-04-23 14:28: **********Train Epoch 119: averaged Loss: 0.024185\n",
      "2025-04-23 14:28: **********Train Epoch 119: averaged Loss: 0.024185\n",
      "2025-04-23 14:28: **********Train Epoch 119: averaged Loss: 0.024185\n",
      "2025-04-23 14:28: **********Val Epoch 119: average Loss: 0.018713\n",
      "2025-04-23 14:28: **********Val Epoch 119: average Loss: 0.018713\n",
      "2025-04-23 14:28: **********Val Epoch 119: average Loss: 0.018713\n",
      "2025-04-23 14:28: Train Epoch 120: 0/37 Loss: 0.057849\n",
      "2025-04-23 14:28: Train Epoch 120: 0/37 Loss: 0.057849\n",
      "2025-04-23 14:28: Train Epoch 120: 0/37 Loss: 0.057849\n",
      "2025-04-23 14:28: Train Epoch 120: 20/37 Loss: 0.018957\n",
      "2025-04-23 14:28: Train Epoch 120: 20/37 Loss: 0.018957\n",
      "2025-04-23 14:28: Train Epoch 120: 20/37 Loss: 0.018957\n",
      "2025-04-23 14:28: **********Train Epoch 120: averaged Loss: 0.036149\n",
      "2025-04-23 14:28: **********Train Epoch 120: averaged Loss: 0.036149\n",
      "2025-04-23 14:28: **********Train Epoch 120: averaged Loss: 0.036149\n",
      "2025-04-23 14:28: **********Val Epoch 120: average Loss: 0.073464\n",
      "2025-04-23 14:28: **********Val Epoch 120: average Loss: 0.073464\n",
      "2025-04-23 14:28: **********Val Epoch 120: average Loss: 0.073464\n",
      "2025-04-23 14:28: Train Epoch 121: 0/37 Loss: 0.006036\n",
      "2025-04-23 14:28: Train Epoch 121: 0/37 Loss: 0.006036\n",
      "2025-04-23 14:28: Train Epoch 121: 0/37 Loss: 0.006036\n",
      "2025-04-23 14:28: Train Epoch 121: 20/37 Loss: 0.012534\n",
      "2025-04-23 14:28: Train Epoch 121: 20/37 Loss: 0.012534\n",
      "2025-04-23 14:28: Train Epoch 121: 20/37 Loss: 0.012534\n",
      "2025-04-23 14:28: **********Train Epoch 121: averaged Loss: 0.029397\n",
      "2025-04-23 14:28: **********Train Epoch 121: averaged Loss: 0.029397\n",
      "2025-04-23 14:28: **********Train Epoch 121: averaged Loss: 0.029397\n",
      "2025-04-23 14:28: **********Val Epoch 121: average Loss: 0.080515\n",
      "2025-04-23 14:28: **********Val Epoch 121: average Loss: 0.080515\n",
      "2025-04-23 14:28: **********Val Epoch 121: average Loss: 0.080515\n",
      "2025-04-23 14:28: Train Epoch 122: 0/37 Loss: 0.073422\n",
      "2025-04-23 14:28: Train Epoch 122: 0/37 Loss: 0.073422\n",
      "2025-04-23 14:28: Train Epoch 122: 0/37 Loss: 0.073422\n",
      "2025-04-23 14:28: Train Epoch 122: 20/37 Loss: 0.011687\n",
      "2025-04-23 14:28: Train Epoch 122: 20/37 Loss: 0.011687\n",
      "2025-04-23 14:28: Train Epoch 122: 20/37 Loss: 0.011687\n",
      "2025-04-23 14:28: **********Train Epoch 122: averaged Loss: 0.029182\n",
      "2025-04-23 14:28: **********Train Epoch 122: averaged Loss: 0.029182\n",
      "2025-04-23 14:28: **********Train Epoch 122: averaged Loss: 0.029182\n",
      "2025-04-23 14:28: **********Val Epoch 122: average Loss: 0.020866\n",
      "2025-04-23 14:28: **********Val Epoch 122: average Loss: 0.020866\n",
      "2025-04-23 14:28: **********Val Epoch 122: average Loss: 0.020866\n",
      "2025-04-23 14:28: Train Epoch 123: 0/37 Loss: 0.013587\n",
      "2025-04-23 14:28: Train Epoch 123: 0/37 Loss: 0.013587\n",
      "2025-04-23 14:28: Train Epoch 123: 0/37 Loss: 0.013587\n",
      "2025-04-23 14:28: Train Epoch 123: 20/37 Loss: 0.010216\n",
      "2025-04-23 14:28: Train Epoch 123: 20/37 Loss: 0.010216\n",
      "2025-04-23 14:28: Train Epoch 123: 20/37 Loss: 0.010216\n",
      "2025-04-23 14:28: **********Train Epoch 123: averaged Loss: 0.029097\n",
      "2025-04-23 14:28: **********Train Epoch 123: averaged Loss: 0.029097\n",
      "2025-04-23 14:28: **********Train Epoch 123: averaged Loss: 0.029097\n",
      "2025-04-23 14:28: **********Val Epoch 123: average Loss: 0.078043\n",
      "2025-04-23 14:28: **********Val Epoch 123: average Loss: 0.078043\n",
      "2025-04-23 14:28: **********Val Epoch 123: average Loss: 0.078043\n",
      "2025-04-23 14:28: Train Epoch 124: 0/37 Loss: 0.087384\n",
      "2025-04-23 14:28: Train Epoch 124: 0/37 Loss: 0.087384\n",
      "2025-04-23 14:28: Train Epoch 124: 0/37 Loss: 0.087384\n",
      "2025-04-23 14:28: Train Epoch 124: 20/37 Loss: 0.010934\n",
      "2025-04-23 14:28: Train Epoch 124: 20/37 Loss: 0.010934\n",
      "2025-04-23 14:28: Train Epoch 124: 20/37 Loss: 0.010934\n",
      "2025-04-23 14:28: **********Train Epoch 124: averaged Loss: 0.037519\n",
      "2025-04-23 14:28: **********Train Epoch 124: averaged Loss: 0.037519\n",
      "2025-04-23 14:28: **********Train Epoch 124: averaged Loss: 0.037519\n",
      "2025-04-23 14:28: **********Val Epoch 124: average Loss: 0.034518\n",
      "2025-04-23 14:28: **********Val Epoch 124: average Loss: 0.034518\n",
      "2025-04-23 14:28: **********Val Epoch 124: average Loss: 0.034518\n",
      "2025-04-23 14:28: Train Epoch 125: 0/37 Loss: 0.045874\n",
      "2025-04-23 14:28: Train Epoch 125: 0/37 Loss: 0.045874\n",
      "2025-04-23 14:28: Train Epoch 125: 0/37 Loss: 0.045874\n",
      "2025-04-23 14:28: Train Epoch 125: 20/37 Loss: 0.030647\n",
      "2025-04-23 14:28: Train Epoch 125: 20/37 Loss: 0.030647\n",
      "2025-04-23 14:28: Train Epoch 125: 20/37 Loss: 0.030647\n",
      "2025-04-23 14:28: **********Train Epoch 125: averaged Loss: 0.042922\n",
      "2025-04-23 14:28: **********Train Epoch 125: averaged Loss: 0.042922\n",
      "2025-04-23 14:28: **********Train Epoch 125: averaged Loss: 0.042922\n",
      "2025-04-23 14:28: **********Val Epoch 125: average Loss: 0.070406\n",
      "2025-04-23 14:28: **********Val Epoch 125: average Loss: 0.070406\n",
      "2025-04-23 14:28: **********Val Epoch 125: average Loss: 0.070406\n",
      "2025-04-23 14:28: Train Epoch 126: 0/37 Loss: 0.062155\n",
      "2025-04-23 14:28: Train Epoch 126: 0/37 Loss: 0.062155\n",
      "2025-04-23 14:28: Train Epoch 126: 0/37 Loss: 0.062155\n",
      "2025-04-23 14:28: Train Epoch 126: 20/37 Loss: 0.053272\n",
      "2025-04-23 14:28: Train Epoch 126: 20/37 Loss: 0.053272\n",
      "2025-04-23 14:28: Train Epoch 126: 20/37 Loss: 0.053272\n",
      "2025-04-23 14:28: **********Train Epoch 126: averaged Loss: 0.041160\n",
      "2025-04-23 14:28: **********Train Epoch 126: averaged Loss: 0.041160\n",
      "2025-04-23 14:28: **********Train Epoch 126: averaged Loss: 0.041160\n",
      "2025-04-23 14:28: **********Val Epoch 126: average Loss: 0.059868\n",
      "2025-04-23 14:28: **********Val Epoch 126: average Loss: 0.059868\n",
      "2025-04-23 14:28: **********Val Epoch 126: average Loss: 0.059868\n",
      "2025-04-23 14:28: Train Epoch 127: 0/37 Loss: 0.072681\n",
      "2025-04-23 14:28: Train Epoch 127: 0/37 Loss: 0.072681\n",
      "2025-04-23 14:28: Train Epoch 127: 0/37 Loss: 0.072681\n",
      "2025-04-23 14:28: Train Epoch 127: 20/37 Loss: 0.017303\n",
      "2025-04-23 14:28: Train Epoch 127: 20/37 Loss: 0.017303\n",
      "2025-04-23 14:28: Train Epoch 127: 20/37 Loss: 0.017303\n",
      "2025-04-23 14:28: **********Train Epoch 127: averaged Loss: 0.029727\n",
      "2025-04-23 14:28: **********Train Epoch 127: averaged Loss: 0.029727\n",
      "2025-04-23 14:28: **********Train Epoch 127: averaged Loss: 0.029727\n",
      "2025-04-23 14:28: **********Val Epoch 127: average Loss: 0.049107\n",
      "2025-04-23 14:28: **********Val Epoch 127: average Loss: 0.049107\n",
      "2025-04-23 14:28: **********Val Epoch 127: average Loss: 0.049107\n",
      "2025-04-23 14:28: Train Epoch 128: 0/37 Loss: 0.015386\n",
      "2025-04-23 14:28: Train Epoch 128: 0/37 Loss: 0.015386\n",
      "2025-04-23 14:28: Train Epoch 128: 0/37 Loss: 0.015386\n",
      "2025-04-23 14:28: Train Epoch 128: 20/37 Loss: 0.030004\n",
      "2025-04-23 14:28: Train Epoch 128: 20/37 Loss: 0.030004\n",
      "2025-04-23 14:28: Train Epoch 128: 20/37 Loss: 0.030004\n",
      "2025-04-23 14:28: **********Train Epoch 128: averaged Loss: 0.032633\n",
      "2025-04-23 14:28: **********Train Epoch 128: averaged Loss: 0.032633\n",
      "2025-04-23 14:28: **********Train Epoch 128: averaged Loss: 0.032633\n",
      "2025-04-23 14:28: **********Val Epoch 128: average Loss: 0.032821\n",
      "2025-04-23 14:28: **********Val Epoch 128: average Loss: 0.032821\n",
      "2025-04-23 14:28: **********Val Epoch 128: average Loss: 0.032821\n",
      "2025-04-23 14:28: Train Epoch 129: 0/37 Loss: 0.180811\n",
      "2025-04-23 14:28: Train Epoch 129: 0/37 Loss: 0.180811\n",
      "2025-04-23 14:28: Train Epoch 129: 0/37 Loss: 0.180811\n",
      "2025-04-23 14:28: Train Epoch 129: 20/37 Loss: 0.012361\n",
      "2025-04-23 14:28: Train Epoch 129: 20/37 Loss: 0.012361\n",
      "2025-04-23 14:28: Train Epoch 129: 20/37 Loss: 0.012361\n",
      "2025-04-23 14:28: **********Train Epoch 129: averaged Loss: 0.056727\n",
      "2025-04-23 14:28: **********Train Epoch 129: averaged Loss: 0.056727\n",
      "2025-04-23 14:28: **********Train Epoch 129: averaged Loss: 0.056727\n",
      "2025-04-23 14:28: **********Val Epoch 129: average Loss: 0.028528\n",
      "2025-04-23 14:28: **********Val Epoch 129: average Loss: 0.028528\n",
      "2025-04-23 14:28: **********Val Epoch 129: average Loss: 0.028528\n",
      "2025-04-23 14:28: Train Epoch 130: 0/37 Loss: 0.092744\n",
      "2025-04-23 14:28: Train Epoch 130: 0/37 Loss: 0.092744\n",
      "2025-04-23 14:28: Train Epoch 130: 0/37 Loss: 0.092744\n",
      "2025-04-23 14:28: Train Epoch 130: 20/37 Loss: 0.042480\n",
      "2025-04-23 14:28: Train Epoch 130: 20/37 Loss: 0.042480\n",
      "2025-04-23 14:28: Train Epoch 130: 20/37 Loss: 0.042480\n",
      "2025-04-23 14:28: **********Train Epoch 130: averaged Loss: 0.051491\n",
      "2025-04-23 14:28: **********Train Epoch 130: averaged Loss: 0.051491\n",
      "2025-04-23 14:28: **********Train Epoch 130: averaged Loss: 0.051491\n",
      "2025-04-23 14:28: **********Val Epoch 130: average Loss: 0.060160\n",
      "2025-04-23 14:28: **********Val Epoch 130: average Loss: 0.060160\n",
      "2025-04-23 14:28: **********Val Epoch 130: average Loss: 0.060160\n",
      "2025-04-23 14:28: Train Epoch 131: 0/37 Loss: 0.084771\n",
      "2025-04-23 14:28: Train Epoch 131: 0/37 Loss: 0.084771\n",
      "2025-04-23 14:28: Train Epoch 131: 0/37 Loss: 0.084771\n",
      "2025-04-23 14:28: Train Epoch 131: 20/37 Loss: 0.025527\n",
      "2025-04-23 14:28: Train Epoch 131: 20/37 Loss: 0.025527\n",
      "2025-04-23 14:28: Train Epoch 131: 20/37 Loss: 0.025527\n",
      "2025-04-23 14:28: **********Train Epoch 131: averaged Loss: 0.034902\n",
      "2025-04-23 14:28: **********Train Epoch 131: averaged Loss: 0.034902\n",
      "2025-04-23 14:28: **********Train Epoch 131: averaged Loss: 0.034902\n",
      "2025-04-23 14:28: **********Val Epoch 131: average Loss: 0.094129\n",
      "2025-04-23 14:28: **********Val Epoch 131: average Loss: 0.094129\n",
      "2025-04-23 14:28: **********Val Epoch 131: average Loss: 0.094129\n",
      "2025-04-23 14:28: Train Epoch 132: 0/37 Loss: 0.047279\n",
      "2025-04-23 14:28: Train Epoch 132: 0/37 Loss: 0.047279\n",
      "2025-04-23 14:28: Train Epoch 132: 0/37 Loss: 0.047279\n",
      "2025-04-23 14:28: Train Epoch 132: 20/37 Loss: 0.073663\n",
      "2025-04-23 14:28: Train Epoch 132: 20/37 Loss: 0.073663\n",
      "2025-04-23 14:28: Train Epoch 132: 20/37 Loss: 0.073663\n",
      "2025-04-23 14:28: **********Train Epoch 132: averaged Loss: 0.037905\n",
      "2025-04-23 14:28: **********Train Epoch 132: averaged Loss: 0.037905\n",
      "2025-04-23 14:28: **********Train Epoch 132: averaged Loss: 0.037905\n",
      "2025-04-23 14:28: **********Val Epoch 132: average Loss: 0.076468\n",
      "2025-04-23 14:28: **********Val Epoch 132: average Loss: 0.076468\n",
      "2025-04-23 14:28: **********Val Epoch 132: average Loss: 0.076468\n",
      "2025-04-23 14:28: Train Epoch 133: 0/37 Loss: 0.032577\n",
      "2025-04-23 14:28: Train Epoch 133: 0/37 Loss: 0.032577\n",
      "2025-04-23 14:28: Train Epoch 133: 0/37 Loss: 0.032577\n",
      "2025-04-23 14:28: Train Epoch 133: 20/37 Loss: 0.022499\n",
      "2025-04-23 14:28: Train Epoch 133: 20/37 Loss: 0.022499\n",
      "2025-04-23 14:28: Train Epoch 133: 20/37 Loss: 0.022499\n",
      "2025-04-23 14:28: **********Train Epoch 133: averaged Loss: 0.070503\n",
      "2025-04-23 14:28: **********Train Epoch 133: averaged Loss: 0.070503\n",
      "2025-04-23 14:28: **********Train Epoch 133: averaged Loss: 0.070503\n",
      "2025-04-23 14:28: **********Val Epoch 133: average Loss: 0.014618\n",
      "2025-04-23 14:28: **********Val Epoch 133: average Loss: 0.014618\n",
      "2025-04-23 14:28: **********Val Epoch 133: average Loss: 0.014618\n",
      "2025-04-23 14:28: Train Epoch 134: 0/37 Loss: 0.114441\n",
      "2025-04-23 14:28: Train Epoch 134: 0/37 Loss: 0.114441\n",
      "2025-04-23 14:28: Train Epoch 134: 0/37 Loss: 0.114441\n",
      "2025-04-23 14:28: Train Epoch 134: 20/37 Loss: 0.091701\n",
      "2025-04-23 14:28: Train Epoch 134: 20/37 Loss: 0.091701\n",
      "2025-04-23 14:28: Train Epoch 134: 20/37 Loss: 0.091701\n",
      "2025-04-23 14:28: **********Train Epoch 134: averaged Loss: 0.099167\n",
      "2025-04-23 14:28: **********Train Epoch 134: averaged Loss: 0.099167\n",
      "2025-04-23 14:28: **********Train Epoch 134: averaged Loss: 0.099167\n",
      "2025-04-23 14:28: **********Val Epoch 134: average Loss: 0.026955\n",
      "2025-04-23 14:28: **********Val Epoch 134: average Loss: 0.026955\n",
      "2025-04-23 14:28: **********Val Epoch 134: average Loss: 0.026955\n",
      "2025-04-23 14:28: Train Epoch 135: 0/37 Loss: 0.091977\n",
      "2025-04-23 14:28: Train Epoch 135: 0/37 Loss: 0.091977\n",
      "2025-04-23 14:28: Train Epoch 135: 0/37 Loss: 0.091977\n",
      "2025-04-23 14:28: Train Epoch 135: 20/37 Loss: 0.006151\n",
      "2025-04-23 14:28: Train Epoch 135: 20/37 Loss: 0.006151\n",
      "2025-04-23 14:28: Train Epoch 135: 20/37 Loss: 0.006151\n",
      "2025-04-23 14:28: **********Train Epoch 135: averaged Loss: 0.044218\n",
      "2025-04-23 14:28: **********Train Epoch 135: averaged Loss: 0.044218\n",
      "2025-04-23 14:28: **********Train Epoch 135: averaged Loss: 0.044218\n",
      "2025-04-23 14:28: **********Val Epoch 135: average Loss: 0.014758\n",
      "2025-04-23 14:28: **********Val Epoch 135: average Loss: 0.014758\n",
      "2025-04-23 14:28: **********Val Epoch 135: average Loss: 0.014758\n",
      "2025-04-23 14:28: Train Epoch 136: 0/37 Loss: 0.022960\n",
      "2025-04-23 14:28: Train Epoch 136: 0/37 Loss: 0.022960\n",
      "2025-04-23 14:28: Train Epoch 136: 0/37 Loss: 0.022960\n",
      "2025-04-23 14:28: Train Epoch 136: 20/37 Loss: 0.060603\n",
      "2025-04-23 14:28: Train Epoch 136: 20/37 Loss: 0.060603\n",
      "2025-04-23 14:28: Train Epoch 136: 20/37 Loss: 0.060603\n",
      "2025-04-23 14:28: **********Train Epoch 136: averaged Loss: 0.053880\n",
      "2025-04-23 14:28: **********Train Epoch 136: averaged Loss: 0.053880\n",
      "2025-04-23 14:28: **********Train Epoch 136: averaged Loss: 0.053880\n",
      "2025-04-23 14:28: **********Val Epoch 136: average Loss: 0.030355\n",
      "2025-04-23 14:28: **********Val Epoch 136: average Loss: 0.030355\n",
      "2025-04-23 14:28: **********Val Epoch 136: average Loss: 0.030355\n",
      "2025-04-23 14:28: Train Epoch 137: 0/37 Loss: 0.099928\n",
      "2025-04-23 14:28: Train Epoch 137: 0/37 Loss: 0.099928\n",
      "2025-04-23 14:28: Train Epoch 137: 0/37 Loss: 0.099928\n",
      "2025-04-23 14:28: Train Epoch 137: 20/37 Loss: 0.009260\n",
      "2025-04-23 14:28: Train Epoch 137: 20/37 Loss: 0.009260\n",
      "2025-04-23 14:28: Train Epoch 137: 20/37 Loss: 0.009260\n",
      "2025-04-23 14:28: **********Train Epoch 137: averaged Loss: 0.044356\n",
      "2025-04-23 14:28: **********Train Epoch 137: averaged Loss: 0.044356\n",
      "2025-04-23 14:28: **********Train Epoch 137: averaged Loss: 0.044356\n",
      "2025-04-23 14:28: **********Val Epoch 137: average Loss: 0.061103\n",
      "2025-04-23 14:28: **********Val Epoch 137: average Loss: 0.061103\n",
      "2025-04-23 14:28: **********Val Epoch 137: average Loss: 0.061103\n",
      "2025-04-23 14:28: Train Epoch 138: 0/37 Loss: 0.090098\n",
      "2025-04-23 14:28: Train Epoch 138: 0/37 Loss: 0.090098\n",
      "2025-04-23 14:28: Train Epoch 138: 0/37 Loss: 0.090098\n",
      "2025-04-23 14:28: Train Epoch 138: 20/37 Loss: 0.071072\n",
      "2025-04-23 14:28: Train Epoch 138: 20/37 Loss: 0.071072\n",
      "2025-04-23 14:28: Train Epoch 138: 20/37 Loss: 0.071072\n",
      "2025-04-23 14:28: **********Train Epoch 138: averaged Loss: 0.047360\n",
      "2025-04-23 14:28: **********Train Epoch 138: averaged Loss: 0.047360\n",
      "2025-04-23 14:28: **********Train Epoch 138: averaged Loss: 0.047360\n",
      "2025-04-23 14:28: **********Val Epoch 138: average Loss: 0.042634\n",
      "2025-04-23 14:28: **********Val Epoch 138: average Loss: 0.042634\n",
      "2025-04-23 14:28: **********Val Epoch 138: average Loss: 0.042634\n",
      "2025-04-23 14:28: Train Epoch 139: 0/37 Loss: 0.042773\n",
      "2025-04-23 14:28: Train Epoch 139: 0/37 Loss: 0.042773\n",
      "2025-04-23 14:28: Train Epoch 139: 0/37 Loss: 0.042773\n",
      "2025-04-23 14:28: Train Epoch 139: 20/37 Loss: 0.075361\n",
      "2025-04-23 14:28: Train Epoch 139: 20/37 Loss: 0.075361\n",
      "2025-04-23 14:28: Train Epoch 139: 20/37 Loss: 0.075361\n",
      "2025-04-23 14:28: **********Train Epoch 139: averaged Loss: 0.054405\n",
      "2025-04-23 14:28: **********Train Epoch 139: averaged Loss: 0.054405\n",
      "2025-04-23 14:28: **********Train Epoch 139: averaged Loss: 0.054405\n",
      "2025-04-23 14:28: **********Val Epoch 139: average Loss: 0.025982\n",
      "2025-04-23 14:28: **********Val Epoch 139: average Loss: 0.025982\n",
      "2025-04-23 14:28: **********Val Epoch 139: average Loss: 0.025982\n",
      "2025-04-23 14:28: Train Epoch 140: 0/37 Loss: 0.094155\n",
      "2025-04-23 14:28: Train Epoch 140: 0/37 Loss: 0.094155\n",
      "2025-04-23 14:28: Train Epoch 140: 0/37 Loss: 0.094155\n",
      "2025-04-23 14:28: Train Epoch 140: 20/37 Loss: 0.007403\n",
      "2025-04-23 14:28: Train Epoch 140: 20/37 Loss: 0.007403\n",
      "2025-04-23 14:28: Train Epoch 140: 20/37 Loss: 0.007403\n",
      "2025-04-23 14:28: **********Train Epoch 140: averaged Loss: 0.036067\n",
      "2025-04-23 14:28: **********Train Epoch 140: averaged Loss: 0.036067\n",
      "2025-04-23 14:28: **********Train Epoch 140: averaged Loss: 0.036067\n",
      "2025-04-23 14:28: **********Val Epoch 140: average Loss: 0.076789\n",
      "2025-04-23 14:28: **********Val Epoch 140: average Loss: 0.076789\n",
      "2025-04-23 14:28: **********Val Epoch 140: average Loss: 0.076789\n",
      "2025-04-23 14:28: Train Epoch 141: 0/37 Loss: 0.048218\n",
      "2025-04-23 14:28: Train Epoch 141: 0/37 Loss: 0.048218\n",
      "2025-04-23 14:28: Train Epoch 141: 0/37 Loss: 0.048218\n",
      "2025-04-23 14:29: Train Epoch 141: 20/37 Loss: 0.033595\n",
      "2025-04-23 14:29: Train Epoch 141: 20/37 Loss: 0.033595\n",
      "2025-04-23 14:29: Train Epoch 141: 20/37 Loss: 0.033595\n",
      "2025-04-23 14:29: **********Train Epoch 141: averaged Loss: 0.026406\n",
      "2025-04-23 14:29: **********Train Epoch 141: averaged Loss: 0.026406\n",
      "2025-04-23 14:29: **********Train Epoch 141: averaged Loss: 0.026406\n",
      "2025-04-23 14:29: **********Val Epoch 141: average Loss: 0.079729\n",
      "2025-04-23 14:29: **********Val Epoch 141: average Loss: 0.079729\n",
      "2025-04-23 14:29: **********Val Epoch 141: average Loss: 0.079729\n",
      "2025-04-23 14:29: Train Epoch 142: 0/37 Loss: 0.039798\n",
      "2025-04-23 14:29: Train Epoch 142: 0/37 Loss: 0.039798\n",
      "2025-04-23 14:29: Train Epoch 142: 0/37 Loss: 0.039798\n",
      "2025-04-23 14:29: Train Epoch 142: 20/37 Loss: 0.019356\n",
      "2025-04-23 14:29: Train Epoch 142: 20/37 Loss: 0.019356\n",
      "2025-04-23 14:29: Train Epoch 142: 20/37 Loss: 0.019356\n",
      "2025-04-23 14:29: **********Train Epoch 142: averaged Loss: 0.043455\n",
      "2025-04-23 14:29: **********Train Epoch 142: averaged Loss: 0.043455\n",
      "2025-04-23 14:29: **********Train Epoch 142: averaged Loss: 0.043455\n",
      "2025-04-23 14:29: **********Val Epoch 142: average Loss: 0.014153\n",
      "2025-04-23 14:29: **********Val Epoch 142: average Loss: 0.014153\n",
      "2025-04-23 14:29: **********Val Epoch 142: average Loss: 0.014153\n",
      "2025-04-23 14:29: Train Epoch 143: 0/37 Loss: 0.094771\n",
      "2025-04-23 14:29: Train Epoch 143: 0/37 Loss: 0.094771\n",
      "2025-04-23 14:29: Train Epoch 143: 0/37 Loss: 0.094771\n",
      "2025-04-23 14:29: Train Epoch 143: 20/37 Loss: 0.010931\n",
      "2025-04-23 14:29: Train Epoch 143: 20/37 Loss: 0.010931\n",
      "2025-04-23 14:29: Train Epoch 143: 20/37 Loss: 0.010931\n",
      "2025-04-23 14:29: **********Train Epoch 143: averaged Loss: 0.027929\n",
      "2025-04-23 14:29: **********Train Epoch 143: averaged Loss: 0.027929\n",
      "2025-04-23 14:29: **********Train Epoch 143: averaged Loss: 0.027929\n",
      "2025-04-23 14:29: **********Val Epoch 143: average Loss: 0.020395\n",
      "2025-04-23 14:29: **********Val Epoch 143: average Loss: 0.020395\n",
      "2025-04-23 14:29: **********Val Epoch 143: average Loss: 0.020395\n",
      "2025-04-23 14:29: Train Epoch 144: 0/37 Loss: 0.056276\n",
      "2025-04-23 14:29: Train Epoch 144: 0/37 Loss: 0.056276\n",
      "2025-04-23 14:29: Train Epoch 144: 0/37 Loss: 0.056276\n",
      "2025-04-23 14:29: Train Epoch 144: 20/37 Loss: 0.013741\n",
      "2025-04-23 14:29: Train Epoch 144: 20/37 Loss: 0.013741\n",
      "2025-04-23 14:29: Train Epoch 144: 20/37 Loss: 0.013741\n",
      "2025-04-23 14:29: **********Train Epoch 144: averaged Loss: 0.022204\n",
      "2025-04-23 14:29: **********Train Epoch 144: averaged Loss: 0.022204\n",
      "2025-04-23 14:29: **********Train Epoch 144: averaged Loss: 0.022204\n",
      "2025-04-23 14:29: **********Val Epoch 144: average Loss: 0.034510\n",
      "2025-04-23 14:29: **********Val Epoch 144: average Loss: 0.034510\n",
      "2025-04-23 14:29: **********Val Epoch 144: average Loss: 0.034510\n",
      "2025-04-23 14:29: Train Epoch 145: 0/37 Loss: 0.010721\n",
      "2025-04-23 14:29: Train Epoch 145: 0/37 Loss: 0.010721\n",
      "2025-04-23 14:29: Train Epoch 145: 0/37 Loss: 0.010721\n",
      "2025-04-23 14:29: Train Epoch 145: 20/37 Loss: 0.018050\n",
      "2025-04-23 14:29: Train Epoch 145: 20/37 Loss: 0.018050\n",
      "2025-04-23 14:29: Train Epoch 145: 20/37 Loss: 0.018050\n",
      "2025-04-23 14:29: **********Train Epoch 145: averaged Loss: 0.031672\n",
      "2025-04-23 14:29: **********Train Epoch 145: averaged Loss: 0.031672\n",
      "2025-04-23 14:29: **********Train Epoch 145: averaged Loss: 0.031672\n",
      "2025-04-23 14:29: **********Val Epoch 145: average Loss: 0.040997\n",
      "2025-04-23 14:29: **********Val Epoch 145: average Loss: 0.040997\n",
      "2025-04-23 14:29: **********Val Epoch 145: average Loss: 0.040997\n",
      "2025-04-23 14:29: Train Epoch 146: 0/37 Loss: 0.080895\n",
      "2025-04-23 14:29: Train Epoch 146: 0/37 Loss: 0.080895\n",
      "2025-04-23 14:29: Train Epoch 146: 0/37 Loss: 0.080895\n",
      "2025-04-23 14:29: Train Epoch 146: 20/37 Loss: 0.009368\n",
      "2025-04-23 14:29: Train Epoch 146: 20/37 Loss: 0.009368\n",
      "2025-04-23 14:29: Train Epoch 146: 20/37 Loss: 0.009368\n",
      "2025-04-23 14:29: **********Train Epoch 146: averaged Loss: 0.042989\n",
      "2025-04-23 14:29: **********Train Epoch 146: averaged Loss: 0.042989\n",
      "2025-04-23 14:29: **********Train Epoch 146: averaged Loss: 0.042989\n",
      "2025-04-23 14:29: **********Val Epoch 146: average Loss: 0.053398\n",
      "2025-04-23 14:29: **********Val Epoch 146: average Loss: 0.053398\n",
      "2025-04-23 14:29: **********Val Epoch 146: average Loss: 0.053398\n",
      "2025-04-23 14:29: Train Epoch 147: 0/37 Loss: 0.086481\n",
      "2025-04-23 14:29: Train Epoch 147: 0/37 Loss: 0.086481\n",
      "2025-04-23 14:29: Train Epoch 147: 0/37 Loss: 0.086481\n",
      "2025-04-23 14:29: Train Epoch 147: 20/37 Loss: 0.038216\n",
      "2025-04-23 14:29: Train Epoch 147: 20/37 Loss: 0.038216\n",
      "2025-04-23 14:29: Train Epoch 147: 20/37 Loss: 0.038216\n",
      "2025-04-23 14:29: **********Train Epoch 147: averaged Loss: 0.035283\n",
      "2025-04-23 14:29: **********Train Epoch 147: averaged Loss: 0.035283\n",
      "2025-04-23 14:29: **********Train Epoch 147: averaged Loss: 0.035283\n",
      "2025-04-23 14:29: **********Val Epoch 147: average Loss: 0.027880\n",
      "2025-04-23 14:29: **********Val Epoch 147: average Loss: 0.027880\n",
      "2025-04-23 14:29: **********Val Epoch 147: average Loss: 0.027880\n",
      "2025-04-23 14:29: Train Epoch 148: 0/37 Loss: 0.092218\n",
      "2025-04-23 14:29: Train Epoch 148: 0/37 Loss: 0.092218\n",
      "2025-04-23 14:29: Train Epoch 148: 0/37 Loss: 0.092218\n",
      "2025-04-23 14:29: Train Epoch 148: 20/37 Loss: 0.012204\n",
      "2025-04-23 14:29: Train Epoch 148: 20/37 Loss: 0.012204\n",
      "2025-04-23 14:29: Train Epoch 148: 20/37 Loss: 0.012204\n",
      "2025-04-23 14:29: **********Train Epoch 148: averaged Loss: 0.033036\n",
      "2025-04-23 14:29: **********Train Epoch 148: averaged Loss: 0.033036\n",
      "2025-04-23 14:29: **********Train Epoch 148: averaged Loss: 0.033036\n",
      "2025-04-23 14:29: **********Val Epoch 148: average Loss: 0.018104\n",
      "2025-04-23 14:29: **********Val Epoch 148: average Loss: 0.018104\n",
      "2025-04-23 14:29: **********Val Epoch 148: average Loss: 0.018104\n",
      "2025-04-23 14:29: Train Epoch 149: 0/37 Loss: 0.038023\n",
      "2025-04-23 14:29: Train Epoch 149: 0/37 Loss: 0.038023\n",
      "2025-04-23 14:29: Train Epoch 149: 0/37 Loss: 0.038023\n",
      "2025-04-23 14:29: Train Epoch 149: 20/37 Loss: 0.040630\n",
      "2025-04-23 14:29: Train Epoch 149: 20/37 Loss: 0.040630\n",
      "2025-04-23 14:29: Train Epoch 149: 20/37 Loss: 0.040630\n",
      "2025-04-23 14:29: **********Train Epoch 149: averaged Loss: 0.030486\n",
      "2025-04-23 14:29: **********Train Epoch 149: averaged Loss: 0.030486\n",
      "2025-04-23 14:29: **********Train Epoch 149: averaged Loss: 0.030486\n",
      "2025-04-23 14:29: **********Val Epoch 149: average Loss: 0.063526\n",
      "2025-04-23 14:29: **********Val Epoch 149: average Loss: 0.063526\n",
      "2025-04-23 14:29: **********Val Epoch 149: average Loss: 0.063526\n",
      "2025-04-23 14:29: Train Epoch 150: 0/37 Loss: 0.069001\n",
      "2025-04-23 14:29: Train Epoch 150: 0/37 Loss: 0.069001\n",
      "2025-04-23 14:29: Train Epoch 150: 0/37 Loss: 0.069001\n",
      "2025-04-23 14:29: Train Epoch 150: 20/37 Loss: 0.013573\n",
      "2025-04-23 14:29: Train Epoch 150: 20/37 Loss: 0.013573\n",
      "2025-04-23 14:29: Train Epoch 150: 20/37 Loss: 0.013573\n",
      "2025-04-23 14:29: **********Train Epoch 150: averaged Loss: 0.053608\n",
      "2025-04-23 14:29: **********Train Epoch 150: averaged Loss: 0.053608\n",
      "2025-04-23 14:29: **********Train Epoch 150: averaged Loss: 0.053608\n",
      "2025-04-23 14:29: **********Val Epoch 150: average Loss: 0.066087\n",
      "2025-04-23 14:29: **********Val Epoch 150: average Loss: 0.066087\n",
      "2025-04-23 14:29: **********Val Epoch 150: average Loss: 0.066087\n",
      "2025-04-23 14:29: Train Epoch 151: 0/37 Loss: 0.084531\n",
      "2025-04-23 14:29: Train Epoch 151: 0/37 Loss: 0.084531\n",
      "2025-04-23 14:29: Train Epoch 151: 0/37 Loss: 0.084531\n",
      "2025-04-23 14:29: Train Epoch 151: 20/37 Loss: 0.092502\n",
      "2025-04-23 14:29: Train Epoch 151: 20/37 Loss: 0.092502\n",
      "2025-04-23 14:29: Train Epoch 151: 20/37 Loss: 0.092502\n",
      "2025-04-23 14:29: **********Train Epoch 151: averaged Loss: 0.060856\n",
      "2025-04-23 14:29: **********Train Epoch 151: averaged Loss: 0.060856\n",
      "2025-04-23 14:29: **********Train Epoch 151: averaged Loss: 0.060856\n",
      "2025-04-23 14:29: **********Val Epoch 151: average Loss: 0.072325\n",
      "2025-04-23 14:29: **********Val Epoch 151: average Loss: 0.072325\n",
      "2025-04-23 14:29: **********Val Epoch 151: average Loss: 0.072325\n",
      "2025-04-23 14:29: Train Epoch 152: 0/37 Loss: 0.080708\n",
      "2025-04-23 14:29: Train Epoch 152: 0/37 Loss: 0.080708\n",
      "2025-04-23 14:29: Train Epoch 152: 0/37 Loss: 0.080708\n",
      "2025-04-23 14:29: Train Epoch 152: 20/37 Loss: 0.013407\n",
      "2025-04-23 14:29: Train Epoch 152: 20/37 Loss: 0.013407\n",
      "2025-04-23 14:29: Train Epoch 152: 20/37 Loss: 0.013407\n",
      "2025-04-23 14:29: **********Train Epoch 152: averaged Loss: 0.031624\n",
      "2025-04-23 14:29: **********Train Epoch 152: averaged Loss: 0.031624\n",
      "2025-04-23 14:29: **********Train Epoch 152: averaged Loss: 0.031624\n",
      "2025-04-23 14:29: **********Val Epoch 152: average Loss: 0.018536\n",
      "2025-04-23 14:29: **********Val Epoch 152: average Loss: 0.018536\n",
      "2025-04-23 14:29: **********Val Epoch 152: average Loss: 0.018536\n",
      "2025-04-23 14:29: Train Epoch 153: 0/37 Loss: 0.022892\n",
      "2025-04-23 14:29: Train Epoch 153: 0/37 Loss: 0.022892\n",
      "2025-04-23 14:29: Train Epoch 153: 0/37 Loss: 0.022892\n",
      "2025-04-23 14:29: Train Epoch 153: 20/37 Loss: 0.014643\n",
      "2025-04-23 14:29: Train Epoch 153: 20/37 Loss: 0.014643\n",
      "2025-04-23 14:29: Train Epoch 153: 20/37 Loss: 0.014643\n",
      "2025-04-23 14:29: **********Train Epoch 153: averaged Loss: 0.035476\n",
      "2025-04-23 14:29: **********Train Epoch 153: averaged Loss: 0.035476\n",
      "2025-04-23 14:29: **********Train Epoch 153: averaged Loss: 0.035476\n",
      "2025-04-23 14:29: **********Val Epoch 153: average Loss: 0.025387\n",
      "2025-04-23 14:29: **********Val Epoch 153: average Loss: 0.025387\n",
      "2025-04-23 14:29: **********Val Epoch 153: average Loss: 0.025387\n",
      "2025-04-23 14:29: Train Epoch 154: 0/37 Loss: 0.075916\n",
      "2025-04-23 14:29: Train Epoch 154: 0/37 Loss: 0.075916\n",
      "2025-04-23 14:29: Train Epoch 154: 0/37 Loss: 0.075916\n",
      "2025-04-23 14:29: Train Epoch 154: 20/37 Loss: 0.030196\n",
      "2025-04-23 14:29: Train Epoch 154: 20/37 Loss: 0.030196\n",
      "2025-04-23 14:29: Train Epoch 154: 20/37 Loss: 0.030196\n",
      "2025-04-23 14:29: **********Train Epoch 154: averaged Loss: 0.031397\n",
      "2025-04-23 14:29: **********Train Epoch 154: averaged Loss: 0.031397\n",
      "2025-04-23 14:29: **********Train Epoch 154: averaged Loss: 0.031397\n",
      "2025-04-23 14:29: **********Val Epoch 154: average Loss: 0.020992\n",
      "2025-04-23 14:29: **********Val Epoch 154: average Loss: 0.020992\n",
      "2025-04-23 14:29: **********Val Epoch 154: average Loss: 0.020992\n",
      "2025-04-23 14:29: Train Epoch 155: 0/37 Loss: 0.080117\n",
      "2025-04-23 14:29: Train Epoch 155: 0/37 Loss: 0.080117\n",
      "2025-04-23 14:29: Train Epoch 155: 0/37 Loss: 0.080117\n",
      "2025-04-23 14:29: Train Epoch 155: 20/37 Loss: 0.006888\n",
      "2025-04-23 14:29: Train Epoch 155: 20/37 Loss: 0.006888\n",
      "2025-04-23 14:29: Train Epoch 155: 20/37 Loss: 0.006888\n",
      "2025-04-23 14:29: **********Train Epoch 155: averaged Loss: 0.028607\n",
      "2025-04-23 14:29: **********Train Epoch 155: averaged Loss: 0.028607\n",
      "2025-04-23 14:29: **********Train Epoch 155: averaged Loss: 0.028607\n",
      "2025-04-23 14:29: **********Val Epoch 155: average Loss: 0.054025\n",
      "2025-04-23 14:29: **********Val Epoch 155: average Loss: 0.054025\n",
      "2025-04-23 14:29: **********Val Epoch 155: average Loss: 0.054025\n",
      "2025-04-23 14:29: Train Epoch 156: 0/37 Loss: 0.013207\n",
      "2025-04-23 14:29: Train Epoch 156: 0/37 Loss: 0.013207\n",
      "2025-04-23 14:29: Train Epoch 156: 0/37 Loss: 0.013207\n",
      "2025-04-23 14:29: Train Epoch 156: 20/37 Loss: 0.036059\n",
      "2025-04-23 14:29: Train Epoch 156: 20/37 Loss: 0.036059\n",
      "2025-04-23 14:29: Train Epoch 156: 20/37 Loss: 0.036059\n",
      "2025-04-23 14:29: **********Train Epoch 156: averaged Loss: 0.032790\n",
      "2025-04-23 14:29: **********Train Epoch 156: averaged Loss: 0.032790\n",
      "2025-04-23 14:29: **********Train Epoch 156: averaged Loss: 0.032790\n",
      "2025-04-23 14:29: **********Val Epoch 156: average Loss: 0.037716\n",
      "2025-04-23 14:29: **********Val Epoch 156: average Loss: 0.037716\n",
      "2025-04-23 14:29: **********Val Epoch 156: average Loss: 0.037716\n",
      "2025-04-23 14:29: Train Epoch 157: 0/37 Loss: 0.027267\n",
      "2025-04-23 14:29: Train Epoch 157: 0/37 Loss: 0.027267\n",
      "2025-04-23 14:29: Train Epoch 157: 0/37 Loss: 0.027267\n",
      "2025-04-23 14:29: Train Epoch 157: 20/37 Loss: 0.031540\n",
      "2025-04-23 14:29: Train Epoch 157: 20/37 Loss: 0.031540\n",
      "2025-04-23 14:29: Train Epoch 157: 20/37 Loss: 0.031540\n",
      "2025-04-23 14:29: **********Train Epoch 157: averaged Loss: 0.031887\n",
      "2025-04-23 14:29: **********Train Epoch 157: averaged Loss: 0.031887\n",
      "2025-04-23 14:29: **********Train Epoch 157: averaged Loss: 0.031887\n",
      "2025-04-23 14:29: **********Val Epoch 157: average Loss: 0.052221\n",
      "2025-04-23 14:29: **********Val Epoch 157: average Loss: 0.052221\n",
      "2025-04-23 14:29: **********Val Epoch 157: average Loss: 0.052221\n",
      "2025-04-23 14:29: Train Epoch 158: 0/37 Loss: 0.099233\n",
      "2025-04-23 14:29: Train Epoch 158: 0/37 Loss: 0.099233\n",
      "2025-04-23 14:29: Train Epoch 158: 0/37 Loss: 0.099233\n",
      "2025-04-23 14:29: Train Epoch 158: 20/37 Loss: 0.014468\n",
      "2025-04-23 14:29: Train Epoch 158: 20/37 Loss: 0.014468\n",
      "2025-04-23 14:29: Train Epoch 158: 20/37 Loss: 0.014468\n",
      "2025-04-23 14:29: **********Train Epoch 158: averaged Loss: 0.035163\n",
      "2025-04-23 14:29: **********Train Epoch 158: averaged Loss: 0.035163\n",
      "2025-04-23 14:29: **********Train Epoch 158: averaged Loss: 0.035163\n",
      "2025-04-23 14:29: **********Val Epoch 158: average Loss: 0.068055\n",
      "2025-04-23 14:29: **********Val Epoch 158: average Loss: 0.068055\n",
      "2025-04-23 14:29: **********Val Epoch 158: average Loss: 0.068055\n",
      "2025-04-23 14:29: Train Epoch 159: 0/37 Loss: 0.023509\n",
      "2025-04-23 14:29: Train Epoch 159: 0/37 Loss: 0.023509\n",
      "2025-04-23 14:29: Train Epoch 159: 0/37 Loss: 0.023509\n",
      "2025-04-23 14:29: Train Epoch 159: 20/37 Loss: 0.008429\n",
      "2025-04-23 14:29: Train Epoch 159: 20/37 Loss: 0.008429\n",
      "2025-04-23 14:29: Train Epoch 159: 20/37 Loss: 0.008429\n",
      "2025-04-23 14:29: **********Train Epoch 159: averaged Loss: 0.032122\n",
      "2025-04-23 14:29: **********Train Epoch 159: averaged Loss: 0.032122\n",
      "2025-04-23 14:29: **********Train Epoch 159: averaged Loss: 0.032122\n",
      "2025-04-23 14:29: **********Val Epoch 159: average Loss: 0.016734\n",
      "2025-04-23 14:29: **********Val Epoch 159: average Loss: 0.016734\n",
      "2025-04-23 14:29: **********Val Epoch 159: average Loss: 0.016734\n",
      "2025-04-23 14:29: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:29: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:29: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:29: Total training time: 4.4463min, best loss: 0.009039\n",
      "2025-04-23 14:29: Total training time: 4.4463min, best loss: 0.009039\n",
      "2025-04-23 14:29: Total training time: 4.4463min, best loss: 0.009039\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0401, MSE: 0.0425\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0401, MSE: 0.0425\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0401, MSE: 0.0425\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0421, MSE: 0.0436\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0421, MSE: 0.0436\n",
      "2025-04-23 14:29: Average Horizon, MAE: 0.0421, MSE: 0.0436\n",
      "2025-04-23 14:29: Experiment log path in: ./\n",
      "2025-04-23 14:29: Experiment log path in: ./\n",
      "2025-04-23 14:29: Experiment log path in: ./\n",
      "2025-04-23 14:29: Experiment log path in: ./\n",
      "2025-04-23 14:29: Train Epoch 1: 0/37 Loss: 0.980595\n",
      "2025-04-23 14:29: Train Epoch 1: 0/37 Loss: 0.980595\n",
      "2025-04-23 14:29: Train Epoch 1: 0/37 Loss: 0.980595\n",
      "2025-04-23 14:29: Train Epoch 1: 0/37 Loss: 0.980595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "graphsage.fc.weight torch.Size([1, 10]) True\n",
      "graphsage.fc.bias torch.Size([1]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240663\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:29: Train Epoch 1: 20/37 Loss: 0.017101\n",
      "2025-04-23 14:29: Train Epoch 1: 20/37 Loss: 0.017101\n",
      "2025-04-23 14:29: Train Epoch 1: 20/37 Loss: 0.017101\n",
      "2025-04-23 14:29: Train Epoch 1: 20/37 Loss: 0.017101\n",
      "2025-04-23 14:29: **********Train Epoch 1: averaged Loss: 0.149301\n",
      "2025-04-23 14:29: **********Train Epoch 1: averaged Loss: 0.149301\n",
      "2025-04-23 14:29: **********Train Epoch 1: averaged Loss: 0.149301\n",
      "2025-04-23 14:29: **********Train Epoch 1: averaged Loss: 0.149301\n",
      "2025-04-23 14:29: **********Val Epoch 1: average Loss: 0.205110\n",
      "2025-04-23 14:29: **********Val Epoch 1: average Loss: 0.205110\n",
      "2025-04-23 14:29: **********Val Epoch 1: average Loss: 0.205110\n",
      "2025-04-23 14:29: **********Val Epoch 1: average Loss: 0.205110\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: Train Epoch 2: 0/37 Loss: 0.802558\n",
      "2025-04-23 14:29: Train Epoch 2: 0/37 Loss: 0.802558\n",
      "2025-04-23 14:29: Train Epoch 2: 0/37 Loss: 0.802558\n",
      "2025-04-23 14:29: Train Epoch 2: 0/37 Loss: 0.802558\n",
      "2025-04-23 14:29: Train Epoch 2: 20/37 Loss: 0.016886\n",
      "2025-04-23 14:29: Train Epoch 2: 20/37 Loss: 0.016886\n",
      "2025-04-23 14:29: Train Epoch 2: 20/37 Loss: 0.016886\n",
      "2025-04-23 14:29: Train Epoch 2: 20/37 Loss: 0.016886\n",
      "2025-04-23 14:29: **********Train Epoch 2: averaged Loss: 0.199372\n",
      "2025-04-23 14:29: **********Train Epoch 2: averaged Loss: 0.199372\n",
      "2025-04-23 14:29: **********Train Epoch 2: averaged Loss: 0.199372\n",
      "2025-04-23 14:29: **********Train Epoch 2: averaged Loss: 0.199372\n",
      "2025-04-23 14:29: **********Val Epoch 2: average Loss: 0.267126\n",
      "2025-04-23 14:29: **********Val Epoch 2: average Loss: 0.267126\n",
      "2025-04-23 14:29: **********Val Epoch 2: average Loss: 0.267126\n",
      "2025-04-23 14:29: **********Val Epoch 2: average Loss: 0.267126\n",
      "2025-04-23 14:29: Train Epoch 3: 0/37 Loss: 0.876601\n",
      "2025-04-23 14:29: Train Epoch 3: 0/37 Loss: 0.876601\n",
      "2025-04-23 14:29: Train Epoch 3: 0/37 Loss: 0.876601\n",
      "2025-04-23 14:29: Train Epoch 3: 0/37 Loss: 0.876601\n",
      "2025-04-23 14:29: Train Epoch 3: 20/37 Loss: 0.257036\n",
      "2025-04-23 14:29: Train Epoch 3: 20/37 Loss: 0.257036\n",
      "2025-04-23 14:29: Train Epoch 3: 20/37 Loss: 0.257036\n",
      "2025-04-23 14:29: Train Epoch 3: 20/37 Loss: 0.257036\n",
      "2025-04-23 14:29: **********Train Epoch 3: averaged Loss: 0.280785\n",
      "2025-04-23 14:29: **********Train Epoch 3: averaged Loss: 0.280785\n",
      "2025-04-23 14:29: **********Train Epoch 3: averaged Loss: 0.280785\n",
      "2025-04-23 14:29: **********Train Epoch 3: averaged Loss: 0.280785\n",
      "2025-04-23 14:29: **********Val Epoch 3: average Loss: 0.112747\n",
      "2025-04-23 14:29: **********Val Epoch 3: average Loss: 0.112747\n",
      "2025-04-23 14:29: **********Val Epoch 3: average Loss: 0.112747\n",
      "2025-04-23 14:29: **********Val Epoch 3: average Loss: 0.112747\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: Train Epoch 4: 0/37 Loss: 0.734813\n",
      "2025-04-23 14:29: Train Epoch 4: 0/37 Loss: 0.734813\n",
      "2025-04-23 14:29: Train Epoch 4: 0/37 Loss: 0.734813\n",
      "2025-04-23 14:29: Train Epoch 4: 0/37 Loss: 0.734813\n",
      "2025-04-23 14:29: Train Epoch 4: 20/37 Loss: 0.067101\n",
      "2025-04-23 14:29: Train Epoch 4: 20/37 Loss: 0.067101\n",
      "2025-04-23 14:29: Train Epoch 4: 20/37 Loss: 0.067101\n",
      "2025-04-23 14:29: Train Epoch 4: 20/37 Loss: 0.067101\n",
      "2025-04-23 14:29: **********Train Epoch 4: averaged Loss: 0.363448\n",
      "2025-04-23 14:29: **********Train Epoch 4: averaged Loss: 0.363448\n",
      "2025-04-23 14:29: **********Train Epoch 4: averaged Loss: 0.363448\n",
      "2025-04-23 14:29: **********Train Epoch 4: averaged Loss: 0.363448\n",
      "2025-04-23 14:29: **********Val Epoch 4: average Loss: 0.226502\n",
      "2025-04-23 14:29: **********Val Epoch 4: average Loss: 0.226502\n",
      "2025-04-23 14:29: **********Val Epoch 4: average Loss: 0.226502\n",
      "2025-04-23 14:29: **********Val Epoch 4: average Loss: 0.226502\n",
      "2025-04-23 14:29: Train Epoch 5: 0/37 Loss: 0.385023\n",
      "2025-04-23 14:29: Train Epoch 5: 0/37 Loss: 0.385023\n",
      "2025-04-23 14:29: Train Epoch 5: 0/37 Loss: 0.385023\n",
      "2025-04-23 14:29: Train Epoch 5: 0/37 Loss: 0.385023\n",
      "2025-04-23 14:29: Train Epoch 5: 20/37 Loss: 0.094557\n",
      "2025-04-23 14:29: Train Epoch 5: 20/37 Loss: 0.094557\n",
      "2025-04-23 14:29: Train Epoch 5: 20/37 Loss: 0.094557\n",
      "2025-04-23 14:29: Train Epoch 5: 20/37 Loss: 0.094557\n",
      "2025-04-23 14:29: **********Train Epoch 5: averaged Loss: 0.256149\n",
      "2025-04-23 14:29: **********Train Epoch 5: averaged Loss: 0.256149\n",
      "2025-04-23 14:29: **********Train Epoch 5: averaged Loss: 0.256149\n",
      "2025-04-23 14:29: **********Train Epoch 5: averaged Loss: 0.256149\n",
      "2025-04-23 14:29: **********Val Epoch 5: average Loss: 0.225192\n",
      "2025-04-23 14:29: **********Val Epoch 5: average Loss: 0.225192\n",
      "2025-04-23 14:29: **********Val Epoch 5: average Loss: 0.225192\n",
      "2025-04-23 14:29: **********Val Epoch 5: average Loss: 0.225192\n",
      "2025-04-23 14:29: Train Epoch 6: 0/37 Loss: 0.364145\n",
      "2025-04-23 14:29: Train Epoch 6: 0/37 Loss: 0.364145\n",
      "2025-04-23 14:29: Train Epoch 6: 0/37 Loss: 0.364145\n",
      "2025-04-23 14:29: Train Epoch 6: 0/37 Loss: 0.364145\n",
      "2025-04-23 14:29: Train Epoch 6: 20/37 Loss: 0.099884\n",
      "2025-04-23 14:29: Train Epoch 6: 20/37 Loss: 0.099884\n",
      "2025-04-23 14:29: Train Epoch 6: 20/37 Loss: 0.099884\n",
      "2025-04-23 14:29: Train Epoch 6: 20/37 Loss: 0.099884\n",
      "2025-04-23 14:29: **********Train Epoch 6: averaged Loss: 0.252231\n",
      "2025-04-23 14:29: **********Train Epoch 6: averaged Loss: 0.252231\n",
      "2025-04-23 14:29: **********Train Epoch 6: averaged Loss: 0.252231\n",
      "2025-04-23 14:29: **********Train Epoch 6: averaged Loss: 0.252231\n",
      "2025-04-23 14:29: **********Val Epoch 6: average Loss: 0.278857\n",
      "2025-04-23 14:29: **********Val Epoch 6: average Loss: 0.278857\n",
      "2025-04-23 14:29: **********Val Epoch 6: average Loss: 0.278857\n",
      "2025-04-23 14:29: **********Val Epoch 6: average Loss: 0.278857\n",
      "2025-04-23 14:29: Train Epoch 7: 0/37 Loss: 0.323133\n",
      "2025-04-23 14:29: Train Epoch 7: 0/37 Loss: 0.323133\n",
      "2025-04-23 14:29: Train Epoch 7: 0/37 Loss: 0.323133\n",
      "2025-04-23 14:29: Train Epoch 7: 0/37 Loss: 0.323133\n",
      "2025-04-23 14:29: Train Epoch 7: 20/37 Loss: 0.134979\n",
      "2025-04-23 14:29: Train Epoch 7: 20/37 Loss: 0.134979\n",
      "2025-04-23 14:29: Train Epoch 7: 20/37 Loss: 0.134979\n",
      "2025-04-23 14:29: Train Epoch 7: 20/37 Loss: 0.134979\n",
      "2025-04-23 14:29: **********Train Epoch 7: averaged Loss: 0.169071\n",
      "2025-04-23 14:29: **********Train Epoch 7: averaged Loss: 0.169071\n",
      "2025-04-23 14:29: **********Train Epoch 7: averaged Loss: 0.169071\n",
      "2025-04-23 14:29: **********Train Epoch 7: averaged Loss: 0.169071\n",
      "2025-04-23 14:29: **********Val Epoch 7: average Loss: 0.100821\n",
      "2025-04-23 14:29: **********Val Epoch 7: average Loss: 0.100821\n",
      "2025-04-23 14:29: **********Val Epoch 7: average Loss: 0.100821\n",
      "2025-04-23 14:29: **********Val Epoch 7: average Loss: 0.100821\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: Train Epoch 8: 0/37 Loss: 0.684880\n",
      "2025-04-23 14:29: Train Epoch 8: 0/37 Loss: 0.684880\n",
      "2025-04-23 14:29: Train Epoch 8: 0/37 Loss: 0.684880\n",
      "2025-04-23 14:29: Train Epoch 8: 0/37 Loss: 0.684880\n",
      "2025-04-23 14:29: Train Epoch 8: 20/37 Loss: 0.223455\n",
      "2025-04-23 14:29: Train Epoch 8: 20/37 Loss: 0.223455\n",
      "2025-04-23 14:29: Train Epoch 8: 20/37 Loss: 0.223455\n",
      "2025-04-23 14:29: Train Epoch 8: 20/37 Loss: 0.223455\n",
      "2025-04-23 14:29: **********Train Epoch 8: averaged Loss: 0.253279\n",
      "2025-04-23 14:29: **********Train Epoch 8: averaged Loss: 0.253279\n",
      "2025-04-23 14:29: **********Train Epoch 8: averaged Loss: 0.253279\n",
      "2025-04-23 14:29: **********Train Epoch 8: averaged Loss: 0.253279\n",
      "2025-04-23 14:29: **********Val Epoch 8: average Loss: 0.304662\n",
      "2025-04-23 14:29: **********Val Epoch 8: average Loss: 0.304662\n",
      "2025-04-23 14:29: **********Val Epoch 8: average Loss: 0.304662\n",
      "2025-04-23 14:29: **********Val Epoch 8: average Loss: 0.304662\n",
      "2025-04-23 14:29: Train Epoch 9: 0/37 Loss: 0.295201\n",
      "2025-04-23 14:29: Train Epoch 9: 0/37 Loss: 0.295201\n",
      "2025-04-23 14:29: Train Epoch 9: 0/37 Loss: 0.295201\n",
      "2025-04-23 14:29: Train Epoch 9: 0/37 Loss: 0.295201\n",
      "2025-04-23 14:29: Train Epoch 9: 20/37 Loss: 0.021474\n",
      "2025-04-23 14:29: Train Epoch 9: 20/37 Loss: 0.021474\n",
      "2025-04-23 14:29: Train Epoch 9: 20/37 Loss: 0.021474\n",
      "2025-04-23 14:29: Train Epoch 9: 20/37 Loss: 0.021474\n",
      "2025-04-23 14:29: **********Train Epoch 9: averaged Loss: 0.204974\n",
      "2025-04-23 14:29: **********Train Epoch 9: averaged Loss: 0.204974\n",
      "2025-04-23 14:29: **********Train Epoch 9: averaged Loss: 0.204974\n",
      "2025-04-23 14:29: **********Train Epoch 9: averaged Loss: 0.204974\n",
      "2025-04-23 14:29: **********Val Epoch 9: average Loss: 0.346963\n",
      "2025-04-23 14:29: **********Val Epoch 9: average Loss: 0.346963\n",
      "2025-04-23 14:29: **********Val Epoch 9: average Loss: 0.346963\n",
      "2025-04-23 14:29: **********Val Epoch 9: average Loss: 0.346963\n",
      "2025-04-23 14:29: Train Epoch 10: 0/37 Loss: 0.263485\n",
      "2025-04-23 14:29: Train Epoch 10: 0/37 Loss: 0.263485\n",
      "2025-04-23 14:29: Train Epoch 10: 0/37 Loss: 0.263485\n",
      "2025-04-23 14:29: Train Epoch 10: 0/37 Loss: 0.263485\n",
      "2025-04-23 14:29: Train Epoch 10: 20/37 Loss: 0.044433\n",
      "2025-04-23 14:29: Train Epoch 10: 20/37 Loss: 0.044433\n",
      "2025-04-23 14:29: Train Epoch 10: 20/37 Loss: 0.044433\n",
      "2025-04-23 14:29: Train Epoch 10: 20/37 Loss: 0.044433\n",
      "2025-04-23 14:29: **********Train Epoch 10: averaged Loss: 0.195863\n",
      "2025-04-23 14:29: **********Train Epoch 10: averaged Loss: 0.195863\n",
      "2025-04-23 14:29: **********Train Epoch 10: averaged Loss: 0.195863\n",
      "2025-04-23 14:29: **********Train Epoch 10: averaged Loss: 0.195863\n",
      "2025-04-23 14:29: **********Val Epoch 10: average Loss: 0.353759\n",
      "2025-04-23 14:29: **********Val Epoch 10: average Loss: 0.353759\n",
      "2025-04-23 14:29: **********Val Epoch 10: average Loss: 0.353759\n",
      "2025-04-23 14:29: **********Val Epoch 10: average Loss: 0.353759\n",
      "2025-04-23 14:29: Train Epoch 11: 0/37 Loss: 0.239800\n",
      "2025-04-23 14:29: Train Epoch 11: 0/37 Loss: 0.239800\n",
      "2025-04-23 14:29: Train Epoch 11: 0/37 Loss: 0.239800\n",
      "2025-04-23 14:29: Train Epoch 11: 0/37 Loss: 0.239800\n",
      "2025-04-23 14:29: Train Epoch 11: 20/37 Loss: 0.062125\n",
      "2025-04-23 14:29: Train Epoch 11: 20/37 Loss: 0.062125\n",
      "2025-04-23 14:29: Train Epoch 11: 20/37 Loss: 0.062125\n",
      "2025-04-23 14:29: Train Epoch 11: 20/37 Loss: 0.062125\n",
      "2025-04-23 14:29: **********Train Epoch 11: averaged Loss: 0.193343\n",
      "2025-04-23 14:29: **********Train Epoch 11: averaged Loss: 0.193343\n",
      "2025-04-23 14:29: **********Train Epoch 11: averaged Loss: 0.193343\n",
      "2025-04-23 14:29: **********Train Epoch 11: averaged Loss: 0.193343\n",
      "2025-04-23 14:29: **********Val Epoch 11: average Loss: 0.396464\n",
      "2025-04-23 14:29: **********Val Epoch 11: average Loss: 0.396464\n",
      "2025-04-23 14:29: **********Val Epoch 11: average Loss: 0.396464\n",
      "2025-04-23 14:29: **********Val Epoch 11: average Loss: 0.396464\n",
      "2025-04-23 14:29: Train Epoch 12: 0/37 Loss: 0.209150\n",
      "2025-04-23 14:29: Train Epoch 12: 0/37 Loss: 0.209150\n",
      "2025-04-23 14:29: Train Epoch 12: 0/37 Loss: 0.209150\n",
      "2025-04-23 14:29: Train Epoch 12: 0/37 Loss: 0.209150\n",
      "2025-04-23 14:29: Train Epoch 12: 20/37 Loss: 0.052186\n",
      "2025-04-23 14:29: Train Epoch 12: 20/37 Loss: 0.052186\n",
      "2025-04-23 14:29: Train Epoch 12: 20/37 Loss: 0.052186\n",
      "2025-04-23 14:29: Train Epoch 12: 20/37 Loss: 0.052186\n",
      "2025-04-23 14:29: **********Train Epoch 12: averaged Loss: 0.154990\n",
      "2025-04-23 14:29: **********Train Epoch 12: averaged Loss: 0.154990\n",
      "2025-04-23 14:29: **********Train Epoch 12: averaged Loss: 0.154990\n",
      "2025-04-23 14:29: **********Train Epoch 12: averaged Loss: 0.154990\n",
      "2025-04-23 14:29: **********Val Epoch 12: average Loss: 0.281978\n",
      "2025-04-23 14:29: **********Val Epoch 12: average Loss: 0.281978\n",
      "2025-04-23 14:29: **********Val Epoch 12: average Loss: 0.281978\n",
      "2025-04-23 14:29: **********Val Epoch 12: average Loss: 0.281978\n",
      "2025-04-23 14:29: Train Epoch 13: 0/37 Loss: 0.244256\n",
      "2025-04-23 14:29: Train Epoch 13: 0/37 Loss: 0.244256\n",
      "2025-04-23 14:29: Train Epoch 13: 0/37 Loss: 0.244256\n",
      "2025-04-23 14:29: Train Epoch 13: 0/37 Loss: 0.244256\n",
      "2025-04-23 14:29: Train Epoch 13: 20/37 Loss: 0.060648\n",
      "2025-04-23 14:29: Train Epoch 13: 20/37 Loss: 0.060648\n",
      "2025-04-23 14:29: Train Epoch 13: 20/37 Loss: 0.060648\n",
      "2025-04-23 14:29: Train Epoch 13: 20/37 Loss: 0.060648\n",
      "2025-04-23 14:29: **********Train Epoch 13: averaged Loss: 0.181181\n",
      "2025-04-23 14:29: **********Train Epoch 13: averaged Loss: 0.181181\n",
      "2025-04-23 14:29: **********Train Epoch 13: averaged Loss: 0.181181\n",
      "2025-04-23 14:29: **********Train Epoch 13: averaged Loss: 0.181181\n",
      "2025-04-23 14:29: **********Val Epoch 13: average Loss: 0.253236\n",
      "2025-04-23 14:29: **********Val Epoch 13: average Loss: 0.253236\n",
      "2025-04-23 14:29: **********Val Epoch 13: average Loss: 0.253236\n",
      "2025-04-23 14:29: **********Val Epoch 13: average Loss: 0.253236\n",
      "2025-04-23 14:29: Train Epoch 14: 0/37 Loss: 0.202181\n",
      "2025-04-23 14:29: Train Epoch 14: 0/37 Loss: 0.202181\n",
      "2025-04-23 14:29: Train Epoch 14: 0/37 Loss: 0.202181\n",
      "2025-04-23 14:29: Train Epoch 14: 0/37 Loss: 0.202181\n",
      "2025-04-23 14:29: Train Epoch 14: 20/37 Loss: 0.015111\n",
      "2025-04-23 14:29: Train Epoch 14: 20/37 Loss: 0.015111\n",
      "2025-04-23 14:29: Train Epoch 14: 20/37 Loss: 0.015111\n",
      "2025-04-23 14:29: Train Epoch 14: 20/37 Loss: 0.015111\n",
      "2025-04-23 14:29: **********Train Epoch 14: averaged Loss: 0.092074\n",
      "2025-04-23 14:29: **********Train Epoch 14: averaged Loss: 0.092074\n",
      "2025-04-23 14:29: **********Train Epoch 14: averaged Loss: 0.092074\n",
      "2025-04-23 14:29: **********Train Epoch 14: averaged Loss: 0.092074\n",
      "2025-04-23 14:29: **********Val Epoch 14: average Loss: 0.016999\n",
      "2025-04-23 14:29: **********Val Epoch 14: average Loss: 0.016999\n",
      "2025-04-23 14:29: **********Val Epoch 14: average Loss: 0.016999\n",
      "2025-04-23 14:29: **********Val Epoch 14: average Loss: 0.016999\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: *********************************Current best model saved!\n",
      "2025-04-23 14:29: Train Epoch 15: 0/37 Loss: 0.155760\n",
      "2025-04-23 14:29: Train Epoch 15: 0/37 Loss: 0.155760\n",
      "2025-04-23 14:29: Train Epoch 15: 0/37 Loss: 0.155760\n",
      "2025-04-23 14:29: Train Epoch 15: 0/37 Loss: 0.155760\n",
      "2025-04-23 14:29: Train Epoch 15: 20/37 Loss: 0.141696\n",
      "2025-04-23 14:29: Train Epoch 15: 20/37 Loss: 0.141696\n",
      "2025-04-23 14:29: Train Epoch 15: 20/37 Loss: 0.141696\n",
      "2025-04-23 14:29: Train Epoch 15: 20/37 Loss: 0.141696\n",
      "2025-04-23 14:29: **********Train Epoch 15: averaged Loss: 0.094617\n",
      "2025-04-23 14:29: **********Train Epoch 15: averaged Loss: 0.094617\n",
      "2025-04-23 14:29: **********Train Epoch 15: averaged Loss: 0.094617\n",
      "2025-04-23 14:29: **********Train Epoch 15: averaged Loss: 0.094617\n",
      "2025-04-23 14:29: **********Val Epoch 15: average Loss: 0.031665\n",
      "2025-04-23 14:29: **********Val Epoch 15: average Loss: 0.031665\n",
      "2025-04-23 14:29: **********Val Epoch 15: average Loss: 0.031665\n",
      "2025-04-23 14:29: **********Val Epoch 15: average Loss: 0.031665\n",
      "2025-04-23 14:29: Train Epoch 16: 0/37 Loss: 0.254673\n",
      "2025-04-23 14:29: Train Epoch 16: 0/37 Loss: 0.254673\n",
      "2025-04-23 14:29: Train Epoch 16: 0/37 Loss: 0.254673\n",
      "2025-04-23 14:29: Train Epoch 16: 0/37 Loss: 0.254673\n",
      "2025-04-23 14:29: Train Epoch 16: 20/37 Loss: 0.117860\n",
      "2025-04-23 14:29: Train Epoch 16: 20/37 Loss: 0.117860\n",
      "2025-04-23 14:29: Train Epoch 16: 20/37 Loss: 0.117860\n",
      "2025-04-23 14:29: Train Epoch 16: 20/37 Loss: 0.117860\n",
      "2025-04-23 14:29: **********Train Epoch 16: averaged Loss: 0.109792\n",
      "2025-04-23 14:29: **********Train Epoch 16: averaged Loss: 0.109792\n",
      "2025-04-23 14:29: **********Train Epoch 16: averaged Loss: 0.109792\n",
      "2025-04-23 14:29: **********Train Epoch 16: averaged Loss: 0.109792\n",
      "2025-04-23 14:29: **********Val Epoch 16: average Loss: 0.020607\n",
      "2025-04-23 14:29: **********Val Epoch 16: average Loss: 0.020607\n",
      "2025-04-23 14:29: **********Val Epoch 16: average Loss: 0.020607\n",
      "2025-04-23 14:29: **********Val Epoch 16: average Loss: 0.020607\n",
      "2025-04-23 14:29: Train Epoch 17: 0/37 Loss: 0.156324\n",
      "2025-04-23 14:29: Train Epoch 17: 0/37 Loss: 0.156324\n",
      "2025-04-23 14:29: Train Epoch 17: 0/37 Loss: 0.156324\n",
      "2025-04-23 14:29: Train Epoch 17: 0/37 Loss: 0.156324\n",
      "2025-04-23 14:29: Train Epoch 17: 20/37 Loss: 0.135639\n",
      "2025-04-23 14:29: Train Epoch 17: 20/37 Loss: 0.135639\n",
      "2025-04-23 14:29: Train Epoch 17: 20/37 Loss: 0.135639\n",
      "2025-04-23 14:29: Train Epoch 17: 20/37 Loss: 0.135639\n",
      "2025-04-23 14:30: **********Train Epoch 17: averaged Loss: 0.097399\n",
      "2025-04-23 14:30: **********Train Epoch 17: averaged Loss: 0.097399\n",
      "2025-04-23 14:30: **********Train Epoch 17: averaged Loss: 0.097399\n",
      "2025-04-23 14:30: **********Train Epoch 17: averaged Loss: 0.097399\n",
      "2025-04-23 14:30: **********Val Epoch 17: average Loss: 0.121472\n",
      "2025-04-23 14:30: **********Val Epoch 17: average Loss: 0.121472\n",
      "2025-04-23 14:30: **********Val Epoch 17: average Loss: 0.121472\n",
      "2025-04-23 14:30: **********Val Epoch 17: average Loss: 0.121472\n",
      "2025-04-23 14:30: Train Epoch 18: 0/37 Loss: 0.142214\n",
      "2025-04-23 14:30: Train Epoch 18: 0/37 Loss: 0.142214\n",
      "2025-04-23 14:30: Train Epoch 18: 0/37 Loss: 0.142214\n",
      "2025-04-23 14:30: Train Epoch 18: 0/37 Loss: 0.142214\n",
      "2025-04-23 14:30: Train Epoch 18: 20/37 Loss: 0.068804\n",
      "2025-04-23 14:30: Train Epoch 18: 20/37 Loss: 0.068804\n",
      "2025-04-23 14:30: Train Epoch 18: 20/37 Loss: 0.068804\n",
      "2025-04-23 14:30: Train Epoch 18: 20/37 Loss: 0.068804\n",
      "2025-04-23 14:30: **********Train Epoch 18: averaged Loss: 0.063782\n",
      "2025-04-23 14:30: **********Train Epoch 18: averaged Loss: 0.063782\n",
      "2025-04-23 14:30: **********Train Epoch 18: averaged Loss: 0.063782\n",
      "2025-04-23 14:30: **********Train Epoch 18: averaged Loss: 0.063782\n",
      "2025-04-23 14:30: **********Val Epoch 18: average Loss: 0.054022\n",
      "2025-04-23 14:30: **********Val Epoch 18: average Loss: 0.054022\n",
      "2025-04-23 14:30: **********Val Epoch 18: average Loss: 0.054022\n",
      "2025-04-23 14:30: **********Val Epoch 18: average Loss: 0.054022\n",
      "2025-04-23 14:30: Train Epoch 19: 0/37 Loss: 0.068821\n",
      "2025-04-23 14:30: Train Epoch 19: 0/37 Loss: 0.068821\n",
      "2025-04-23 14:30: Train Epoch 19: 0/37 Loss: 0.068821\n",
      "2025-04-23 14:30: Train Epoch 19: 0/37 Loss: 0.068821\n",
      "2025-04-23 14:30: Train Epoch 19: 20/37 Loss: 0.092633\n",
      "2025-04-23 14:30: Train Epoch 19: 20/37 Loss: 0.092633\n",
      "2025-04-23 14:30: Train Epoch 19: 20/37 Loss: 0.092633\n",
      "2025-04-23 14:30: Train Epoch 19: 20/37 Loss: 0.092633\n",
      "2025-04-23 14:30: **********Train Epoch 19: averaged Loss: 0.100333\n",
      "2025-04-23 14:30: **********Train Epoch 19: averaged Loss: 0.100333\n",
      "2025-04-23 14:30: **********Train Epoch 19: averaged Loss: 0.100333\n",
      "2025-04-23 14:30: **********Train Epoch 19: averaged Loss: 0.100333\n",
      "2025-04-23 14:30: **********Val Epoch 19: average Loss: 0.145707\n",
      "2025-04-23 14:30: **********Val Epoch 19: average Loss: 0.145707\n",
      "2025-04-23 14:30: **********Val Epoch 19: average Loss: 0.145707\n",
      "2025-04-23 14:30: **********Val Epoch 19: average Loss: 0.145707\n",
      "2025-04-23 14:30: Train Epoch 20: 0/37 Loss: 0.160981\n",
      "2025-04-23 14:30: Train Epoch 20: 0/37 Loss: 0.160981\n",
      "2025-04-23 14:30: Train Epoch 20: 0/37 Loss: 0.160981\n",
      "2025-04-23 14:30: Train Epoch 20: 0/37 Loss: 0.160981\n",
      "2025-04-23 14:30: Train Epoch 20: 20/37 Loss: 0.137500\n",
      "2025-04-23 14:30: Train Epoch 20: 20/37 Loss: 0.137500\n",
      "2025-04-23 14:30: Train Epoch 20: 20/37 Loss: 0.137500\n",
      "2025-04-23 14:30: Train Epoch 20: 20/37 Loss: 0.137500\n",
      "2025-04-23 14:30: **********Train Epoch 20: averaged Loss: 0.117719\n",
      "2025-04-23 14:30: **********Train Epoch 20: averaged Loss: 0.117719\n",
      "2025-04-23 14:30: **********Train Epoch 20: averaged Loss: 0.117719\n",
      "2025-04-23 14:30: **********Train Epoch 20: averaged Loss: 0.117719\n",
      "2025-04-23 14:30: **********Val Epoch 20: average Loss: 0.036817\n",
      "2025-04-23 14:30: **********Val Epoch 20: average Loss: 0.036817\n",
      "2025-04-23 14:30: **********Val Epoch 20: average Loss: 0.036817\n",
      "2025-04-23 14:30: **********Val Epoch 20: average Loss: 0.036817\n",
      "2025-04-23 14:30: Train Epoch 21: 0/37 Loss: 0.140799\n",
      "2025-04-23 14:30: Train Epoch 21: 0/37 Loss: 0.140799\n",
      "2025-04-23 14:30: Train Epoch 21: 0/37 Loss: 0.140799\n",
      "2025-04-23 14:30: Train Epoch 21: 0/37 Loss: 0.140799\n",
      "2025-04-23 14:30: Train Epoch 21: 20/37 Loss: 0.028615\n",
      "2025-04-23 14:30: Train Epoch 21: 20/37 Loss: 0.028615\n",
      "2025-04-23 14:30: Train Epoch 21: 20/37 Loss: 0.028615\n",
      "2025-04-23 14:30: Train Epoch 21: 20/37 Loss: 0.028615\n",
      "2025-04-23 14:30: **********Train Epoch 21: averaged Loss: 0.077057\n",
      "2025-04-23 14:30: **********Train Epoch 21: averaged Loss: 0.077057\n",
      "2025-04-23 14:30: **********Train Epoch 21: averaged Loss: 0.077057\n",
      "2025-04-23 14:30: **********Train Epoch 21: averaged Loss: 0.077057\n",
      "2025-04-23 14:30: **********Val Epoch 21: average Loss: 0.045667\n",
      "2025-04-23 14:30: **********Val Epoch 21: average Loss: 0.045667\n",
      "2025-04-23 14:30: **********Val Epoch 21: average Loss: 0.045667\n",
      "2025-04-23 14:30: **********Val Epoch 21: average Loss: 0.045667\n",
      "2025-04-23 14:30: Train Epoch 22: 0/37 Loss: 0.121627\n",
      "2025-04-23 14:30: Train Epoch 22: 0/37 Loss: 0.121627\n",
      "2025-04-23 14:30: Train Epoch 22: 0/37 Loss: 0.121627\n",
      "2025-04-23 14:30: Train Epoch 22: 0/37 Loss: 0.121627\n",
      "2025-04-23 14:30: Train Epoch 22: 20/37 Loss: 0.134468\n",
      "2025-04-23 14:30: Train Epoch 22: 20/37 Loss: 0.134468\n",
      "2025-04-23 14:30: Train Epoch 22: 20/37 Loss: 0.134468\n",
      "2025-04-23 14:30: Train Epoch 22: 20/37 Loss: 0.134468\n",
      "2025-04-23 14:30: **********Train Epoch 22: averaged Loss: 0.082591\n",
      "2025-04-23 14:30: **********Train Epoch 22: averaged Loss: 0.082591\n",
      "2025-04-23 14:30: **********Train Epoch 22: averaged Loss: 0.082591\n",
      "2025-04-23 14:30: **********Train Epoch 22: averaged Loss: 0.082591\n",
      "2025-04-23 14:30: **********Val Epoch 22: average Loss: 0.057394\n",
      "2025-04-23 14:30: **********Val Epoch 22: average Loss: 0.057394\n",
      "2025-04-23 14:30: **********Val Epoch 22: average Loss: 0.057394\n",
      "2025-04-23 14:30: **********Val Epoch 22: average Loss: 0.057394\n",
      "2025-04-23 14:30: Train Epoch 23: 0/37 Loss: 0.155695\n",
      "2025-04-23 14:30: Train Epoch 23: 0/37 Loss: 0.155695\n",
      "2025-04-23 14:30: Train Epoch 23: 0/37 Loss: 0.155695\n",
      "2025-04-23 14:30: Train Epoch 23: 0/37 Loss: 0.155695\n",
      "2025-04-23 14:30: Train Epoch 23: 20/37 Loss: 0.008070\n",
      "2025-04-23 14:30: Train Epoch 23: 20/37 Loss: 0.008070\n",
      "2025-04-23 14:30: Train Epoch 23: 20/37 Loss: 0.008070\n",
      "2025-04-23 14:30: Train Epoch 23: 20/37 Loss: 0.008070\n",
      "2025-04-23 14:30: **********Train Epoch 23: averaged Loss: 0.068176\n",
      "2025-04-23 14:30: **********Train Epoch 23: averaged Loss: 0.068176\n",
      "2025-04-23 14:30: **********Train Epoch 23: averaged Loss: 0.068176\n",
      "2025-04-23 14:30: **********Train Epoch 23: averaged Loss: 0.068176\n",
      "2025-04-23 14:30: **********Val Epoch 23: average Loss: 0.012395\n",
      "2025-04-23 14:30: **********Val Epoch 23: average Loss: 0.012395\n",
      "2025-04-23 14:30: **********Val Epoch 23: average Loss: 0.012395\n",
      "2025-04-23 14:30: **********Val Epoch 23: average Loss: 0.012395\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: Train Epoch 24: 0/37 Loss: 0.089862\n",
      "2025-04-23 14:30: Train Epoch 24: 0/37 Loss: 0.089862\n",
      "2025-04-23 14:30: Train Epoch 24: 0/37 Loss: 0.089862\n",
      "2025-04-23 14:30: Train Epoch 24: 0/37 Loss: 0.089862\n",
      "2025-04-23 14:30: Train Epoch 24: 20/37 Loss: 0.135411\n",
      "2025-04-23 14:30: Train Epoch 24: 20/37 Loss: 0.135411\n",
      "2025-04-23 14:30: Train Epoch 24: 20/37 Loss: 0.135411\n",
      "2025-04-23 14:30: Train Epoch 24: 20/37 Loss: 0.135411\n",
      "2025-04-23 14:30: **********Train Epoch 24: averaged Loss: 0.069062\n",
      "2025-04-23 14:30: **********Train Epoch 24: averaged Loss: 0.069062\n",
      "2025-04-23 14:30: **********Train Epoch 24: averaged Loss: 0.069062\n",
      "2025-04-23 14:30: **********Train Epoch 24: averaged Loss: 0.069062\n",
      "2025-04-23 14:30: **********Val Epoch 24: average Loss: 0.044537\n",
      "2025-04-23 14:30: **********Val Epoch 24: average Loss: 0.044537\n",
      "2025-04-23 14:30: **********Val Epoch 24: average Loss: 0.044537\n",
      "2025-04-23 14:30: **********Val Epoch 24: average Loss: 0.044537\n",
      "2025-04-23 14:30: Train Epoch 25: 0/37 Loss: 0.157915\n",
      "2025-04-23 14:30: Train Epoch 25: 0/37 Loss: 0.157915\n",
      "2025-04-23 14:30: Train Epoch 25: 0/37 Loss: 0.157915\n",
      "2025-04-23 14:30: Train Epoch 25: 0/37 Loss: 0.157915\n",
      "2025-04-23 14:30: Train Epoch 25: 20/37 Loss: 0.036507\n",
      "2025-04-23 14:30: Train Epoch 25: 20/37 Loss: 0.036507\n",
      "2025-04-23 14:30: Train Epoch 25: 20/37 Loss: 0.036507\n",
      "2025-04-23 14:30: Train Epoch 25: 20/37 Loss: 0.036507\n",
      "2025-04-23 14:30: **********Train Epoch 25: averaged Loss: 0.063774\n",
      "2025-04-23 14:30: **********Train Epoch 25: averaged Loss: 0.063774\n",
      "2025-04-23 14:30: **********Train Epoch 25: averaged Loss: 0.063774\n",
      "2025-04-23 14:30: **********Train Epoch 25: averaged Loss: 0.063774\n",
      "2025-04-23 14:30: **********Val Epoch 25: average Loss: 0.041278\n",
      "2025-04-23 14:30: **********Val Epoch 25: average Loss: 0.041278\n",
      "2025-04-23 14:30: **********Val Epoch 25: average Loss: 0.041278\n",
      "2025-04-23 14:30: **********Val Epoch 25: average Loss: 0.041278\n",
      "2025-04-23 14:30: Train Epoch 26: 0/37 Loss: 0.100180\n",
      "2025-04-23 14:30: Train Epoch 26: 0/37 Loss: 0.100180\n",
      "2025-04-23 14:30: Train Epoch 26: 0/37 Loss: 0.100180\n",
      "2025-04-23 14:30: Train Epoch 26: 0/37 Loss: 0.100180\n",
      "2025-04-23 14:30: Train Epoch 26: 20/37 Loss: 0.009517\n",
      "2025-04-23 14:30: Train Epoch 26: 20/37 Loss: 0.009517\n",
      "2025-04-23 14:30: Train Epoch 26: 20/37 Loss: 0.009517\n",
      "2025-04-23 14:30: Train Epoch 26: 20/37 Loss: 0.009517\n",
      "2025-04-23 14:30: **********Train Epoch 26: averaged Loss: 0.046408\n",
      "2025-04-23 14:30: **********Train Epoch 26: averaged Loss: 0.046408\n",
      "2025-04-23 14:30: **********Train Epoch 26: averaged Loss: 0.046408\n",
      "2025-04-23 14:30: **********Train Epoch 26: averaged Loss: 0.046408\n",
      "2025-04-23 14:30: **********Val Epoch 26: average Loss: 0.021509\n",
      "2025-04-23 14:30: **********Val Epoch 26: average Loss: 0.021509\n",
      "2025-04-23 14:30: **********Val Epoch 26: average Loss: 0.021509\n",
      "2025-04-23 14:30: **********Val Epoch 26: average Loss: 0.021509\n",
      "2025-04-23 14:30: Train Epoch 27: 0/37 Loss: 0.102661\n",
      "2025-04-23 14:30: Train Epoch 27: 0/37 Loss: 0.102661\n",
      "2025-04-23 14:30: Train Epoch 27: 0/37 Loss: 0.102661\n",
      "2025-04-23 14:30: Train Epoch 27: 0/37 Loss: 0.102661\n",
      "2025-04-23 14:30: Train Epoch 27: 20/37 Loss: 0.010127\n",
      "2025-04-23 14:30: Train Epoch 27: 20/37 Loss: 0.010127\n",
      "2025-04-23 14:30: Train Epoch 27: 20/37 Loss: 0.010127\n",
      "2025-04-23 14:30: Train Epoch 27: 20/37 Loss: 0.010127\n",
      "2025-04-23 14:30: **********Train Epoch 27: averaged Loss: 0.046839\n",
      "2025-04-23 14:30: **********Train Epoch 27: averaged Loss: 0.046839\n",
      "2025-04-23 14:30: **********Train Epoch 27: averaged Loss: 0.046839\n",
      "2025-04-23 14:30: **********Train Epoch 27: averaged Loss: 0.046839\n",
      "2025-04-23 14:30: **********Val Epoch 27: average Loss: 0.023243\n",
      "2025-04-23 14:30: **********Val Epoch 27: average Loss: 0.023243\n",
      "2025-04-23 14:30: **********Val Epoch 27: average Loss: 0.023243\n",
      "2025-04-23 14:30: **********Val Epoch 27: average Loss: 0.023243\n",
      "2025-04-23 14:30: Train Epoch 28: 0/37 Loss: 0.149172\n",
      "2025-04-23 14:30: Train Epoch 28: 0/37 Loss: 0.149172\n",
      "2025-04-23 14:30: Train Epoch 28: 0/37 Loss: 0.149172\n",
      "2025-04-23 14:30: Train Epoch 28: 0/37 Loss: 0.149172\n",
      "2025-04-23 14:30: Train Epoch 28: 20/37 Loss: 0.048237\n",
      "2025-04-23 14:30: Train Epoch 28: 20/37 Loss: 0.048237\n",
      "2025-04-23 14:30: Train Epoch 28: 20/37 Loss: 0.048237\n",
      "2025-04-23 14:30: Train Epoch 28: 20/37 Loss: 0.048237\n",
      "2025-04-23 14:30: **********Train Epoch 28: averaged Loss: 0.063947\n",
      "2025-04-23 14:30: **********Train Epoch 28: averaged Loss: 0.063947\n",
      "2025-04-23 14:30: **********Train Epoch 28: averaged Loss: 0.063947\n",
      "2025-04-23 14:30: **********Train Epoch 28: averaged Loss: 0.063947\n",
      "2025-04-23 14:30: **********Val Epoch 28: average Loss: 0.025012\n",
      "2025-04-23 14:30: **********Val Epoch 28: average Loss: 0.025012\n",
      "2025-04-23 14:30: **********Val Epoch 28: average Loss: 0.025012\n",
      "2025-04-23 14:30: **********Val Epoch 28: average Loss: 0.025012\n",
      "2025-04-23 14:30: Train Epoch 29: 0/37 Loss: 0.173759\n",
      "2025-04-23 14:30: Train Epoch 29: 0/37 Loss: 0.173759\n",
      "2025-04-23 14:30: Train Epoch 29: 0/37 Loss: 0.173759\n",
      "2025-04-23 14:30: Train Epoch 29: 0/37 Loss: 0.173759\n",
      "2025-04-23 14:30: Train Epoch 29: 20/37 Loss: 0.096050\n",
      "2025-04-23 14:30: Train Epoch 29: 20/37 Loss: 0.096050\n",
      "2025-04-23 14:30: Train Epoch 29: 20/37 Loss: 0.096050\n",
      "2025-04-23 14:30: Train Epoch 29: 20/37 Loss: 0.096050\n",
      "2025-04-23 14:30: **********Train Epoch 29: averaged Loss: 0.076964\n",
      "2025-04-23 14:30: **********Train Epoch 29: averaged Loss: 0.076964\n",
      "2025-04-23 14:30: **********Train Epoch 29: averaged Loss: 0.076964\n",
      "2025-04-23 14:30: **********Train Epoch 29: averaged Loss: 0.076964\n",
      "2025-04-23 14:30: **********Val Epoch 29: average Loss: 0.110033\n",
      "2025-04-23 14:30: **********Val Epoch 29: average Loss: 0.110033\n",
      "2025-04-23 14:30: **********Val Epoch 29: average Loss: 0.110033\n",
      "2025-04-23 14:30: **********Val Epoch 29: average Loss: 0.110033\n",
      "2025-04-23 14:30: Train Epoch 30: 0/37 Loss: 0.162527\n",
      "2025-04-23 14:30: Train Epoch 30: 0/37 Loss: 0.162527\n",
      "2025-04-23 14:30: Train Epoch 30: 0/37 Loss: 0.162527\n",
      "2025-04-23 14:30: Train Epoch 30: 0/37 Loss: 0.162527\n",
      "2025-04-23 14:30: Train Epoch 30: 20/37 Loss: 0.035031\n",
      "2025-04-23 14:30: Train Epoch 30: 20/37 Loss: 0.035031\n",
      "2025-04-23 14:30: Train Epoch 30: 20/37 Loss: 0.035031\n",
      "2025-04-23 14:30: Train Epoch 30: 20/37 Loss: 0.035031\n",
      "2025-04-23 14:30: **********Train Epoch 30: averaged Loss: 0.063056\n",
      "2025-04-23 14:30: **********Train Epoch 30: averaged Loss: 0.063056\n",
      "2025-04-23 14:30: **********Train Epoch 30: averaged Loss: 0.063056\n",
      "2025-04-23 14:30: **********Train Epoch 30: averaged Loss: 0.063056\n",
      "2025-04-23 14:30: **********Val Epoch 30: average Loss: 0.024765\n",
      "2025-04-23 14:30: **********Val Epoch 30: average Loss: 0.024765\n",
      "2025-04-23 14:30: **********Val Epoch 30: average Loss: 0.024765\n",
      "2025-04-23 14:30: **********Val Epoch 30: average Loss: 0.024765\n",
      "2025-04-23 14:30: Train Epoch 31: 0/37 Loss: 0.100558\n",
      "2025-04-23 14:30: Train Epoch 31: 0/37 Loss: 0.100558\n",
      "2025-04-23 14:30: Train Epoch 31: 0/37 Loss: 0.100558\n",
      "2025-04-23 14:30: Train Epoch 31: 0/37 Loss: 0.100558\n",
      "2025-04-23 14:30: Train Epoch 31: 20/37 Loss: 0.006953\n",
      "2025-04-23 14:30: Train Epoch 31: 20/37 Loss: 0.006953\n",
      "2025-04-23 14:30: Train Epoch 31: 20/37 Loss: 0.006953\n",
      "2025-04-23 14:30: Train Epoch 31: 20/37 Loss: 0.006953\n",
      "2025-04-23 14:30: **********Train Epoch 31: averaged Loss: 0.032978\n",
      "2025-04-23 14:30: **********Train Epoch 31: averaged Loss: 0.032978\n",
      "2025-04-23 14:30: **********Train Epoch 31: averaged Loss: 0.032978\n",
      "2025-04-23 14:30: **********Train Epoch 31: averaged Loss: 0.032978\n",
      "2025-04-23 14:30: **********Val Epoch 31: average Loss: 0.085462\n",
      "2025-04-23 14:30: **********Val Epoch 31: average Loss: 0.085462\n",
      "2025-04-23 14:30: **********Val Epoch 31: average Loss: 0.085462\n",
      "2025-04-23 14:30: **********Val Epoch 31: average Loss: 0.085462\n",
      "2025-04-23 14:30: Train Epoch 32: 0/37 Loss: 0.120209\n",
      "2025-04-23 14:30: Train Epoch 32: 0/37 Loss: 0.120209\n",
      "2025-04-23 14:30: Train Epoch 32: 0/37 Loss: 0.120209\n",
      "2025-04-23 14:30: Train Epoch 32: 0/37 Loss: 0.120209\n",
      "2025-04-23 14:30: Train Epoch 32: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:30: Train Epoch 32: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:30: Train Epoch 32: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:30: Train Epoch 32: 20/37 Loss: 0.045371\n",
      "2025-04-23 14:30: **********Train Epoch 32: averaged Loss: 0.046640\n",
      "2025-04-23 14:30: **********Train Epoch 32: averaged Loss: 0.046640\n",
      "2025-04-23 14:30: **********Train Epoch 32: averaged Loss: 0.046640\n",
      "2025-04-23 14:30: **********Train Epoch 32: averaged Loss: 0.046640\n",
      "2025-04-23 14:30: **********Val Epoch 32: average Loss: 0.081218\n",
      "2025-04-23 14:30: **********Val Epoch 32: average Loss: 0.081218\n",
      "2025-04-23 14:30: **********Val Epoch 32: average Loss: 0.081218\n",
      "2025-04-23 14:30: **********Val Epoch 32: average Loss: 0.081218\n",
      "2025-04-23 14:30: Train Epoch 33: 0/37 Loss: 0.155086\n",
      "2025-04-23 14:30: Train Epoch 33: 0/37 Loss: 0.155086\n",
      "2025-04-23 14:30: Train Epoch 33: 0/37 Loss: 0.155086\n",
      "2025-04-23 14:30: Train Epoch 33: 0/37 Loss: 0.155086\n",
      "2025-04-23 14:30: Train Epoch 33: 20/37 Loss: 0.044658\n",
      "2025-04-23 14:30: Train Epoch 33: 20/37 Loss: 0.044658\n",
      "2025-04-23 14:30: Train Epoch 33: 20/37 Loss: 0.044658\n",
      "2025-04-23 14:30: Train Epoch 33: 20/37 Loss: 0.044658\n",
      "2025-04-23 14:30: **********Train Epoch 33: averaged Loss: 0.074650\n",
      "2025-04-23 14:30: **********Train Epoch 33: averaged Loss: 0.074650\n",
      "2025-04-23 14:30: **********Train Epoch 33: averaged Loss: 0.074650\n",
      "2025-04-23 14:30: **********Train Epoch 33: averaged Loss: 0.074650\n",
      "2025-04-23 14:30: **********Val Epoch 33: average Loss: 0.036377\n",
      "2025-04-23 14:30: **********Val Epoch 33: average Loss: 0.036377\n",
      "2025-04-23 14:30: **********Val Epoch 33: average Loss: 0.036377\n",
      "2025-04-23 14:30: **********Val Epoch 33: average Loss: 0.036377\n",
      "2025-04-23 14:30: Train Epoch 34: 0/37 Loss: 0.100441\n",
      "2025-04-23 14:30: Train Epoch 34: 0/37 Loss: 0.100441\n",
      "2025-04-23 14:30: Train Epoch 34: 0/37 Loss: 0.100441\n",
      "2025-04-23 14:30: Train Epoch 34: 0/37 Loss: 0.100441\n",
      "2025-04-23 14:30: Train Epoch 34: 20/37 Loss: 0.006346\n",
      "2025-04-23 14:30: Train Epoch 34: 20/37 Loss: 0.006346\n",
      "2025-04-23 14:30: Train Epoch 34: 20/37 Loss: 0.006346\n",
      "2025-04-23 14:30: Train Epoch 34: 20/37 Loss: 0.006346\n",
      "2025-04-23 14:30: **********Train Epoch 34: averaged Loss: 0.039959\n",
      "2025-04-23 14:30: **********Train Epoch 34: averaged Loss: 0.039959\n",
      "2025-04-23 14:30: **********Train Epoch 34: averaged Loss: 0.039959\n",
      "2025-04-23 14:30: **********Train Epoch 34: averaged Loss: 0.039959\n",
      "2025-04-23 14:30: **********Val Epoch 34: average Loss: 0.075388\n",
      "2025-04-23 14:30: **********Val Epoch 34: average Loss: 0.075388\n",
      "2025-04-23 14:30: **********Val Epoch 34: average Loss: 0.075388\n",
      "2025-04-23 14:30: **********Val Epoch 34: average Loss: 0.075388\n",
      "2025-04-23 14:30: Train Epoch 35: 0/37 Loss: 0.092132\n",
      "2025-04-23 14:30: Train Epoch 35: 0/37 Loss: 0.092132\n",
      "2025-04-23 14:30: Train Epoch 35: 0/37 Loss: 0.092132\n",
      "2025-04-23 14:30: Train Epoch 35: 0/37 Loss: 0.092132\n",
      "2025-04-23 14:30: Train Epoch 35: 20/37 Loss: 0.012265\n",
      "2025-04-23 14:30: Train Epoch 35: 20/37 Loss: 0.012265\n",
      "2025-04-23 14:30: Train Epoch 35: 20/37 Loss: 0.012265\n",
      "2025-04-23 14:30: Train Epoch 35: 20/37 Loss: 0.012265\n",
      "2025-04-23 14:30: **********Train Epoch 35: averaged Loss: 0.041964\n",
      "2025-04-23 14:30: **********Train Epoch 35: averaged Loss: 0.041964\n",
      "2025-04-23 14:30: **********Train Epoch 35: averaged Loss: 0.041964\n",
      "2025-04-23 14:30: **********Train Epoch 35: averaged Loss: 0.041964\n",
      "2025-04-23 14:30: **********Val Epoch 35: average Loss: 0.010652\n",
      "2025-04-23 14:30: **********Val Epoch 35: average Loss: 0.010652\n",
      "2025-04-23 14:30: **********Val Epoch 35: average Loss: 0.010652\n",
      "2025-04-23 14:30: **********Val Epoch 35: average Loss: 0.010652\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: *********************************Current best model saved!\n",
      "2025-04-23 14:30: Train Epoch 36: 0/37 Loss: 0.095773\n",
      "2025-04-23 14:30: Train Epoch 36: 0/37 Loss: 0.095773\n",
      "2025-04-23 14:30: Train Epoch 36: 0/37 Loss: 0.095773\n",
      "2025-04-23 14:30: Train Epoch 36: 0/37 Loss: 0.095773\n",
      "2025-04-23 14:30: Train Epoch 36: 20/37 Loss: 0.008102\n",
      "2025-04-23 14:30: Train Epoch 36: 20/37 Loss: 0.008102\n",
      "2025-04-23 14:30: Train Epoch 36: 20/37 Loss: 0.008102\n",
      "2025-04-23 14:30: Train Epoch 36: 20/37 Loss: 0.008102\n",
      "2025-04-23 14:30: **********Train Epoch 36: averaged Loss: 0.055879\n",
      "2025-04-23 14:30: **********Train Epoch 36: averaged Loss: 0.055879\n",
      "2025-04-23 14:30: **********Train Epoch 36: averaged Loss: 0.055879\n",
      "2025-04-23 14:30: **********Train Epoch 36: averaged Loss: 0.055879\n",
      "2025-04-23 14:30: **********Val Epoch 36: average Loss: 0.116709\n",
      "2025-04-23 14:30: **********Val Epoch 36: average Loss: 0.116709\n",
      "2025-04-23 14:30: **********Val Epoch 36: average Loss: 0.116709\n",
      "2025-04-23 14:30: **********Val Epoch 36: average Loss: 0.116709\n",
      "2025-04-23 14:30: Train Epoch 37: 0/37 Loss: 0.108075\n",
      "2025-04-23 14:30: Train Epoch 37: 0/37 Loss: 0.108075\n",
      "2025-04-23 14:30: Train Epoch 37: 0/37 Loss: 0.108075\n",
      "2025-04-23 14:30: Train Epoch 37: 0/37 Loss: 0.108075\n",
      "2025-04-23 14:30: Train Epoch 37: 20/37 Loss: 0.040564\n",
      "2025-04-23 14:30: Train Epoch 37: 20/37 Loss: 0.040564\n",
      "2025-04-23 14:30: Train Epoch 37: 20/37 Loss: 0.040564\n",
      "2025-04-23 14:30: Train Epoch 37: 20/37 Loss: 0.040564\n",
      "2025-04-23 14:30: **********Train Epoch 37: averaged Loss: 0.041222\n",
      "2025-04-23 14:30: **********Train Epoch 37: averaged Loss: 0.041222\n",
      "2025-04-23 14:30: **********Train Epoch 37: averaged Loss: 0.041222\n",
      "2025-04-23 14:30: **********Train Epoch 37: averaged Loss: 0.041222\n",
      "2025-04-23 14:30: **********Val Epoch 37: average Loss: 0.056556\n",
      "2025-04-23 14:30: **********Val Epoch 37: average Loss: 0.056556\n",
      "2025-04-23 14:30: **********Val Epoch 37: average Loss: 0.056556\n",
      "2025-04-23 14:30: **********Val Epoch 37: average Loss: 0.056556\n",
      "2025-04-23 14:30: Train Epoch 38: 0/37 Loss: 0.198601\n",
      "2025-04-23 14:30: Train Epoch 38: 0/37 Loss: 0.198601\n",
      "2025-04-23 14:30: Train Epoch 38: 0/37 Loss: 0.198601\n",
      "2025-04-23 14:30: Train Epoch 38: 0/37 Loss: 0.198601\n",
      "2025-04-23 14:30: Train Epoch 38: 20/37 Loss: 0.065156\n",
      "2025-04-23 14:30: Train Epoch 38: 20/37 Loss: 0.065156\n",
      "2025-04-23 14:30: Train Epoch 38: 20/37 Loss: 0.065156\n",
      "2025-04-23 14:30: Train Epoch 38: 20/37 Loss: 0.065156\n",
      "2025-04-23 14:30: **********Train Epoch 38: averaged Loss: 0.074460\n",
      "2025-04-23 14:30: **********Train Epoch 38: averaged Loss: 0.074460\n",
      "2025-04-23 14:30: **********Train Epoch 38: averaged Loss: 0.074460\n",
      "2025-04-23 14:30: **********Train Epoch 38: averaged Loss: 0.074460\n",
      "2025-04-23 14:30: **********Val Epoch 38: average Loss: 0.061244\n",
      "2025-04-23 14:30: **********Val Epoch 38: average Loss: 0.061244\n",
      "2025-04-23 14:30: **********Val Epoch 38: average Loss: 0.061244\n",
      "2025-04-23 14:30: **********Val Epoch 38: average Loss: 0.061244\n",
      "2025-04-23 14:30: Train Epoch 39: 0/37 Loss: 0.076436\n",
      "2025-04-23 14:30: Train Epoch 39: 0/37 Loss: 0.076436\n",
      "2025-04-23 14:30: Train Epoch 39: 0/37 Loss: 0.076436\n",
      "2025-04-23 14:30: Train Epoch 39: 0/37 Loss: 0.076436\n",
      "2025-04-23 14:30: Train Epoch 39: 20/37 Loss: 0.022045\n",
      "2025-04-23 14:30: Train Epoch 39: 20/37 Loss: 0.022045\n",
      "2025-04-23 14:30: Train Epoch 39: 20/37 Loss: 0.022045\n",
      "2025-04-23 14:30: Train Epoch 39: 20/37 Loss: 0.022045\n",
      "2025-04-23 14:30: **********Train Epoch 39: averaged Loss: 0.033557\n",
      "2025-04-23 14:30: **********Train Epoch 39: averaged Loss: 0.033557\n",
      "2025-04-23 14:30: **********Train Epoch 39: averaged Loss: 0.033557\n",
      "2025-04-23 14:30: **********Train Epoch 39: averaged Loss: 0.033557\n",
      "2025-04-23 14:30: **********Val Epoch 39: average Loss: 0.051593\n",
      "2025-04-23 14:30: **********Val Epoch 39: average Loss: 0.051593\n",
      "2025-04-23 14:30: **********Val Epoch 39: average Loss: 0.051593\n",
      "2025-04-23 14:30: **********Val Epoch 39: average Loss: 0.051593\n",
      "2025-04-23 14:30: Train Epoch 40: 0/37 Loss: 0.031836\n",
      "2025-04-23 14:30: Train Epoch 40: 0/37 Loss: 0.031836\n",
      "2025-04-23 14:30: Train Epoch 40: 0/37 Loss: 0.031836\n",
      "2025-04-23 14:30: Train Epoch 40: 0/37 Loss: 0.031836\n",
      "2025-04-23 14:30: Train Epoch 40: 20/37 Loss: 0.027914\n",
      "2025-04-23 14:30: Train Epoch 40: 20/37 Loss: 0.027914\n",
      "2025-04-23 14:30: Train Epoch 40: 20/37 Loss: 0.027914\n",
      "2025-04-23 14:30: Train Epoch 40: 20/37 Loss: 0.027914\n",
      "2025-04-23 14:30: **********Train Epoch 40: averaged Loss: 0.062284\n",
      "2025-04-23 14:30: **********Train Epoch 40: averaged Loss: 0.062284\n",
      "2025-04-23 14:30: **********Train Epoch 40: averaged Loss: 0.062284\n",
      "2025-04-23 14:30: **********Train Epoch 40: averaged Loss: 0.062284\n",
      "2025-04-23 14:30: **********Val Epoch 40: average Loss: 0.175140\n",
      "2025-04-23 14:30: **********Val Epoch 40: average Loss: 0.175140\n",
      "2025-04-23 14:30: **********Val Epoch 40: average Loss: 0.175140\n",
      "2025-04-23 14:30: **********Val Epoch 40: average Loss: 0.175140\n",
      "2025-04-23 14:30: Train Epoch 41: 0/37 Loss: 0.042641\n",
      "2025-04-23 14:30: Train Epoch 41: 0/37 Loss: 0.042641\n",
      "2025-04-23 14:30: Train Epoch 41: 0/37 Loss: 0.042641\n",
      "2025-04-23 14:30: Train Epoch 41: 0/37 Loss: 0.042641\n",
      "2025-04-23 14:30: Train Epoch 41: 20/37 Loss: 0.056368\n",
      "2025-04-23 14:30: Train Epoch 41: 20/37 Loss: 0.056368\n",
      "2025-04-23 14:30: Train Epoch 41: 20/37 Loss: 0.056368\n",
      "2025-04-23 14:30: Train Epoch 41: 20/37 Loss: 0.056368\n",
      "2025-04-23 14:30: **********Train Epoch 41: averaged Loss: 0.063935\n",
      "2025-04-23 14:30: **********Train Epoch 41: averaged Loss: 0.063935\n",
      "2025-04-23 14:30: **********Train Epoch 41: averaged Loss: 0.063935\n",
      "2025-04-23 14:30: **********Train Epoch 41: averaged Loss: 0.063935\n",
      "2025-04-23 14:30: **********Val Epoch 41: average Loss: 0.063884\n",
      "2025-04-23 14:30: **********Val Epoch 41: average Loss: 0.063884\n",
      "2025-04-23 14:30: **********Val Epoch 41: average Loss: 0.063884\n",
      "2025-04-23 14:30: **********Val Epoch 41: average Loss: 0.063884\n",
      "2025-04-23 14:30: Train Epoch 42: 0/37 Loss: 0.243378\n",
      "2025-04-23 14:30: Train Epoch 42: 0/37 Loss: 0.243378\n",
      "2025-04-23 14:30: Train Epoch 42: 0/37 Loss: 0.243378\n",
      "2025-04-23 14:30: Train Epoch 42: 0/37 Loss: 0.243378\n",
      "2025-04-23 14:30: Train Epoch 42: 20/37 Loss: 0.026125\n",
      "2025-04-23 14:30: Train Epoch 42: 20/37 Loss: 0.026125\n",
      "2025-04-23 14:30: Train Epoch 42: 20/37 Loss: 0.026125\n",
      "2025-04-23 14:30: Train Epoch 42: 20/37 Loss: 0.026125\n",
      "2025-04-23 14:30: **********Train Epoch 42: averaged Loss: 0.108488\n",
      "2025-04-23 14:30: **********Train Epoch 42: averaged Loss: 0.108488\n",
      "2025-04-23 14:30: **********Train Epoch 42: averaged Loss: 0.108488\n",
      "2025-04-23 14:30: **********Train Epoch 42: averaged Loss: 0.108488\n",
      "2025-04-23 14:30: **********Val Epoch 42: average Loss: 0.072822\n",
      "2025-04-23 14:30: **********Val Epoch 42: average Loss: 0.072822\n",
      "2025-04-23 14:30: **********Val Epoch 42: average Loss: 0.072822\n",
      "2025-04-23 14:30: **********Val Epoch 42: average Loss: 0.072822\n",
      "2025-04-23 14:30: Train Epoch 43: 0/37 Loss: 0.056751\n",
      "2025-04-23 14:30: Train Epoch 43: 0/37 Loss: 0.056751\n",
      "2025-04-23 14:30: Train Epoch 43: 0/37 Loss: 0.056751\n",
      "2025-04-23 14:30: Train Epoch 43: 0/37 Loss: 0.056751\n",
      "2025-04-23 14:30: Train Epoch 43: 20/37 Loss: 0.014553\n",
      "2025-04-23 14:30: Train Epoch 43: 20/37 Loss: 0.014553\n",
      "2025-04-23 14:30: Train Epoch 43: 20/37 Loss: 0.014553\n",
      "2025-04-23 14:30: Train Epoch 43: 20/37 Loss: 0.014553\n",
      "2025-04-23 14:30: **********Train Epoch 43: averaged Loss: 0.050944\n",
      "2025-04-23 14:30: **********Train Epoch 43: averaged Loss: 0.050944\n",
      "2025-04-23 14:30: **********Train Epoch 43: averaged Loss: 0.050944\n",
      "2025-04-23 14:30: **********Train Epoch 43: averaged Loss: 0.050944\n",
      "2025-04-23 14:30: **********Val Epoch 43: average Loss: 0.028887\n",
      "2025-04-23 14:30: **********Val Epoch 43: average Loss: 0.028887\n",
      "2025-04-23 14:30: **********Val Epoch 43: average Loss: 0.028887\n",
      "2025-04-23 14:30: **********Val Epoch 43: average Loss: 0.028887\n",
      "2025-04-23 14:30: Train Epoch 44: 0/37 Loss: 0.206991\n",
      "2025-04-23 14:30: Train Epoch 44: 0/37 Loss: 0.206991\n",
      "2025-04-23 14:30: Train Epoch 44: 0/37 Loss: 0.206991\n",
      "2025-04-23 14:30: Train Epoch 44: 0/37 Loss: 0.206991\n",
      "2025-04-23 14:30: Train Epoch 44: 20/37 Loss: 0.039735\n",
      "2025-04-23 14:30: Train Epoch 44: 20/37 Loss: 0.039735\n",
      "2025-04-23 14:30: Train Epoch 44: 20/37 Loss: 0.039735\n",
      "2025-04-23 14:30: Train Epoch 44: 20/37 Loss: 0.039735\n",
      "2025-04-23 14:30: **********Train Epoch 44: averaged Loss: 0.080521\n",
      "2025-04-23 14:30: **********Train Epoch 44: averaged Loss: 0.080521\n",
      "2025-04-23 14:30: **********Train Epoch 44: averaged Loss: 0.080521\n",
      "2025-04-23 14:30: **********Train Epoch 44: averaged Loss: 0.080521\n",
      "2025-04-23 14:30: **********Val Epoch 44: average Loss: 0.019669\n",
      "2025-04-23 14:30: **********Val Epoch 44: average Loss: 0.019669\n",
      "2025-04-23 14:30: **********Val Epoch 44: average Loss: 0.019669\n",
      "2025-04-23 14:30: **********Val Epoch 44: average Loss: 0.019669\n",
      "2025-04-23 14:30: Train Epoch 45: 0/37 Loss: 0.076310\n",
      "2025-04-23 14:30: Train Epoch 45: 0/37 Loss: 0.076310\n",
      "2025-04-23 14:30: Train Epoch 45: 0/37 Loss: 0.076310\n",
      "2025-04-23 14:30: Train Epoch 45: 0/37 Loss: 0.076310\n",
      "2025-04-23 14:30: Train Epoch 45: 20/37 Loss: 0.010234\n",
      "2025-04-23 14:30: Train Epoch 45: 20/37 Loss: 0.010234\n",
      "2025-04-23 14:30: Train Epoch 45: 20/37 Loss: 0.010234\n",
      "2025-04-23 14:30: Train Epoch 45: 20/37 Loss: 0.010234\n",
      "2025-04-23 14:30: **********Train Epoch 45: averaged Loss: 0.033252\n",
      "2025-04-23 14:30: **********Train Epoch 45: averaged Loss: 0.033252\n",
      "2025-04-23 14:30: **********Train Epoch 45: averaged Loss: 0.033252\n",
      "2025-04-23 14:30: **********Train Epoch 45: averaged Loss: 0.033252\n",
      "2025-04-23 14:30: **********Val Epoch 45: average Loss: 0.095757\n",
      "2025-04-23 14:30: **********Val Epoch 45: average Loss: 0.095757\n",
      "2025-04-23 14:30: **********Val Epoch 45: average Loss: 0.095757\n",
      "2025-04-23 14:30: **********Val Epoch 45: average Loss: 0.095757\n",
      "2025-04-23 14:30: Train Epoch 46: 0/37 Loss: 0.079636\n",
      "2025-04-23 14:30: Train Epoch 46: 0/37 Loss: 0.079636\n",
      "2025-04-23 14:30: Train Epoch 46: 0/37 Loss: 0.079636\n",
      "2025-04-23 14:30: Train Epoch 46: 0/37 Loss: 0.079636\n",
      "2025-04-23 14:30: Train Epoch 46: 20/37 Loss: 0.011061\n",
      "2025-04-23 14:30: Train Epoch 46: 20/37 Loss: 0.011061\n",
      "2025-04-23 14:30: Train Epoch 46: 20/37 Loss: 0.011061\n",
      "2025-04-23 14:30: Train Epoch 46: 20/37 Loss: 0.011061\n",
      "2025-04-23 14:30: **********Train Epoch 46: averaged Loss: 0.023604\n",
      "2025-04-23 14:30: **********Train Epoch 46: averaged Loss: 0.023604\n",
      "2025-04-23 14:30: **********Train Epoch 46: averaged Loss: 0.023604\n",
      "2025-04-23 14:30: **********Train Epoch 46: averaged Loss: 0.023604\n",
      "2025-04-23 14:30: **********Val Epoch 46: average Loss: 0.051967\n",
      "2025-04-23 14:30: **********Val Epoch 46: average Loss: 0.051967\n",
      "2025-04-23 14:30: **********Val Epoch 46: average Loss: 0.051967\n",
      "2025-04-23 14:30: **********Val Epoch 46: average Loss: 0.051967\n",
      "2025-04-23 14:30: Train Epoch 47: 0/37 Loss: 0.069032\n",
      "2025-04-23 14:30: Train Epoch 47: 0/37 Loss: 0.069032\n",
      "2025-04-23 14:30: Train Epoch 47: 0/37 Loss: 0.069032\n",
      "2025-04-23 14:30: Train Epoch 47: 0/37 Loss: 0.069032\n",
      "2025-04-23 14:30: Train Epoch 47: 20/37 Loss: 0.006336\n",
      "2025-04-23 14:30: Train Epoch 47: 20/37 Loss: 0.006336\n",
      "2025-04-23 14:30: Train Epoch 47: 20/37 Loss: 0.006336\n",
      "2025-04-23 14:30: Train Epoch 47: 20/37 Loss: 0.006336\n",
      "2025-04-23 14:30: **********Train Epoch 47: averaged Loss: 0.037078\n",
      "2025-04-23 14:30: **********Train Epoch 47: averaged Loss: 0.037078\n",
      "2025-04-23 14:30: **********Train Epoch 47: averaged Loss: 0.037078\n",
      "2025-04-23 14:30: **********Train Epoch 47: averaged Loss: 0.037078\n",
      "2025-04-23 14:30: **********Val Epoch 47: average Loss: 0.030892\n",
      "2025-04-23 14:30: **********Val Epoch 47: average Loss: 0.030892\n",
      "2025-04-23 14:30: **********Val Epoch 47: average Loss: 0.030892\n",
      "2025-04-23 14:30: **********Val Epoch 47: average Loss: 0.030892\n",
      "2025-04-23 14:30: Train Epoch 48: 0/37 Loss: 0.102550\n",
      "2025-04-23 14:30: Train Epoch 48: 0/37 Loss: 0.102550\n",
      "2025-04-23 14:30: Train Epoch 48: 0/37 Loss: 0.102550\n",
      "2025-04-23 14:30: Train Epoch 48: 0/37 Loss: 0.102550\n",
      "2025-04-23 14:30: Train Epoch 48: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:30: Train Epoch 48: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:30: Train Epoch 48: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:30: Train Epoch 48: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:30: **********Train Epoch 48: averaged Loss: 0.061350\n",
      "2025-04-23 14:30: **********Train Epoch 48: averaged Loss: 0.061350\n",
      "2025-04-23 14:30: **********Train Epoch 48: averaged Loss: 0.061350\n",
      "2025-04-23 14:30: **********Train Epoch 48: averaged Loss: 0.061350\n",
      "2025-04-23 14:30: **********Val Epoch 48: average Loss: 0.158414\n",
      "2025-04-23 14:30: **********Val Epoch 48: average Loss: 0.158414\n",
      "2025-04-23 14:30: **********Val Epoch 48: average Loss: 0.158414\n",
      "2025-04-23 14:30: **********Val Epoch 48: average Loss: 0.158414\n",
      "2025-04-23 14:30: Train Epoch 49: 0/37 Loss: 0.055196\n",
      "2025-04-23 14:30: Train Epoch 49: 0/37 Loss: 0.055196\n",
      "2025-04-23 14:30: Train Epoch 49: 0/37 Loss: 0.055196\n",
      "2025-04-23 14:30: Train Epoch 49: 0/37 Loss: 0.055196\n",
      "2025-04-23 14:30: Train Epoch 49: 20/37 Loss: 0.033150\n",
      "2025-04-23 14:30: Train Epoch 49: 20/37 Loss: 0.033150\n",
      "2025-04-23 14:30: Train Epoch 49: 20/37 Loss: 0.033150\n",
      "2025-04-23 14:30: Train Epoch 49: 20/37 Loss: 0.033150\n",
      "2025-04-23 14:30: **********Train Epoch 49: averaged Loss: 0.038187\n",
      "2025-04-23 14:30: **********Train Epoch 49: averaged Loss: 0.038187\n",
      "2025-04-23 14:30: **********Train Epoch 49: averaged Loss: 0.038187\n",
      "2025-04-23 14:30: **********Train Epoch 49: averaged Loss: 0.038187\n",
      "2025-04-23 14:30: **********Val Epoch 49: average Loss: 0.103002\n",
      "2025-04-23 14:30: **********Val Epoch 49: average Loss: 0.103002\n",
      "2025-04-23 14:30: **********Val Epoch 49: average Loss: 0.103002\n",
      "2025-04-23 14:30: **********Val Epoch 49: average Loss: 0.103002\n",
      "2025-04-23 14:30: Train Epoch 50: 0/37 Loss: 0.237535\n",
      "2025-04-23 14:30: Train Epoch 50: 0/37 Loss: 0.237535\n",
      "2025-04-23 14:30: Train Epoch 50: 0/37 Loss: 0.237535\n",
      "2025-04-23 14:30: Train Epoch 50: 0/37 Loss: 0.237535\n",
      "2025-04-23 14:30: Train Epoch 50: 20/37 Loss: 0.028046\n",
      "2025-04-23 14:30: Train Epoch 50: 20/37 Loss: 0.028046\n",
      "2025-04-23 14:30: Train Epoch 50: 20/37 Loss: 0.028046\n",
      "2025-04-23 14:30: Train Epoch 50: 20/37 Loss: 0.028046\n",
      "2025-04-23 14:30: **********Train Epoch 50: averaged Loss: 0.097835\n",
      "2025-04-23 14:30: **********Train Epoch 50: averaged Loss: 0.097835\n",
      "2025-04-23 14:30: **********Train Epoch 50: averaged Loss: 0.097835\n",
      "2025-04-23 14:30: **********Train Epoch 50: averaged Loss: 0.097835\n",
      "2025-04-23 14:30: **********Val Epoch 50: average Loss: 0.025871\n",
      "2025-04-23 14:30: **********Val Epoch 50: average Loss: 0.025871\n",
      "2025-04-23 14:30: **********Val Epoch 50: average Loss: 0.025871\n",
      "2025-04-23 14:30: **********Val Epoch 50: average Loss: 0.025871\n",
      "2025-04-23 14:30: Train Epoch 51: 0/37 Loss: 0.038418\n",
      "2025-04-23 14:30: Train Epoch 51: 0/37 Loss: 0.038418\n",
      "2025-04-23 14:30: Train Epoch 51: 0/37 Loss: 0.038418\n",
      "2025-04-23 14:30: Train Epoch 51: 0/37 Loss: 0.038418\n",
      "2025-04-23 14:30: Train Epoch 51: 20/37 Loss: 0.011678\n",
      "2025-04-23 14:30: Train Epoch 51: 20/37 Loss: 0.011678\n",
      "2025-04-23 14:30: Train Epoch 51: 20/37 Loss: 0.011678\n",
      "2025-04-23 14:30: Train Epoch 51: 20/37 Loss: 0.011678\n",
      "2025-04-23 14:30: **********Train Epoch 51: averaged Loss: 0.026617\n",
      "2025-04-23 14:30: **********Train Epoch 51: averaged Loss: 0.026617\n",
      "2025-04-23 14:30: **********Train Epoch 51: averaged Loss: 0.026617\n",
      "2025-04-23 14:30: **********Train Epoch 51: averaged Loss: 0.026617\n",
      "2025-04-23 14:30: **********Val Epoch 51: average Loss: 0.046180\n",
      "2025-04-23 14:30: **********Val Epoch 51: average Loss: 0.046180\n",
      "2025-04-23 14:30: **********Val Epoch 51: average Loss: 0.046180\n",
      "2025-04-23 14:30: **********Val Epoch 51: average Loss: 0.046180\n",
      "2025-04-23 14:30: Train Epoch 52: 0/37 Loss: 0.049657\n",
      "2025-04-23 14:30: Train Epoch 52: 0/37 Loss: 0.049657\n",
      "2025-04-23 14:30: Train Epoch 52: 0/37 Loss: 0.049657\n",
      "2025-04-23 14:30: Train Epoch 52: 0/37 Loss: 0.049657\n",
      "2025-04-23 14:30: Train Epoch 52: 20/37 Loss: 0.007014\n",
      "2025-04-23 14:30: Train Epoch 52: 20/37 Loss: 0.007014\n",
      "2025-04-23 14:30: Train Epoch 52: 20/37 Loss: 0.007014\n",
      "2025-04-23 14:30: Train Epoch 52: 20/37 Loss: 0.007014\n",
      "2025-04-23 14:30: **********Train Epoch 52: averaged Loss: 0.022293\n",
      "2025-04-23 14:30: **********Train Epoch 52: averaged Loss: 0.022293\n",
      "2025-04-23 14:30: **********Train Epoch 52: averaged Loss: 0.022293\n",
      "2025-04-23 14:30: **********Train Epoch 52: averaged Loss: 0.022293\n",
      "2025-04-23 14:30: **********Val Epoch 52: average Loss: 0.074929\n",
      "2025-04-23 14:30: **********Val Epoch 52: average Loss: 0.074929\n",
      "2025-04-23 14:30: **********Val Epoch 52: average Loss: 0.074929\n",
      "2025-04-23 14:30: **********Val Epoch 52: average Loss: 0.074929\n",
      "2025-04-23 14:30: Train Epoch 53: 0/37 Loss: 0.045907\n",
      "2025-04-23 14:30: Train Epoch 53: 0/37 Loss: 0.045907\n",
      "2025-04-23 14:30: Train Epoch 53: 0/37 Loss: 0.045907\n",
      "2025-04-23 14:30: Train Epoch 53: 0/37 Loss: 0.045907\n",
      "2025-04-23 14:31: Train Epoch 53: 20/37 Loss: 0.011465\n",
      "2025-04-23 14:31: Train Epoch 53: 20/37 Loss: 0.011465\n",
      "2025-04-23 14:31: Train Epoch 53: 20/37 Loss: 0.011465\n",
      "2025-04-23 14:31: Train Epoch 53: 20/37 Loss: 0.011465\n",
      "2025-04-23 14:31: **********Train Epoch 53: averaged Loss: 0.035579\n",
      "2025-04-23 14:31: **********Train Epoch 53: averaged Loss: 0.035579\n",
      "2025-04-23 14:31: **********Train Epoch 53: averaged Loss: 0.035579\n",
      "2025-04-23 14:31: **********Train Epoch 53: averaged Loss: 0.035579\n",
      "2025-04-23 14:31: **********Val Epoch 53: average Loss: 0.042131\n",
      "2025-04-23 14:31: **********Val Epoch 53: average Loss: 0.042131\n",
      "2025-04-23 14:31: **********Val Epoch 53: average Loss: 0.042131\n",
      "2025-04-23 14:31: **********Val Epoch 53: average Loss: 0.042131\n",
      "2025-04-23 14:31: Train Epoch 54: 0/37 Loss: 0.057021\n",
      "2025-04-23 14:31: Train Epoch 54: 0/37 Loss: 0.057021\n",
      "2025-04-23 14:31: Train Epoch 54: 0/37 Loss: 0.057021\n",
      "2025-04-23 14:31: Train Epoch 54: 0/37 Loss: 0.057021\n",
      "2025-04-23 14:31: Train Epoch 54: 20/37 Loss: 0.008623\n",
      "2025-04-23 14:31: Train Epoch 54: 20/37 Loss: 0.008623\n",
      "2025-04-23 14:31: Train Epoch 54: 20/37 Loss: 0.008623\n",
      "2025-04-23 14:31: Train Epoch 54: 20/37 Loss: 0.008623\n",
      "2025-04-23 14:31: **********Train Epoch 54: averaged Loss: 0.023304\n",
      "2025-04-23 14:31: **********Train Epoch 54: averaged Loss: 0.023304\n",
      "2025-04-23 14:31: **********Train Epoch 54: averaged Loss: 0.023304\n",
      "2025-04-23 14:31: **********Train Epoch 54: averaged Loss: 0.023304\n",
      "2025-04-23 14:31: **********Val Epoch 54: average Loss: 0.062572\n",
      "2025-04-23 14:31: **********Val Epoch 54: average Loss: 0.062572\n",
      "2025-04-23 14:31: **********Val Epoch 54: average Loss: 0.062572\n",
      "2025-04-23 14:31: **********Val Epoch 54: average Loss: 0.062572\n",
      "2025-04-23 14:31: Train Epoch 55: 0/37 Loss: 0.072497\n",
      "2025-04-23 14:31: Train Epoch 55: 0/37 Loss: 0.072497\n",
      "2025-04-23 14:31: Train Epoch 55: 0/37 Loss: 0.072497\n",
      "2025-04-23 14:31: Train Epoch 55: 0/37 Loss: 0.072497\n",
      "2025-04-23 14:31: Train Epoch 55: 20/37 Loss: 0.007614\n",
      "2025-04-23 14:31: Train Epoch 55: 20/37 Loss: 0.007614\n",
      "2025-04-23 14:31: Train Epoch 55: 20/37 Loss: 0.007614\n",
      "2025-04-23 14:31: Train Epoch 55: 20/37 Loss: 0.007614\n",
      "2025-04-23 14:31: **********Train Epoch 55: averaged Loss: 0.040646\n",
      "2025-04-23 14:31: **********Train Epoch 55: averaged Loss: 0.040646\n",
      "2025-04-23 14:31: **********Train Epoch 55: averaged Loss: 0.040646\n",
      "2025-04-23 14:31: **********Train Epoch 55: averaged Loss: 0.040646\n",
      "2025-04-23 14:31: **********Val Epoch 55: average Loss: 0.047854\n",
      "2025-04-23 14:31: **********Val Epoch 55: average Loss: 0.047854\n",
      "2025-04-23 14:31: **********Val Epoch 55: average Loss: 0.047854\n",
      "2025-04-23 14:31: **********Val Epoch 55: average Loss: 0.047854\n",
      "2025-04-23 14:31: Train Epoch 56: 0/37 Loss: 0.045121\n",
      "2025-04-23 14:31: Train Epoch 56: 0/37 Loss: 0.045121\n",
      "2025-04-23 14:31: Train Epoch 56: 0/37 Loss: 0.045121\n",
      "2025-04-23 14:31: Train Epoch 56: 0/37 Loss: 0.045121\n",
      "2025-04-23 14:31: Train Epoch 56: 20/37 Loss: 0.013384\n",
      "2025-04-23 14:31: Train Epoch 56: 20/37 Loss: 0.013384\n",
      "2025-04-23 14:31: Train Epoch 56: 20/37 Loss: 0.013384\n",
      "2025-04-23 14:31: Train Epoch 56: 20/37 Loss: 0.013384\n",
      "2025-04-23 14:31: **********Train Epoch 56: averaged Loss: 0.021174\n",
      "2025-04-23 14:31: **********Train Epoch 56: averaged Loss: 0.021174\n",
      "2025-04-23 14:31: **********Train Epoch 56: averaged Loss: 0.021174\n",
      "2025-04-23 14:31: **********Train Epoch 56: averaged Loss: 0.021174\n",
      "2025-04-23 14:31: **********Val Epoch 56: average Loss: 0.044876\n",
      "2025-04-23 14:31: **********Val Epoch 56: average Loss: 0.044876\n",
      "2025-04-23 14:31: **********Val Epoch 56: average Loss: 0.044876\n",
      "2025-04-23 14:31: **********Val Epoch 56: average Loss: 0.044876\n",
      "2025-04-23 14:31: Train Epoch 57: 0/37 Loss: 0.111732\n",
      "2025-04-23 14:31: Train Epoch 57: 0/37 Loss: 0.111732\n",
      "2025-04-23 14:31: Train Epoch 57: 0/37 Loss: 0.111732\n",
      "2025-04-23 14:31: Train Epoch 57: 0/37 Loss: 0.111732\n",
      "2025-04-23 14:31: Train Epoch 57: 20/37 Loss: 0.026359\n",
      "2025-04-23 14:31: Train Epoch 57: 20/37 Loss: 0.026359\n",
      "2025-04-23 14:31: Train Epoch 57: 20/37 Loss: 0.026359\n",
      "2025-04-23 14:31: Train Epoch 57: 20/37 Loss: 0.026359\n",
      "2025-04-23 14:31: **********Train Epoch 57: averaged Loss: 0.034982\n",
      "2025-04-23 14:31: **********Train Epoch 57: averaged Loss: 0.034982\n",
      "2025-04-23 14:31: **********Train Epoch 57: averaged Loss: 0.034982\n",
      "2025-04-23 14:31: **********Train Epoch 57: averaged Loss: 0.034982\n",
      "2025-04-23 14:31: **********Val Epoch 57: average Loss: 0.015194\n",
      "2025-04-23 14:31: **********Val Epoch 57: average Loss: 0.015194\n",
      "2025-04-23 14:31: **********Val Epoch 57: average Loss: 0.015194\n",
      "2025-04-23 14:31: **********Val Epoch 57: average Loss: 0.015194\n",
      "2025-04-23 14:31: Train Epoch 58: 0/37 Loss: 0.128934\n",
      "2025-04-23 14:31: Train Epoch 58: 0/37 Loss: 0.128934\n",
      "2025-04-23 14:31: Train Epoch 58: 0/37 Loss: 0.128934\n",
      "2025-04-23 14:31: Train Epoch 58: 0/37 Loss: 0.128934\n",
      "2025-04-23 14:31: Train Epoch 58: 20/37 Loss: 0.006997\n",
      "2025-04-23 14:31: Train Epoch 58: 20/37 Loss: 0.006997\n",
      "2025-04-23 14:31: Train Epoch 58: 20/37 Loss: 0.006997\n",
      "2025-04-23 14:31: Train Epoch 58: 20/37 Loss: 0.006997\n",
      "2025-04-23 14:31: **********Train Epoch 58: averaged Loss: 0.039727\n",
      "2025-04-23 14:31: **********Train Epoch 58: averaged Loss: 0.039727\n",
      "2025-04-23 14:31: **********Train Epoch 58: averaged Loss: 0.039727\n",
      "2025-04-23 14:31: **********Train Epoch 58: averaged Loss: 0.039727\n",
      "2025-04-23 14:31: **********Val Epoch 58: average Loss: 0.064006\n",
      "2025-04-23 14:31: **********Val Epoch 58: average Loss: 0.064006\n",
      "2025-04-23 14:31: **********Val Epoch 58: average Loss: 0.064006\n",
      "2025-04-23 14:31: **********Val Epoch 58: average Loss: 0.064006\n",
      "2025-04-23 14:31: Train Epoch 59: 0/37 Loss: 0.026481\n",
      "2025-04-23 14:31: Train Epoch 59: 0/37 Loss: 0.026481\n",
      "2025-04-23 14:31: Train Epoch 59: 0/37 Loss: 0.026481\n",
      "2025-04-23 14:31: Train Epoch 59: 0/37 Loss: 0.026481\n",
      "2025-04-23 14:31: Train Epoch 59: 20/37 Loss: 0.007033\n",
      "2025-04-23 14:31: Train Epoch 59: 20/37 Loss: 0.007033\n",
      "2025-04-23 14:31: Train Epoch 59: 20/37 Loss: 0.007033\n",
      "2025-04-23 14:31: Train Epoch 59: 20/37 Loss: 0.007033\n",
      "2025-04-23 14:31: **********Train Epoch 59: averaged Loss: 0.011852\n",
      "2025-04-23 14:31: **********Train Epoch 59: averaged Loss: 0.011852\n",
      "2025-04-23 14:31: **********Train Epoch 59: averaged Loss: 0.011852\n",
      "2025-04-23 14:31: **********Train Epoch 59: averaged Loss: 0.011852\n",
      "2025-04-23 14:31: **********Val Epoch 59: average Loss: 0.018090\n",
      "2025-04-23 14:31: **********Val Epoch 59: average Loss: 0.018090\n",
      "2025-04-23 14:31: **********Val Epoch 59: average Loss: 0.018090\n",
      "2025-04-23 14:31: **********Val Epoch 59: average Loss: 0.018090\n",
      "2025-04-23 14:31: Train Epoch 60: 0/37 Loss: 0.013279\n",
      "2025-04-23 14:31: Train Epoch 60: 0/37 Loss: 0.013279\n",
      "2025-04-23 14:31: Train Epoch 60: 0/37 Loss: 0.013279\n",
      "2025-04-23 14:31: Train Epoch 60: 0/37 Loss: 0.013279\n",
      "2025-04-23 14:31: Train Epoch 60: 20/37 Loss: 0.006148\n",
      "2025-04-23 14:31: Train Epoch 60: 20/37 Loss: 0.006148\n",
      "2025-04-23 14:31: Train Epoch 60: 20/37 Loss: 0.006148\n",
      "2025-04-23 14:31: Train Epoch 60: 20/37 Loss: 0.006148\n",
      "2025-04-23 14:31: **********Train Epoch 60: averaged Loss: 0.015429\n",
      "2025-04-23 14:31: **********Train Epoch 60: averaged Loss: 0.015429\n",
      "2025-04-23 14:31: **********Train Epoch 60: averaged Loss: 0.015429\n",
      "2025-04-23 14:31: **********Train Epoch 60: averaged Loss: 0.015429\n",
      "2025-04-23 14:31: **********Val Epoch 60: average Loss: 0.093454\n",
      "2025-04-23 14:31: **********Val Epoch 60: average Loss: 0.093454\n",
      "2025-04-23 14:31: **********Val Epoch 60: average Loss: 0.093454\n",
      "2025-04-23 14:31: **********Val Epoch 60: average Loss: 0.093454\n",
      "2025-04-23 14:31: Train Epoch 61: 0/37 Loss: 0.023542\n",
      "2025-04-23 14:31: Train Epoch 61: 0/37 Loss: 0.023542\n",
      "2025-04-23 14:31: Train Epoch 61: 0/37 Loss: 0.023542\n",
      "2025-04-23 14:31: Train Epoch 61: 0/37 Loss: 0.023542\n",
      "2025-04-23 14:31: Train Epoch 61: 20/37 Loss: 0.009101\n",
      "2025-04-23 14:31: Train Epoch 61: 20/37 Loss: 0.009101\n",
      "2025-04-23 14:31: Train Epoch 61: 20/37 Loss: 0.009101\n",
      "2025-04-23 14:31: Train Epoch 61: 20/37 Loss: 0.009101\n",
      "2025-04-23 14:31: **********Train Epoch 61: averaged Loss: 0.030629\n",
      "2025-04-23 14:31: **********Train Epoch 61: averaged Loss: 0.030629\n",
      "2025-04-23 14:31: **********Train Epoch 61: averaged Loss: 0.030629\n",
      "2025-04-23 14:31: **********Train Epoch 61: averaged Loss: 0.030629\n",
      "2025-04-23 14:31: **********Val Epoch 61: average Loss: 0.049321\n",
      "2025-04-23 14:31: **********Val Epoch 61: average Loss: 0.049321\n",
      "2025-04-23 14:31: **********Val Epoch 61: average Loss: 0.049321\n",
      "2025-04-23 14:31: **********Val Epoch 61: average Loss: 0.049321\n",
      "2025-04-23 14:31: Train Epoch 62: 0/37 Loss: 0.124872\n",
      "2025-04-23 14:31: Train Epoch 62: 0/37 Loss: 0.124872\n",
      "2025-04-23 14:31: Train Epoch 62: 0/37 Loss: 0.124872\n",
      "2025-04-23 14:31: Train Epoch 62: 0/37 Loss: 0.124872\n",
      "2025-04-23 14:31: Train Epoch 62: 20/37 Loss: 0.037540\n",
      "2025-04-23 14:31: Train Epoch 62: 20/37 Loss: 0.037540\n",
      "2025-04-23 14:31: Train Epoch 62: 20/37 Loss: 0.037540\n",
      "2025-04-23 14:31: Train Epoch 62: 20/37 Loss: 0.037540\n",
      "2025-04-23 14:31: **********Train Epoch 62: averaged Loss: 0.037372\n",
      "2025-04-23 14:31: **********Train Epoch 62: averaged Loss: 0.037372\n",
      "2025-04-23 14:31: **********Train Epoch 62: averaged Loss: 0.037372\n",
      "2025-04-23 14:31: **********Train Epoch 62: averaged Loss: 0.037372\n",
      "2025-04-23 14:31: **********Val Epoch 62: average Loss: 0.104982\n",
      "2025-04-23 14:31: **********Val Epoch 62: average Loss: 0.104982\n",
      "2025-04-23 14:31: **********Val Epoch 62: average Loss: 0.104982\n",
      "2025-04-23 14:31: **********Val Epoch 62: average Loss: 0.104982\n",
      "2025-04-23 14:31: Train Epoch 63: 0/37 Loss: 0.077699\n",
      "2025-04-23 14:31: Train Epoch 63: 0/37 Loss: 0.077699\n",
      "2025-04-23 14:31: Train Epoch 63: 0/37 Loss: 0.077699\n",
      "2025-04-23 14:31: Train Epoch 63: 0/37 Loss: 0.077699\n",
      "2025-04-23 14:31: Train Epoch 63: 20/37 Loss: 0.006803\n",
      "2025-04-23 14:31: Train Epoch 63: 20/37 Loss: 0.006803\n",
      "2025-04-23 14:31: Train Epoch 63: 20/37 Loss: 0.006803\n",
      "2025-04-23 14:31: Train Epoch 63: 20/37 Loss: 0.006803\n",
      "2025-04-23 14:31: **********Train Epoch 63: averaged Loss: 0.050051\n",
      "2025-04-23 14:31: **********Train Epoch 63: averaged Loss: 0.050051\n",
      "2025-04-23 14:31: **********Train Epoch 63: averaged Loss: 0.050051\n",
      "2025-04-23 14:31: **********Train Epoch 63: averaged Loss: 0.050051\n",
      "2025-04-23 14:31: **********Val Epoch 63: average Loss: 0.136828\n",
      "2025-04-23 14:31: **********Val Epoch 63: average Loss: 0.136828\n",
      "2025-04-23 14:31: **********Val Epoch 63: average Loss: 0.136828\n",
      "2025-04-23 14:31: **********Val Epoch 63: average Loss: 0.136828\n",
      "2025-04-23 14:31: Train Epoch 64: 0/37 Loss: 0.093037\n",
      "2025-04-23 14:31: Train Epoch 64: 0/37 Loss: 0.093037\n",
      "2025-04-23 14:31: Train Epoch 64: 0/37 Loss: 0.093037\n",
      "2025-04-23 14:31: Train Epoch 64: 0/37 Loss: 0.093037\n",
      "2025-04-23 14:31: Train Epoch 64: 20/37 Loss: 0.013867\n",
      "2025-04-23 14:31: Train Epoch 64: 20/37 Loss: 0.013867\n",
      "2025-04-23 14:31: Train Epoch 64: 20/37 Loss: 0.013867\n",
      "2025-04-23 14:31: Train Epoch 64: 20/37 Loss: 0.013867\n",
      "2025-04-23 14:31: **********Train Epoch 64: averaged Loss: 0.040151\n",
      "2025-04-23 14:31: **********Train Epoch 64: averaged Loss: 0.040151\n",
      "2025-04-23 14:31: **********Train Epoch 64: averaged Loss: 0.040151\n",
      "2025-04-23 14:31: **********Train Epoch 64: averaged Loss: 0.040151\n",
      "2025-04-23 14:31: **********Val Epoch 64: average Loss: 0.119237\n",
      "2025-04-23 14:31: **********Val Epoch 64: average Loss: 0.119237\n",
      "2025-04-23 14:31: **********Val Epoch 64: average Loss: 0.119237\n",
      "2025-04-23 14:31: **********Val Epoch 64: average Loss: 0.119237\n",
      "2025-04-23 14:31: Train Epoch 65: 0/37 Loss: 0.186162\n",
      "2025-04-23 14:31: Train Epoch 65: 0/37 Loss: 0.186162\n",
      "2025-04-23 14:31: Train Epoch 65: 0/37 Loss: 0.186162\n",
      "2025-04-23 14:31: Train Epoch 65: 0/37 Loss: 0.186162\n",
      "2025-04-23 14:31: Train Epoch 65: 20/37 Loss: 0.007222\n",
      "2025-04-23 14:31: Train Epoch 65: 20/37 Loss: 0.007222\n",
      "2025-04-23 14:31: Train Epoch 65: 20/37 Loss: 0.007222\n",
      "2025-04-23 14:31: Train Epoch 65: 20/37 Loss: 0.007222\n",
      "2025-04-23 14:31: **********Train Epoch 65: averaged Loss: 0.082213\n",
      "2025-04-23 14:31: **********Train Epoch 65: averaged Loss: 0.082213\n",
      "2025-04-23 14:31: **********Train Epoch 65: averaged Loss: 0.082213\n",
      "2025-04-23 14:31: **********Train Epoch 65: averaged Loss: 0.082213\n",
      "2025-04-23 14:31: **********Val Epoch 65: average Loss: 0.073609\n",
      "2025-04-23 14:31: **********Val Epoch 65: average Loss: 0.073609\n",
      "2025-04-23 14:31: **********Val Epoch 65: average Loss: 0.073609\n",
      "2025-04-23 14:31: **********Val Epoch 65: average Loss: 0.073609\n",
      "2025-04-23 14:31: Train Epoch 66: 0/37 Loss: 0.028862\n",
      "2025-04-23 14:31: Train Epoch 66: 0/37 Loss: 0.028862\n",
      "2025-04-23 14:31: Train Epoch 66: 0/37 Loss: 0.028862\n",
      "2025-04-23 14:31: Train Epoch 66: 0/37 Loss: 0.028862\n",
      "2025-04-23 14:31: Train Epoch 66: 20/37 Loss: 0.068925\n",
      "2025-04-23 14:31: Train Epoch 66: 20/37 Loss: 0.068925\n",
      "2025-04-23 14:31: Train Epoch 66: 20/37 Loss: 0.068925\n",
      "2025-04-23 14:31: Train Epoch 66: 20/37 Loss: 0.068925\n",
      "2025-04-23 14:31: **********Train Epoch 66: averaged Loss: 0.061231\n",
      "2025-04-23 14:31: **********Train Epoch 66: averaged Loss: 0.061231\n",
      "2025-04-23 14:31: **********Train Epoch 66: averaged Loss: 0.061231\n",
      "2025-04-23 14:31: **********Train Epoch 66: averaged Loss: 0.061231\n",
      "2025-04-23 14:31: **********Val Epoch 66: average Loss: 0.101297\n",
      "2025-04-23 14:31: **********Val Epoch 66: average Loss: 0.101297\n",
      "2025-04-23 14:31: **********Val Epoch 66: average Loss: 0.101297\n",
      "2025-04-23 14:31: **********Val Epoch 66: average Loss: 0.101297\n",
      "2025-04-23 14:31: Train Epoch 67: 0/37 Loss: 0.138444\n",
      "2025-04-23 14:31: Train Epoch 67: 0/37 Loss: 0.138444\n",
      "2025-04-23 14:31: Train Epoch 67: 0/37 Loss: 0.138444\n",
      "2025-04-23 14:31: Train Epoch 67: 0/37 Loss: 0.138444\n",
      "2025-04-23 14:31: Train Epoch 67: 20/37 Loss: 0.016923\n",
      "2025-04-23 14:31: Train Epoch 67: 20/37 Loss: 0.016923\n",
      "2025-04-23 14:31: Train Epoch 67: 20/37 Loss: 0.016923\n",
      "2025-04-23 14:31: Train Epoch 67: 20/37 Loss: 0.016923\n",
      "2025-04-23 14:31: **********Train Epoch 67: averaged Loss: 0.054265\n",
      "2025-04-23 14:31: **********Train Epoch 67: averaged Loss: 0.054265\n",
      "2025-04-23 14:31: **********Train Epoch 67: averaged Loss: 0.054265\n",
      "2025-04-23 14:31: **********Train Epoch 67: averaged Loss: 0.054265\n",
      "2025-04-23 14:31: **********Val Epoch 67: average Loss: 0.079491\n",
      "2025-04-23 14:31: **********Val Epoch 67: average Loss: 0.079491\n",
      "2025-04-23 14:31: **********Val Epoch 67: average Loss: 0.079491\n",
      "2025-04-23 14:31: **********Val Epoch 67: average Loss: 0.079491\n",
      "2025-04-23 14:31: Train Epoch 68: 0/37 Loss: 0.057564\n",
      "2025-04-23 14:31: Train Epoch 68: 0/37 Loss: 0.057564\n",
      "2025-04-23 14:31: Train Epoch 68: 0/37 Loss: 0.057564\n",
      "2025-04-23 14:31: Train Epoch 68: 0/37 Loss: 0.057564\n",
      "2025-04-23 14:31: Train Epoch 68: 20/37 Loss: 0.077472\n",
      "2025-04-23 14:31: Train Epoch 68: 20/37 Loss: 0.077472\n",
      "2025-04-23 14:31: Train Epoch 68: 20/37 Loss: 0.077472\n",
      "2025-04-23 14:31: Train Epoch 68: 20/37 Loss: 0.077472\n",
      "2025-04-23 14:31: **********Train Epoch 68: averaged Loss: 0.047150\n",
      "2025-04-23 14:31: **********Train Epoch 68: averaged Loss: 0.047150\n",
      "2025-04-23 14:31: **********Train Epoch 68: averaged Loss: 0.047150\n",
      "2025-04-23 14:31: **********Train Epoch 68: averaged Loss: 0.047150\n",
      "2025-04-23 14:31: **********Val Epoch 68: average Loss: 0.057381\n",
      "2025-04-23 14:31: **********Val Epoch 68: average Loss: 0.057381\n",
      "2025-04-23 14:31: **********Val Epoch 68: average Loss: 0.057381\n",
      "2025-04-23 14:31: **********Val Epoch 68: average Loss: 0.057381\n",
      "2025-04-23 14:31: Train Epoch 69: 0/37 Loss: 0.120862\n",
      "2025-04-23 14:31: Train Epoch 69: 0/37 Loss: 0.120862\n",
      "2025-04-23 14:31: Train Epoch 69: 0/37 Loss: 0.120862\n",
      "2025-04-23 14:31: Train Epoch 69: 0/37 Loss: 0.120862\n",
      "2025-04-23 14:31: Train Epoch 69: 20/37 Loss: 0.014630\n",
      "2025-04-23 14:31: Train Epoch 69: 20/37 Loss: 0.014630\n",
      "2025-04-23 14:31: Train Epoch 69: 20/37 Loss: 0.014630\n",
      "2025-04-23 14:31: Train Epoch 69: 20/37 Loss: 0.014630\n",
      "2025-04-23 14:31: **********Train Epoch 69: averaged Loss: 0.047289\n",
      "2025-04-23 14:31: **********Train Epoch 69: averaged Loss: 0.047289\n",
      "2025-04-23 14:31: **********Train Epoch 69: averaged Loss: 0.047289\n",
      "2025-04-23 14:31: **********Train Epoch 69: averaged Loss: 0.047289\n",
      "2025-04-23 14:31: **********Val Epoch 69: average Loss: 0.083767\n",
      "2025-04-23 14:31: **********Val Epoch 69: average Loss: 0.083767\n",
      "2025-04-23 14:31: **********Val Epoch 69: average Loss: 0.083767\n",
      "2025-04-23 14:31: **********Val Epoch 69: average Loss: 0.083767\n",
      "2025-04-23 14:31: Train Epoch 70: 0/37 Loss: 0.034468\n",
      "2025-04-23 14:31: Train Epoch 70: 0/37 Loss: 0.034468\n",
      "2025-04-23 14:31: Train Epoch 70: 0/37 Loss: 0.034468\n",
      "2025-04-23 14:31: Train Epoch 70: 0/37 Loss: 0.034468\n",
      "2025-04-23 14:31: Train Epoch 70: 20/37 Loss: 0.030489\n",
      "2025-04-23 14:31: Train Epoch 70: 20/37 Loss: 0.030489\n",
      "2025-04-23 14:31: Train Epoch 70: 20/37 Loss: 0.030489\n",
      "2025-04-23 14:31: Train Epoch 70: 20/37 Loss: 0.030489\n",
      "2025-04-23 14:31: **********Train Epoch 70: averaged Loss: 0.035375\n",
      "2025-04-23 14:31: **********Train Epoch 70: averaged Loss: 0.035375\n",
      "2025-04-23 14:31: **********Train Epoch 70: averaged Loss: 0.035375\n",
      "2025-04-23 14:31: **********Train Epoch 70: averaged Loss: 0.035375\n",
      "2025-04-23 14:31: **********Val Epoch 70: average Loss: 0.056042\n",
      "2025-04-23 14:31: **********Val Epoch 70: average Loss: 0.056042\n",
      "2025-04-23 14:31: **********Val Epoch 70: average Loss: 0.056042\n",
      "2025-04-23 14:31: **********Val Epoch 70: average Loss: 0.056042\n",
      "2025-04-23 14:31: Train Epoch 71: 0/37 Loss: 0.121756\n",
      "2025-04-23 14:31: Train Epoch 71: 0/37 Loss: 0.121756\n",
      "2025-04-23 14:31: Train Epoch 71: 0/37 Loss: 0.121756\n",
      "2025-04-23 14:31: Train Epoch 71: 0/37 Loss: 0.121756\n",
      "2025-04-23 14:31: Train Epoch 71: 20/37 Loss: 0.018007\n",
      "2025-04-23 14:31: Train Epoch 71: 20/37 Loss: 0.018007\n",
      "2025-04-23 14:31: Train Epoch 71: 20/37 Loss: 0.018007\n",
      "2025-04-23 14:31: Train Epoch 71: 20/37 Loss: 0.018007\n",
      "2025-04-23 14:31: **********Train Epoch 71: averaged Loss: 0.036181\n",
      "2025-04-23 14:31: **********Train Epoch 71: averaged Loss: 0.036181\n",
      "2025-04-23 14:31: **********Train Epoch 71: averaged Loss: 0.036181\n",
      "2025-04-23 14:31: **********Train Epoch 71: averaged Loss: 0.036181\n",
      "2025-04-23 14:31: **********Val Epoch 71: average Loss: 0.026141\n",
      "2025-04-23 14:31: **********Val Epoch 71: average Loss: 0.026141\n",
      "2025-04-23 14:31: **********Val Epoch 71: average Loss: 0.026141\n",
      "2025-04-23 14:31: **********Val Epoch 71: average Loss: 0.026141\n",
      "2025-04-23 14:31: Train Epoch 72: 0/37 Loss: 0.050684\n",
      "2025-04-23 14:31: Train Epoch 72: 0/37 Loss: 0.050684\n",
      "2025-04-23 14:31: Train Epoch 72: 0/37 Loss: 0.050684\n",
      "2025-04-23 14:31: Train Epoch 72: 0/37 Loss: 0.050684\n",
      "2025-04-23 14:31: Train Epoch 72: 20/37 Loss: 0.014714\n",
      "2025-04-23 14:31: Train Epoch 72: 20/37 Loss: 0.014714\n",
      "2025-04-23 14:31: Train Epoch 72: 20/37 Loss: 0.014714\n",
      "2025-04-23 14:31: Train Epoch 72: 20/37 Loss: 0.014714\n",
      "2025-04-23 14:31: **********Train Epoch 72: averaged Loss: 0.041314\n",
      "2025-04-23 14:31: **********Train Epoch 72: averaged Loss: 0.041314\n",
      "2025-04-23 14:31: **********Train Epoch 72: averaged Loss: 0.041314\n",
      "2025-04-23 14:31: **********Train Epoch 72: averaged Loss: 0.041314\n",
      "2025-04-23 14:31: **********Val Epoch 72: average Loss: 0.054396\n",
      "2025-04-23 14:31: **********Val Epoch 72: average Loss: 0.054396\n",
      "2025-04-23 14:31: **********Val Epoch 72: average Loss: 0.054396\n",
      "2025-04-23 14:31: **********Val Epoch 72: average Loss: 0.054396\n",
      "2025-04-23 14:31: Train Epoch 73: 0/37 Loss: 0.091802\n",
      "2025-04-23 14:31: Train Epoch 73: 0/37 Loss: 0.091802\n",
      "2025-04-23 14:31: Train Epoch 73: 0/37 Loss: 0.091802\n",
      "2025-04-23 14:31: Train Epoch 73: 0/37 Loss: 0.091802\n",
      "2025-04-23 14:31: Train Epoch 73: 20/37 Loss: 0.037818\n",
      "2025-04-23 14:31: Train Epoch 73: 20/37 Loss: 0.037818\n",
      "2025-04-23 14:31: Train Epoch 73: 20/37 Loss: 0.037818\n",
      "2025-04-23 14:31: Train Epoch 73: 20/37 Loss: 0.037818\n",
      "2025-04-23 14:31: **********Train Epoch 73: averaged Loss: 0.044294\n",
      "2025-04-23 14:31: **********Train Epoch 73: averaged Loss: 0.044294\n",
      "2025-04-23 14:31: **********Train Epoch 73: averaged Loss: 0.044294\n",
      "2025-04-23 14:31: **********Train Epoch 73: averaged Loss: 0.044294\n",
      "2025-04-23 14:31: **********Val Epoch 73: average Loss: 0.045249\n",
      "2025-04-23 14:31: **********Val Epoch 73: average Loss: 0.045249\n",
      "2025-04-23 14:31: **********Val Epoch 73: average Loss: 0.045249\n",
      "2025-04-23 14:31: **********Val Epoch 73: average Loss: 0.045249\n",
      "2025-04-23 14:31: Train Epoch 74: 0/37 Loss: 0.114564\n",
      "2025-04-23 14:31: Train Epoch 74: 0/37 Loss: 0.114564\n",
      "2025-04-23 14:31: Train Epoch 74: 0/37 Loss: 0.114564\n",
      "2025-04-23 14:31: Train Epoch 74: 0/37 Loss: 0.114564\n",
      "2025-04-23 14:31: Train Epoch 74: 20/37 Loss: 0.006068\n",
      "2025-04-23 14:31: Train Epoch 74: 20/37 Loss: 0.006068\n",
      "2025-04-23 14:31: Train Epoch 74: 20/37 Loss: 0.006068\n",
      "2025-04-23 14:31: Train Epoch 74: 20/37 Loss: 0.006068\n",
      "2025-04-23 14:31: **********Train Epoch 74: averaged Loss: 0.039483\n",
      "2025-04-23 14:31: **********Train Epoch 74: averaged Loss: 0.039483\n",
      "2025-04-23 14:31: **********Train Epoch 74: averaged Loss: 0.039483\n",
      "2025-04-23 14:31: **********Train Epoch 74: averaged Loss: 0.039483\n",
      "2025-04-23 14:31: **********Val Epoch 74: average Loss: 0.150723\n",
      "2025-04-23 14:31: **********Val Epoch 74: average Loss: 0.150723\n",
      "2025-04-23 14:31: **********Val Epoch 74: average Loss: 0.150723\n",
      "2025-04-23 14:31: **********Val Epoch 74: average Loss: 0.150723\n",
      "2025-04-23 14:31: Train Epoch 75: 0/37 Loss: 0.050892\n",
      "2025-04-23 14:31: Train Epoch 75: 0/37 Loss: 0.050892\n",
      "2025-04-23 14:31: Train Epoch 75: 0/37 Loss: 0.050892\n",
      "2025-04-23 14:31: Train Epoch 75: 0/37 Loss: 0.050892\n",
      "2025-04-23 14:31: Train Epoch 75: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:31: Train Epoch 75: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:31: Train Epoch 75: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:31: Train Epoch 75: 20/37 Loss: 0.011035\n",
      "2025-04-23 14:31: **********Train Epoch 75: averaged Loss: 0.061958\n",
      "2025-04-23 14:31: **********Train Epoch 75: averaged Loss: 0.061958\n",
      "2025-04-23 14:31: **********Train Epoch 75: averaged Loss: 0.061958\n",
      "2025-04-23 14:31: **********Train Epoch 75: averaged Loss: 0.061958\n",
      "2025-04-23 14:31: **********Val Epoch 75: average Loss: 0.233023\n",
      "2025-04-23 14:31: **********Val Epoch 75: average Loss: 0.233023\n",
      "2025-04-23 14:31: **********Val Epoch 75: average Loss: 0.233023\n",
      "2025-04-23 14:31: **********Val Epoch 75: average Loss: 0.233023\n",
      "2025-04-23 14:31: Train Epoch 76: 0/37 Loss: 0.113409\n",
      "2025-04-23 14:31: Train Epoch 76: 0/37 Loss: 0.113409\n",
      "2025-04-23 14:31: Train Epoch 76: 0/37 Loss: 0.113409\n",
      "2025-04-23 14:31: Train Epoch 76: 0/37 Loss: 0.113409\n",
      "2025-04-23 14:31: Train Epoch 76: 20/37 Loss: 0.045467\n",
      "2025-04-23 14:31: Train Epoch 76: 20/37 Loss: 0.045467\n",
      "2025-04-23 14:31: Train Epoch 76: 20/37 Loss: 0.045467\n",
      "2025-04-23 14:31: Train Epoch 76: 20/37 Loss: 0.045467\n",
      "2025-04-23 14:31: **********Train Epoch 76: averaged Loss: 0.084592\n",
      "2025-04-23 14:31: **********Train Epoch 76: averaged Loss: 0.084592\n",
      "2025-04-23 14:31: **********Train Epoch 76: averaged Loss: 0.084592\n",
      "2025-04-23 14:31: **********Train Epoch 76: averaged Loss: 0.084592\n",
      "2025-04-23 14:31: **********Val Epoch 76: average Loss: 0.134948\n",
      "2025-04-23 14:31: **********Val Epoch 76: average Loss: 0.134948\n",
      "2025-04-23 14:31: **********Val Epoch 76: average Loss: 0.134948\n",
      "2025-04-23 14:31: **********Val Epoch 76: average Loss: 0.134948\n",
      "2025-04-23 14:31: Train Epoch 77: 0/37 Loss: 0.119963\n",
      "2025-04-23 14:31: Train Epoch 77: 0/37 Loss: 0.119963\n",
      "2025-04-23 14:31: Train Epoch 77: 0/37 Loss: 0.119963\n",
      "2025-04-23 14:31: Train Epoch 77: 0/37 Loss: 0.119963\n",
      "2025-04-23 14:31: Train Epoch 77: 20/37 Loss: 0.080080\n",
      "2025-04-23 14:31: Train Epoch 77: 20/37 Loss: 0.080080\n",
      "2025-04-23 14:31: Train Epoch 77: 20/37 Loss: 0.080080\n",
      "2025-04-23 14:31: Train Epoch 77: 20/37 Loss: 0.080080\n",
      "2025-04-23 14:31: **********Train Epoch 77: averaged Loss: 0.071098\n",
      "2025-04-23 14:31: **********Train Epoch 77: averaged Loss: 0.071098\n",
      "2025-04-23 14:31: **********Train Epoch 77: averaged Loss: 0.071098\n",
      "2025-04-23 14:31: **********Train Epoch 77: averaged Loss: 0.071098\n",
      "2025-04-23 14:31: **********Val Epoch 77: average Loss: 0.055303\n",
      "2025-04-23 14:31: **********Val Epoch 77: average Loss: 0.055303\n",
      "2025-04-23 14:31: **********Val Epoch 77: average Loss: 0.055303\n",
      "2025-04-23 14:31: **********Val Epoch 77: average Loss: 0.055303\n",
      "2025-04-23 14:31: Train Epoch 78: 0/37 Loss: 0.041047\n",
      "2025-04-23 14:31: Train Epoch 78: 0/37 Loss: 0.041047\n",
      "2025-04-23 14:31: Train Epoch 78: 0/37 Loss: 0.041047\n",
      "2025-04-23 14:31: Train Epoch 78: 0/37 Loss: 0.041047\n",
      "2025-04-23 14:31: Train Epoch 78: 20/37 Loss: 0.125572\n",
      "2025-04-23 14:31: Train Epoch 78: 20/37 Loss: 0.125572\n",
      "2025-04-23 14:31: Train Epoch 78: 20/37 Loss: 0.125572\n",
      "2025-04-23 14:31: Train Epoch 78: 20/37 Loss: 0.125572\n",
      "2025-04-23 14:31: **********Train Epoch 78: averaged Loss: 0.057675\n",
      "2025-04-23 14:31: **********Train Epoch 78: averaged Loss: 0.057675\n",
      "2025-04-23 14:31: **********Train Epoch 78: averaged Loss: 0.057675\n",
      "2025-04-23 14:31: **********Train Epoch 78: averaged Loss: 0.057675\n",
      "2025-04-23 14:31: **********Val Epoch 78: average Loss: 0.024288\n",
      "2025-04-23 14:31: **********Val Epoch 78: average Loss: 0.024288\n",
      "2025-04-23 14:31: **********Val Epoch 78: average Loss: 0.024288\n",
      "2025-04-23 14:31: **********Val Epoch 78: average Loss: 0.024288\n",
      "2025-04-23 14:31: Train Epoch 79: 0/37 Loss: 0.214252\n",
      "2025-04-23 14:31: Train Epoch 79: 0/37 Loss: 0.214252\n",
      "2025-04-23 14:31: Train Epoch 79: 0/37 Loss: 0.214252\n",
      "2025-04-23 14:31: Train Epoch 79: 0/37 Loss: 0.214252\n",
      "2025-04-23 14:31: Train Epoch 79: 20/37 Loss: 0.031330\n",
      "2025-04-23 14:31: Train Epoch 79: 20/37 Loss: 0.031330\n",
      "2025-04-23 14:31: Train Epoch 79: 20/37 Loss: 0.031330\n",
      "2025-04-23 14:31: Train Epoch 79: 20/37 Loss: 0.031330\n",
      "2025-04-23 14:31: **********Train Epoch 79: averaged Loss: 0.072226\n",
      "2025-04-23 14:31: **********Train Epoch 79: averaged Loss: 0.072226\n",
      "2025-04-23 14:31: **********Train Epoch 79: averaged Loss: 0.072226\n",
      "2025-04-23 14:31: **********Train Epoch 79: averaged Loss: 0.072226\n",
      "2025-04-23 14:31: **********Val Epoch 79: average Loss: 0.100605\n",
      "2025-04-23 14:31: **********Val Epoch 79: average Loss: 0.100605\n",
      "2025-04-23 14:31: **********Val Epoch 79: average Loss: 0.100605\n",
      "2025-04-23 14:31: **********Val Epoch 79: average Loss: 0.100605\n",
      "2025-04-23 14:31: Train Epoch 80: 0/37 Loss: 0.133518\n",
      "2025-04-23 14:31: Train Epoch 80: 0/37 Loss: 0.133518\n",
      "2025-04-23 14:31: Train Epoch 80: 0/37 Loss: 0.133518\n",
      "2025-04-23 14:31: Train Epoch 80: 0/37 Loss: 0.133518\n",
      "2025-04-23 14:31: Train Epoch 80: 20/37 Loss: 0.035006\n",
      "2025-04-23 14:31: Train Epoch 80: 20/37 Loss: 0.035006\n",
      "2025-04-23 14:31: Train Epoch 80: 20/37 Loss: 0.035006\n",
      "2025-04-23 14:31: Train Epoch 80: 20/37 Loss: 0.035006\n",
      "2025-04-23 14:31: **********Train Epoch 80: averaged Loss: 0.058743\n",
      "2025-04-23 14:31: **********Train Epoch 80: averaged Loss: 0.058743\n",
      "2025-04-23 14:31: **********Train Epoch 80: averaged Loss: 0.058743\n",
      "2025-04-23 14:31: **********Train Epoch 80: averaged Loss: 0.058743\n",
      "2025-04-23 14:31: **********Val Epoch 80: average Loss: 0.095169\n",
      "2025-04-23 14:31: **********Val Epoch 80: average Loss: 0.095169\n",
      "2025-04-23 14:31: **********Val Epoch 80: average Loss: 0.095169\n",
      "2025-04-23 14:31: **********Val Epoch 80: average Loss: 0.095169\n",
      "2025-04-23 14:31: Train Epoch 81: 0/37 Loss: 0.063350\n",
      "2025-04-23 14:31: Train Epoch 81: 0/37 Loss: 0.063350\n",
      "2025-04-23 14:31: Train Epoch 81: 0/37 Loss: 0.063350\n",
      "2025-04-23 14:31: Train Epoch 81: 0/37 Loss: 0.063350\n",
      "2025-04-23 14:31: Train Epoch 81: 20/37 Loss: 0.145866\n",
      "2025-04-23 14:31: Train Epoch 81: 20/37 Loss: 0.145866\n",
      "2025-04-23 14:31: Train Epoch 81: 20/37 Loss: 0.145866\n",
      "2025-04-23 14:31: Train Epoch 81: 20/37 Loss: 0.145866\n",
      "2025-04-23 14:31: **********Train Epoch 81: averaged Loss: 0.073783\n",
      "2025-04-23 14:31: **********Train Epoch 81: averaged Loss: 0.073783\n",
      "2025-04-23 14:31: **********Train Epoch 81: averaged Loss: 0.073783\n",
      "2025-04-23 14:31: **********Train Epoch 81: averaged Loss: 0.073783\n",
      "2025-04-23 14:31: **********Val Epoch 81: average Loss: 0.085726\n",
      "2025-04-23 14:31: **********Val Epoch 81: average Loss: 0.085726\n",
      "2025-04-23 14:31: **********Val Epoch 81: average Loss: 0.085726\n",
      "2025-04-23 14:31: **********Val Epoch 81: average Loss: 0.085726\n",
      "2025-04-23 14:31: Train Epoch 82: 0/37 Loss: 0.131539\n",
      "2025-04-23 14:31: Train Epoch 82: 0/37 Loss: 0.131539\n",
      "2025-04-23 14:31: Train Epoch 82: 0/37 Loss: 0.131539\n",
      "2025-04-23 14:31: Train Epoch 82: 0/37 Loss: 0.131539\n",
      "2025-04-23 14:31: Train Epoch 82: 20/37 Loss: 0.027080\n",
      "2025-04-23 14:31: Train Epoch 82: 20/37 Loss: 0.027080\n",
      "2025-04-23 14:31: Train Epoch 82: 20/37 Loss: 0.027080\n",
      "2025-04-23 14:31: Train Epoch 82: 20/37 Loss: 0.027080\n",
      "2025-04-23 14:31: **********Train Epoch 82: averaged Loss: 0.054521\n",
      "2025-04-23 14:31: **********Train Epoch 82: averaged Loss: 0.054521\n",
      "2025-04-23 14:31: **********Train Epoch 82: averaged Loss: 0.054521\n",
      "2025-04-23 14:31: **********Train Epoch 82: averaged Loss: 0.054521\n",
      "2025-04-23 14:31: **********Val Epoch 82: average Loss: 0.062012\n",
      "2025-04-23 14:31: **********Val Epoch 82: average Loss: 0.062012\n",
      "2025-04-23 14:31: **********Val Epoch 82: average Loss: 0.062012\n",
      "2025-04-23 14:31: **********Val Epoch 82: average Loss: 0.062012\n",
      "2025-04-23 14:31: Train Epoch 83: 0/37 Loss: 0.108379\n",
      "2025-04-23 14:31: Train Epoch 83: 0/37 Loss: 0.108379\n",
      "2025-04-23 14:31: Train Epoch 83: 0/37 Loss: 0.108379\n",
      "2025-04-23 14:31: Train Epoch 83: 0/37 Loss: 0.108379\n",
      "2025-04-23 14:31: Train Epoch 83: 20/37 Loss: 0.006613\n",
      "2025-04-23 14:31: Train Epoch 83: 20/37 Loss: 0.006613\n",
      "2025-04-23 14:31: Train Epoch 83: 20/37 Loss: 0.006613\n",
      "2025-04-23 14:31: Train Epoch 83: 20/37 Loss: 0.006613\n",
      "2025-04-23 14:31: **********Train Epoch 83: averaged Loss: 0.033474\n",
      "2025-04-23 14:31: **********Train Epoch 83: averaged Loss: 0.033474\n",
      "2025-04-23 14:31: **********Train Epoch 83: averaged Loss: 0.033474\n",
      "2025-04-23 14:31: **********Train Epoch 83: averaged Loss: 0.033474\n",
      "2025-04-23 14:31: **********Val Epoch 83: average Loss: 0.073018\n",
      "2025-04-23 14:31: **********Val Epoch 83: average Loss: 0.073018\n",
      "2025-04-23 14:31: **********Val Epoch 83: average Loss: 0.073018\n",
      "2025-04-23 14:31: **********Val Epoch 83: average Loss: 0.073018\n",
      "2025-04-23 14:31: Train Epoch 84: 0/37 Loss: 0.105842\n",
      "2025-04-23 14:31: Train Epoch 84: 0/37 Loss: 0.105842\n",
      "2025-04-23 14:31: Train Epoch 84: 0/37 Loss: 0.105842\n",
      "2025-04-23 14:31: Train Epoch 84: 0/37 Loss: 0.105842\n",
      "2025-04-23 14:31: Train Epoch 84: 20/37 Loss: 0.007239\n",
      "2025-04-23 14:31: Train Epoch 84: 20/37 Loss: 0.007239\n",
      "2025-04-23 14:31: Train Epoch 84: 20/37 Loss: 0.007239\n",
      "2025-04-23 14:31: Train Epoch 84: 20/37 Loss: 0.007239\n",
      "2025-04-23 14:31: **********Train Epoch 84: averaged Loss: 0.039006\n",
      "2025-04-23 14:31: **********Train Epoch 84: averaged Loss: 0.039006\n",
      "2025-04-23 14:31: **********Train Epoch 84: averaged Loss: 0.039006\n",
      "2025-04-23 14:31: **********Train Epoch 84: averaged Loss: 0.039006\n",
      "2025-04-23 14:31: **********Val Epoch 84: average Loss: 0.096209\n",
      "2025-04-23 14:31: **********Val Epoch 84: average Loss: 0.096209\n",
      "2025-04-23 14:31: **********Val Epoch 84: average Loss: 0.096209\n",
      "2025-04-23 14:31: **********Val Epoch 84: average Loss: 0.096209\n",
      "2025-04-23 14:31: Train Epoch 85: 0/37 Loss: 0.056739\n",
      "2025-04-23 14:31: Train Epoch 85: 0/37 Loss: 0.056739\n",
      "2025-04-23 14:31: Train Epoch 85: 0/37 Loss: 0.056739\n",
      "2025-04-23 14:31: Train Epoch 85: 0/37 Loss: 0.056739\n",
      "2025-04-23 14:31: Train Epoch 85: 20/37 Loss: 0.006747\n",
      "2025-04-23 14:31: Train Epoch 85: 20/37 Loss: 0.006747\n",
      "2025-04-23 14:31: Train Epoch 85: 20/37 Loss: 0.006747\n",
      "2025-04-23 14:31: Train Epoch 85: 20/37 Loss: 0.006747\n",
      "2025-04-23 14:31: **********Train Epoch 85: averaged Loss: 0.034613\n",
      "2025-04-23 14:31: **********Train Epoch 85: averaged Loss: 0.034613\n",
      "2025-04-23 14:31: **********Train Epoch 85: averaged Loss: 0.034613\n",
      "2025-04-23 14:31: **********Train Epoch 85: averaged Loss: 0.034613\n",
      "2025-04-23 14:31: **********Val Epoch 85: average Loss: 0.089653\n",
      "2025-04-23 14:31: **********Val Epoch 85: average Loss: 0.089653\n",
      "2025-04-23 14:31: **********Val Epoch 85: average Loss: 0.089653\n",
      "2025-04-23 14:31: **********Val Epoch 85: average Loss: 0.089653\n",
      "2025-04-23 14:31: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:31: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:31: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:31: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:31: Total training time: 2.3794min, best loss: 0.010652\n",
      "2025-04-23 14:31: Total training time: 2.3794min, best loss: 0.010652\n",
      "2025-04-23 14:31: Total training time: 2.3794min, best loss: 0.010652\n",
      "2025-04-23 14:31: Total training time: 2.3794min, best loss: 0.010652\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1098, MSE: 0.1105\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1098, MSE: 0.1105\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1098, MSE: 0.1105\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1098, MSE: 0.1105\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1119, MSE: 0.1124\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1119, MSE: 0.1124\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1119, MSE: 0.1124\n",
      "2025-04-23 14:31: Average Horizon, MAE: 0.1119, MSE: 0.1124\n",
      "2025-04-23 14:31: Experiment log path in: ./\n",
      "2025-04-23 14:31: Experiment log path in: ./\n",
      "2025-04-23 14:31: Experiment log path in: ./\n",
      "2025-04-23 14:31: Experiment log path in: ./\n",
      "2025-04-23 14:31: Experiment log path in: ./\n",
      "2025-04-23 14:31: Train Epoch 1: 0/37 Loss: 0.013737\n",
      "2025-04-23 14:31: Train Epoch 1: 0/37 Loss: 0.013737\n",
      "2025-04-23 14:31: Train Epoch 1: 0/37 Loss: 0.013737\n",
      "2025-04-23 14:31: Train Epoch 1: 0/37 Loss: 0.013737\n",
      "2025-04-23 14:31: Train Epoch 1: 0/37 Loss: 0.013737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Model Parameter*****************\n",
      "gamma torch.Size([]) True\n",
      "adj torch.Size([82, 10]) True\n",
      "mam1.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers.2.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.0.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.0.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.0.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.0.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.0.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.0.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.0.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.0.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.0.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.0.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.0.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.0.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.1.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.1.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.1.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.1.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.1.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.1.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.1.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.1.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.1.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.1.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.1.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.1.norm.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.A_log torch.Size([64, 128]) True\n",
      "mam1.layers2.2.mixer.D torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.embedding.weight torch.Size([32, 82]) True\n",
      "mam1.layers2.2.mixer.embedding.bias torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.in_proj.weight torch.Size([128, 32]) True\n",
      "mam1.layers2.2.mixer.in_proj_r.weight torch.Size([64, 32]) True\n",
      "mam1.layers2.2.mixer.conv1d.weight torch.Size([64, 1, 3]) True\n",
      "mam1.layers2.2.mixer.conv1d.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.x_proj.weight torch.Size([258, 64]) True\n",
      "mam1.layers2.2.mixer.norm_f.weight torch.Size([32]) True\n",
      "mam1.layers2.2.mixer.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.layers2.2.mixer.dt_proj.weight torch.Size([64, 2]) True\n",
      "mam1.layers2.2.mixer.dt_proj.bias torch.Size([64]) True\n",
      "mam1.layers2.2.mixer.out_proj.weight torch.Size([32, 64]) True\n",
      "mam1.layers2.2.norm.weight torch.Size([32]) True\n",
      "mam1.layers2.2.norm.bias torch.Size([32]) True\n",
      "mam1.lin.0.0.weight torch.Size([5]) True\n",
      "mam1.lin.0.0.bias torch.Size([5]) True\n",
      "mam1.lin.0.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.0.1.bias torch.Size([32]) True\n",
      "mam1.lin.0.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.0.3.bias torch.Size([5]) True\n",
      "mam1.lin.1.0.weight torch.Size([5]) True\n",
      "mam1.lin.1.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.1.1.bias torch.Size([32]) True\n",
      "mam1.lin.1.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.1.3.bias torch.Size([5]) True\n",
      "mam1.lin.2.0.weight torch.Size([5]) True\n",
      "mam1.lin.2.1.weight torch.Size([32, 5]) True\n",
      "mam1.lin.2.1.bias torch.Size([32]) True\n",
      "mam1.lin.2.3.weight torch.Size([5, 32]) True\n",
      "mam1.lin.2.3.bias torch.Size([5]) True\n",
      "mam1.norm_f.weight torch.Size([32]) True\n",
      "mam1.norm_f.bias torch.Size([32]) True\n",
      "mam1.lm_head.weight torch.Size([82, 32]) True\n",
      "mam1.lm_head.bias torch.Size([82]) True\n",
      "mam1.proj.0.weight torch.Size([32, 5]) True\n",
      "mam1.proj.0.bias torch.Size([32]) True\n",
      "mam1.proj.2.weight torch.Size([5, 32]) True\n",
      "mam1.proj.2.bias torch.Size([5]) True\n",
      "mam1.nnl.weight torch.Size([82]) True\n",
      "mam1.nnl.bias torch.Size([82]) True\n",
      "graphsage.fc.weight torch.Size([1, 10]) True\n",
      "graphsage.fc.bias torch.Size([1]) True\n",
      "proj.weight torch.Size([1, 82]) True\n",
      "proj.bias torch.Size([1]) True\n",
      "proj_seq.weight torch.Size([1, 5]) True\n",
      "proj_seq.bias torch.Size([1]) True\n",
      "Total params num: 240663\n",
      "*****************Finish Parameter****************\n",
      "Applying learning rate decay.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 14:31: Train Epoch 1: 20/37 Loss: 0.178309\n",
      "2025-04-23 14:31: Train Epoch 1: 20/37 Loss: 0.178309\n",
      "2025-04-23 14:31: Train Epoch 1: 20/37 Loss: 0.178309\n",
      "2025-04-23 14:31: Train Epoch 1: 20/37 Loss: 0.178309\n",
      "2025-04-23 14:31: Train Epoch 1: 20/37 Loss: 0.178309\n",
      "2025-04-23 14:31: **********Train Epoch 1: averaged Loss: 0.113142\n",
      "2025-04-23 14:31: **********Train Epoch 1: averaged Loss: 0.113142\n",
      "2025-04-23 14:31: **********Train Epoch 1: averaged Loss: 0.113142\n",
      "2025-04-23 14:31: **********Train Epoch 1: averaged Loss: 0.113142\n",
      "2025-04-23 14:31: **********Train Epoch 1: averaged Loss: 0.113142\n",
      "2025-04-23 14:31: **********Val Epoch 1: average Loss: 0.057631\n",
      "2025-04-23 14:31: **********Val Epoch 1: average Loss: 0.057631\n",
      "2025-04-23 14:31: **********Val Epoch 1: average Loss: 0.057631\n",
      "2025-04-23 14:31: **********Val Epoch 1: average Loss: 0.057631\n",
      "2025-04-23 14:31: **********Val Epoch 1: average Loss: 0.057631\n",
      "2025-04-23 14:31: *********************************Current best model saved!\n",
      "2025-04-23 14:31: *********************************Current best model saved!\n",
      "2025-04-23 14:31: *********************************Current best model saved!\n",
      "2025-04-23 14:31: *********************************Current best model saved!\n",
      "2025-04-23 14:31: *********************************Current best model saved!\n",
      "2025-04-23 14:31: Train Epoch 2: 0/37 Loss: 0.631023\n",
      "2025-04-23 14:31: Train Epoch 2: 0/37 Loss: 0.631023\n",
      "2025-04-23 14:31: Train Epoch 2: 0/37 Loss: 0.631023\n",
      "2025-04-23 14:31: Train Epoch 2: 0/37 Loss: 0.631023\n",
      "2025-04-23 14:31: Train Epoch 2: 0/37 Loss: 0.631023\n",
      "2025-04-23 14:31: Train Epoch 2: 20/37 Loss: 0.123440\n",
      "2025-04-23 14:31: Train Epoch 2: 20/37 Loss: 0.123440\n",
      "2025-04-23 14:31: Train Epoch 2: 20/37 Loss: 0.123440\n",
      "2025-04-23 14:31: Train Epoch 2: 20/37 Loss: 0.123440\n",
      "2025-04-23 14:31: Train Epoch 2: 20/37 Loss: 0.123440\n",
      "2025-04-23 14:31: **********Train Epoch 2: averaged Loss: 0.273305\n",
      "2025-04-23 14:31: **********Train Epoch 2: averaged Loss: 0.273305\n",
      "2025-04-23 14:31: **********Train Epoch 2: averaged Loss: 0.273305\n",
      "2025-04-23 14:31: **********Train Epoch 2: averaged Loss: 0.273305\n",
      "2025-04-23 14:31: **********Train Epoch 2: averaged Loss: 0.273305\n",
      "2025-04-23 14:31: **********Val Epoch 2: average Loss: 0.245002\n",
      "2025-04-23 14:31: **********Val Epoch 2: average Loss: 0.245002\n",
      "2025-04-23 14:31: **********Val Epoch 2: average Loss: 0.245002\n",
      "2025-04-23 14:31: **********Val Epoch 2: average Loss: 0.245002\n",
      "2025-04-23 14:31: **********Val Epoch 2: average Loss: 0.245002\n",
      "2025-04-23 14:31: Train Epoch 3: 0/37 Loss: 0.380342\n",
      "2025-04-23 14:31: Train Epoch 3: 0/37 Loss: 0.380342\n",
      "2025-04-23 14:31: Train Epoch 3: 0/37 Loss: 0.380342\n",
      "2025-04-23 14:31: Train Epoch 3: 0/37 Loss: 0.380342\n",
      "2025-04-23 14:31: Train Epoch 3: 0/37 Loss: 0.380342\n",
      "2025-04-23 14:31: Train Epoch 3: 20/37 Loss: 0.105235\n",
      "2025-04-23 14:31: Train Epoch 3: 20/37 Loss: 0.105235\n",
      "2025-04-23 14:31: Train Epoch 3: 20/37 Loss: 0.105235\n",
      "2025-04-23 14:31: Train Epoch 3: 20/37 Loss: 0.105235\n",
      "2025-04-23 14:31: Train Epoch 3: 20/37 Loss: 0.105235\n",
      "2025-04-23 14:32: **********Train Epoch 3: averaged Loss: 0.225202\n",
      "2025-04-23 14:32: **********Train Epoch 3: averaged Loss: 0.225202\n",
      "2025-04-23 14:32: **********Train Epoch 3: averaged Loss: 0.225202\n",
      "2025-04-23 14:32: **********Train Epoch 3: averaged Loss: 0.225202\n",
      "2025-04-23 14:32: **********Train Epoch 3: averaged Loss: 0.225202\n",
      "2025-04-23 14:32: **********Val Epoch 3: average Loss: 0.262663\n",
      "2025-04-23 14:32: **********Val Epoch 3: average Loss: 0.262663\n",
      "2025-04-23 14:32: **********Val Epoch 3: average Loss: 0.262663\n",
      "2025-04-23 14:32: **********Val Epoch 3: average Loss: 0.262663\n",
      "2025-04-23 14:32: **********Val Epoch 3: average Loss: 0.262663\n",
      "2025-04-23 14:32: Train Epoch 4: 0/37 Loss: 0.362680\n",
      "2025-04-23 14:32: Train Epoch 4: 0/37 Loss: 0.362680\n",
      "2025-04-23 14:32: Train Epoch 4: 0/37 Loss: 0.362680\n",
      "2025-04-23 14:32: Train Epoch 4: 0/37 Loss: 0.362680\n",
      "2025-04-23 14:32: Train Epoch 4: 0/37 Loss: 0.362680\n",
      "2025-04-23 14:32: Train Epoch 4: 20/37 Loss: 0.087812\n",
      "2025-04-23 14:32: Train Epoch 4: 20/37 Loss: 0.087812\n",
      "2025-04-23 14:32: Train Epoch 4: 20/37 Loss: 0.087812\n",
      "2025-04-23 14:32: Train Epoch 4: 20/37 Loss: 0.087812\n",
      "2025-04-23 14:32: Train Epoch 4: 20/37 Loss: 0.087812\n",
      "2025-04-23 14:32: **********Train Epoch 4: averaged Loss: 0.217310\n",
      "2025-04-23 14:32: **********Train Epoch 4: averaged Loss: 0.217310\n",
      "2025-04-23 14:32: **********Train Epoch 4: averaged Loss: 0.217310\n",
      "2025-04-23 14:32: **********Train Epoch 4: averaged Loss: 0.217310\n",
      "2025-04-23 14:32: **********Train Epoch 4: averaged Loss: 0.217310\n",
      "2025-04-23 14:32: **********Val Epoch 4: average Loss: 0.279535\n",
      "2025-04-23 14:32: **********Val Epoch 4: average Loss: 0.279535\n",
      "2025-04-23 14:32: **********Val Epoch 4: average Loss: 0.279535\n",
      "2025-04-23 14:32: **********Val Epoch 4: average Loss: 0.279535\n",
      "2025-04-23 14:32: **********Val Epoch 4: average Loss: 0.279535\n",
      "2025-04-23 14:32: Train Epoch 5: 0/37 Loss: 0.345809\n",
      "2025-04-23 14:32: Train Epoch 5: 0/37 Loss: 0.345809\n",
      "2025-04-23 14:32: Train Epoch 5: 0/37 Loss: 0.345809\n",
      "2025-04-23 14:32: Train Epoch 5: 0/37 Loss: 0.345809\n",
      "2025-04-23 14:32: Train Epoch 5: 0/37 Loss: 0.345809\n",
      "2025-04-23 14:32: Train Epoch 5: 20/37 Loss: 0.071193\n",
      "2025-04-23 14:32: Train Epoch 5: 20/37 Loss: 0.071193\n",
      "2025-04-23 14:32: Train Epoch 5: 20/37 Loss: 0.071193\n",
      "2025-04-23 14:32: Train Epoch 5: 20/37 Loss: 0.071193\n",
      "2025-04-23 14:32: Train Epoch 5: 20/37 Loss: 0.071193\n",
      "2025-04-23 14:32: **********Train Epoch 5: averaged Loss: 0.210167\n",
      "2025-04-23 14:32: **********Train Epoch 5: averaged Loss: 0.210167\n",
      "2025-04-23 14:32: **********Train Epoch 5: averaged Loss: 0.210167\n",
      "2025-04-23 14:32: **********Train Epoch 5: averaged Loss: 0.210167\n",
      "2025-04-23 14:32: **********Train Epoch 5: averaged Loss: 0.210167\n",
      "2025-04-23 14:32: **********Val Epoch 5: average Loss: 0.295524\n",
      "2025-04-23 14:32: **********Val Epoch 5: average Loss: 0.295524\n",
      "2025-04-23 14:32: **********Val Epoch 5: average Loss: 0.295524\n",
      "2025-04-23 14:32: **********Val Epoch 5: average Loss: 0.295524\n",
      "2025-04-23 14:32: **********Val Epoch 5: average Loss: 0.295524\n",
      "2025-04-23 14:32: Train Epoch 6: 0/37 Loss: 0.329819\n",
      "2025-04-23 14:32: Train Epoch 6: 0/37 Loss: 0.329819\n",
      "2025-04-23 14:32: Train Epoch 6: 0/37 Loss: 0.329819\n",
      "2025-04-23 14:32: Train Epoch 6: 0/37 Loss: 0.329819\n",
      "2025-04-23 14:32: Train Epoch 6: 0/37 Loss: 0.329819\n",
      "2025-04-23 14:32: Train Epoch 6: 20/37 Loss: 0.055440\n",
      "2025-04-23 14:32: Train Epoch 6: 20/37 Loss: 0.055440\n",
      "2025-04-23 14:32: Train Epoch 6: 20/37 Loss: 0.055440\n",
      "2025-04-23 14:32: Train Epoch 6: 20/37 Loss: 0.055440\n",
      "2025-04-23 14:32: Train Epoch 6: 20/37 Loss: 0.055440\n",
      "2025-04-23 14:32: **********Train Epoch 6: averaged Loss: 0.203668\n",
      "2025-04-23 14:32: **********Train Epoch 6: averaged Loss: 0.203668\n",
      "2025-04-23 14:32: **********Train Epoch 6: averaged Loss: 0.203668\n",
      "2025-04-23 14:32: **********Train Epoch 6: averaged Loss: 0.203668\n",
      "2025-04-23 14:32: **********Train Epoch 6: averaged Loss: 0.203668\n",
      "2025-04-23 14:32: **********Val Epoch 6: average Loss: 0.310096\n",
      "2025-04-23 14:32: **********Val Epoch 6: average Loss: 0.310096\n",
      "2025-04-23 14:32: **********Val Epoch 6: average Loss: 0.310096\n",
      "2025-04-23 14:32: **********Val Epoch 6: average Loss: 0.310096\n",
      "2025-04-23 14:32: **********Val Epoch 6: average Loss: 0.310096\n",
      "2025-04-23 14:32: Train Epoch 7: 0/37 Loss: 0.315248\n",
      "2025-04-23 14:32: Train Epoch 7: 0/37 Loss: 0.315248\n",
      "2025-04-23 14:32: Train Epoch 7: 0/37 Loss: 0.315248\n",
      "2025-04-23 14:32: Train Epoch 7: 0/37 Loss: 0.315248\n",
      "2025-04-23 14:32: Train Epoch 7: 0/37 Loss: 0.315248\n",
      "2025-04-23 14:32: Train Epoch 7: 20/37 Loss: 0.041226\n",
      "2025-04-23 14:32: Train Epoch 7: 20/37 Loss: 0.041226\n",
      "2025-04-23 14:32: Train Epoch 7: 20/37 Loss: 0.041226\n",
      "2025-04-23 14:32: Train Epoch 7: 20/37 Loss: 0.041226\n",
      "2025-04-23 14:32: Train Epoch 7: 20/37 Loss: 0.041226\n",
      "2025-04-23 14:32: **********Train Epoch 7: averaged Loss: 0.198382\n",
      "2025-04-23 14:32: **********Train Epoch 7: averaged Loss: 0.198382\n",
      "2025-04-23 14:32: **********Train Epoch 7: averaged Loss: 0.198382\n",
      "2025-04-23 14:32: **********Train Epoch 7: averaged Loss: 0.198382\n",
      "2025-04-23 14:32: **********Train Epoch 7: averaged Loss: 0.198382\n",
      "2025-04-23 14:32: **********Val Epoch 7: average Loss: 0.322385\n",
      "2025-04-23 14:32: **********Val Epoch 7: average Loss: 0.322385\n",
      "2025-04-23 14:32: **********Val Epoch 7: average Loss: 0.322385\n",
      "2025-04-23 14:32: **********Val Epoch 7: average Loss: 0.322385\n",
      "2025-04-23 14:32: **********Val Epoch 7: average Loss: 0.322385\n",
      "2025-04-23 14:32: Train Epoch 8: 0/37 Loss: 0.302958\n",
      "2025-04-23 14:32: Train Epoch 8: 0/37 Loss: 0.302958\n",
      "2025-04-23 14:32: Train Epoch 8: 0/37 Loss: 0.302958\n",
      "2025-04-23 14:32: Train Epoch 8: 0/37 Loss: 0.302958\n",
      "2025-04-23 14:32: Train Epoch 8: 0/37 Loss: 0.302958\n",
      "2025-04-23 14:32: Train Epoch 8: 20/37 Loss: 0.029468\n",
      "2025-04-23 14:32: Train Epoch 8: 20/37 Loss: 0.029468\n",
      "2025-04-23 14:32: Train Epoch 8: 20/37 Loss: 0.029468\n",
      "2025-04-23 14:32: Train Epoch 8: 20/37 Loss: 0.029468\n",
      "2025-04-23 14:32: Train Epoch 8: 20/37 Loss: 0.029468\n",
      "2025-04-23 14:32: **********Train Epoch 8: averaged Loss: 0.194697\n",
      "2025-04-23 14:32: **********Train Epoch 8: averaged Loss: 0.194697\n",
      "2025-04-23 14:32: **********Train Epoch 8: averaged Loss: 0.194697\n",
      "2025-04-23 14:32: **********Train Epoch 8: averaged Loss: 0.194697\n",
      "2025-04-23 14:32: **********Train Epoch 8: averaged Loss: 0.194697\n",
      "2025-04-23 14:32: **********Val Epoch 8: average Loss: 0.332445\n",
      "2025-04-23 14:32: **********Val Epoch 8: average Loss: 0.332445\n",
      "2025-04-23 14:32: **********Val Epoch 8: average Loss: 0.332445\n",
      "2025-04-23 14:32: **********Val Epoch 8: average Loss: 0.332445\n",
      "2025-04-23 14:32: **********Val Epoch 8: average Loss: 0.332445\n",
      "2025-04-23 14:32: Train Epoch 9: 0/37 Loss: 0.292899\n",
      "2025-04-23 14:32: Train Epoch 9: 0/37 Loss: 0.292899\n",
      "2025-04-23 14:32: Train Epoch 9: 0/37 Loss: 0.292899\n",
      "2025-04-23 14:32: Train Epoch 9: 0/37 Loss: 0.292899\n",
      "2025-04-23 14:32: Train Epoch 9: 0/37 Loss: 0.292899\n",
      "2025-04-23 14:32: Train Epoch 9: 20/37 Loss: 0.019857\n",
      "2025-04-23 14:32: Train Epoch 9: 20/37 Loss: 0.019857\n",
      "2025-04-23 14:32: Train Epoch 9: 20/37 Loss: 0.019857\n",
      "2025-04-23 14:32: Train Epoch 9: 20/37 Loss: 0.019857\n",
      "2025-04-23 14:32: Train Epoch 9: 20/37 Loss: 0.019857\n",
      "2025-04-23 14:32: **********Train Epoch 9: averaged Loss: 0.192131\n",
      "2025-04-23 14:32: **********Train Epoch 9: averaged Loss: 0.192131\n",
      "2025-04-23 14:32: **********Train Epoch 9: averaged Loss: 0.192131\n",
      "2025-04-23 14:32: **********Train Epoch 9: averaged Loss: 0.192131\n",
      "2025-04-23 14:32: **********Train Epoch 9: averaged Loss: 0.192131\n",
      "2025-04-23 14:32: **********Val Epoch 9: average Loss: 0.341009\n",
      "2025-04-23 14:32: **********Val Epoch 9: average Loss: 0.341009\n",
      "2025-04-23 14:32: **********Val Epoch 9: average Loss: 0.341009\n",
      "2025-04-23 14:32: **********Val Epoch 9: average Loss: 0.341009\n",
      "2025-04-23 14:32: **********Val Epoch 9: average Loss: 0.341009\n",
      "2025-04-23 14:32: Train Epoch 10: 0/37 Loss: 0.284334\n",
      "2025-04-23 14:32: Train Epoch 10: 0/37 Loss: 0.284334\n",
      "2025-04-23 14:32: Train Epoch 10: 0/37 Loss: 0.284334\n",
      "2025-04-23 14:32: Train Epoch 10: 0/37 Loss: 0.284334\n",
      "2025-04-23 14:32: Train Epoch 10: 0/37 Loss: 0.284334\n",
      "2025-04-23 14:32: Train Epoch 10: 20/37 Loss: 0.012613\n",
      "2025-04-23 14:32: Train Epoch 10: 20/37 Loss: 0.012613\n",
      "2025-04-23 14:32: Train Epoch 10: 20/37 Loss: 0.012613\n",
      "2025-04-23 14:32: Train Epoch 10: 20/37 Loss: 0.012613\n",
      "2025-04-23 14:32: Train Epoch 10: 20/37 Loss: 0.012613\n",
      "2025-04-23 14:32: **********Train Epoch 10: averaged Loss: 0.190188\n",
      "2025-04-23 14:32: **********Train Epoch 10: averaged Loss: 0.190188\n",
      "2025-04-23 14:32: **********Train Epoch 10: averaged Loss: 0.190188\n",
      "2025-04-23 14:32: **********Train Epoch 10: averaged Loss: 0.190188\n",
      "2025-04-23 14:32: **********Train Epoch 10: averaged Loss: 0.190188\n",
      "2025-04-23 14:32: **********Val Epoch 10: average Loss: 0.348371\n",
      "2025-04-23 14:32: **********Val Epoch 10: average Loss: 0.348371\n",
      "2025-04-23 14:32: **********Val Epoch 10: average Loss: 0.348371\n",
      "2025-04-23 14:32: **********Val Epoch 10: average Loss: 0.348371\n",
      "2025-04-23 14:32: **********Val Epoch 10: average Loss: 0.348371\n",
      "2025-04-23 14:32: Train Epoch 11: 0/37 Loss: 0.276972\n",
      "2025-04-23 14:32: Train Epoch 11: 0/37 Loss: 0.276972\n",
      "2025-04-23 14:32: Train Epoch 11: 0/37 Loss: 0.276972\n",
      "2025-04-23 14:32: Train Epoch 11: 0/37 Loss: 0.276972\n",
      "2025-04-23 14:32: Train Epoch 11: 0/37 Loss: 0.276972\n",
      "2025-04-23 14:32: Train Epoch 11: 20/37 Loss: 0.009778\n",
      "2025-04-23 14:32: Train Epoch 11: 20/37 Loss: 0.009778\n",
      "2025-04-23 14:32: Train Epoch 11: 20/37 Loss: 0.009778\n",
      "2025-04-23 14:32: Train Epoch 11: 20/37 Loss: 0.009778\n",
      "2025-04-23 14:32: Train Epoch 11: 20/37 Loss: 0.009778\n",
      "2025-04-23 14:32: **********Train Epoch 11: averaged Loss: 0.188683\n",
      "2025-04-23 14:32: **********Train Epoch 11: averaged Loss: 0.188683\n",
      "2025-04-23 14:32: **********Train Epoch 11: averaged Loss: 0.188683\n",
      "2025-04-23 14:32: **********Train Epoch 11: averaged Loss: 0.188683\n",
      "2025-04-23 14:32: **********Train Epoch 11: averaged Loss: 0.188683\n",
      "2025-04-23 14:32: **********Val Epoch 11: average Loss: 0.354380\n",
      "2025-04-23 14:32: **********Val Epoch 11: average Loss: 0.354380\n",
      "2025-04-23 14:32: **********Val Epoch 11: average Loss: 0.354380\n",
      "2025-04-23 14:32: **********Val Epoch 11: average Loss: 0.354380\n",
      "2025-04-23 14:32: **********Val Epoch 11: average Loss: 0.354380\n",
      "2025-04-23 14:32: Train Epoch 12: 0/37 Loss: 0.270964\n",
      "2025-04-23 14:32: Train Epoch 12: 0/37 Loss: 0.270964\n",
      "2025-04-23 14:32: Train Epoch 12: 0/37 Loss: 0.270964\n",
      "2025-04-23 14:32: Train Epoch 12: 0/37 Loss: 0.270964\n",
      "2025-04-23 14:32: Train Epoch 12: 0/37 Loss: 0.270964\n",
      "2025-04-23 14:32: Train Epoch 12: 20/37 Loss: 0.009704\n",
      "2025-04-23 14:32: Train Epoch 12: 20/37 Loss: 0.009704\n",
      "2025-04-23 14:32: Train Epoch 12: 20/37 Loss: 0.009704\n",
      "2025-04-23 14:32: Train Epoch 12: 20/37 Loss: 0.009704\n",
      "2025-04-23 14:32: Train Epoch 12: 20/37 Loss: 0.009704\n",
      "2025-04-23 14:32: **********Train Epoch 12: averaged Loss: 0.187714\n",
      "2025-04-23 14:32: **********Train Epoch 12: averaged Loss: 0.187714\n",
      "2025-04-23 14:32: **********Train Epoch 12: averaged Loss: 0.187714\n",
      "2025-04-23 14:32: **********Train Epoch 12: averaged Loss: 0.187714\n",
      "2025-04-23 14:32: **********Train Epoch 12: averaged Loss: 0.187714\n",
      "2025-04-23 14:32: **********Val Epoch 12: average Loss: 0.359764\n",
      "2025-04-23 14:32: **********Val Epoch 12: average Loss: 0.359764\n",
      "2025-04-23 14:32: **********Val Epoch 12: average Loss: 0.359764\n",
      "2025-04-23 14:32: **********Val Epoch 12: average Loss: 0.359764\n",
      "2025-04-23 14:32: **********Val Epoch 12: average Loss: 0.359764\n",
      "2025-04-23 14:32: Train Epoch 13: 0/37 Loss: 0.265579\n",
      "2025-04-23 14:32: Train Epoch 13: 0/37 Loss: 0.265579\n",
      "2025-04-23 14:32: Train Epoch 13: 0/37 Loss: 0.265579\n",
      "2025-04-23 14:32: Train Epoch 13: 0/37 Loss: 0.265579\n",
      "2025-04-23 14:32: Train Epoch 13: 0/37 Loss: 0.265579\n",
      "2025-04-23 14:32: Train Epoch 13: 20/37 Loss: 0.010821\n",
      "2025-04-23 14:32: Train Epoch 13: 20/37 Loss: 0.010821\n",
      "2025-04-23 14:32: Train Epoch 13: 20/37 Loss: 0.010821\n",
      "2025-04-23 14:32: Train Epoch 13: 20/37 Loss: 0.010821\n",
      "2025-04-23 14:32: Train Epoch 13: 20/37 Loss: 0.010821\n",
      "2025-04-23 14:32: **********Train Epoch 13: averaged Loss: 0.186886\n",
      "2025-04-23 14:32: **********Train Epoch 13: averaged Loss: 0.186886\n",
      "2025-04-23 14:32: **********Train Epoch 13: averaged Loss: 0.186886\n",
      "2025-04-23 14:32: **********Train Epoch 13: averaged Loss: 0.186886\n",
      "2025-04-23 14:32: **********Train Epoch 13: averaged Loss: 0.186886\n",
      "2025-04-23 14:32: **********Val Epoch 13: average Loss: 0.364514\n",
      "2025-04-23 14:32: **********Val Epoch 13: average Loss: 0.364514\n",
      "2025-04-23 14:32: **********Val Epoch 13: average Loss: 0.364514\n",
      "2025-04-23 14:32: **********Val Epoch 13: average Loss: 0.364514\n",
      "2025-04-23 14:32: **********Val Epoch 13: average Loss: 0.364514\n",
      "2025-04-23 14:32: Train Epoch 14: 0/37 Loss: 0.260830\n",
      "2025-04-23 14:32: Train Epoch 14: 0/37 Loss: 0.260830\n",
      "2025-04-23 14:32: Train Epoch 14: 0/37 Loss: 0.260830\n",
      "2025-04-23 14:32: Train Epoch 14: 0/37 Loss: 0.260830\n",
      "2025-04-23 14:32: Train Epoch 14: 0/37 Loss: 0.260830\n",
      "2025-04-23 14:32: Train Epoch 14: 20/37 Loss: 0.013178\n",
      "2025-04-23 14:32: Train Epoch 14: 20/37 Loss: 0.013178\n",
      "2025-04-23 14:32: Train Epoch 14: 20/37 Loss: 0.013178\n",
      "2025-04-23 14:32: Train Epoch 14: 20/37 Loss: 0.013178\n",
      "2025-04-23 14:32: Train Epoch 14: 20/37 Loss: 0.013178\n",
      "2025-04-23 14:32: **********Train Epoch 14: averaged Loss: 0.186201\n",
      "2025-04-23 14:32: **********Train Epoch 14: averaged Loss: 0.186201\n",
      "2025-04-23 14:32: **********Train Epoch 14: averaged Loss: 0.186201\n",
      "2025-04-23 14:32: **********Train Epoch 14: averaged Loss: 0.186201\n",
      "2025-04-23 14:32: **********Train Epoch 14: averaged Loss: 0.186201\n",
      "2025-04-23 14:32: **********Val Epoch 14: average Loss: 0.368575\n",
      "2025-04-23 14:32: **********Val Epoch 14: average Loss: 0.368575\n",
      "2025-04-23 14:32: **********Val Epoch 14: average Loss: 0.368575\n",
      "2025-04-23 14:32: **********Val Epoch 14: average Loss: 0.368575\n",
      "2025-04-23 14:32: **********Val Epoch 14: average Loss: 0.368575\n",
      "2025-04-23 14:32: Train Epoch 15: 0/37 Loss: 0.256769\n",
      "2025-04-23 14:32: Train Epoch 15: 0/37 Loss: 0.256769\n",
      "2025-04-23 14:32: Train Epoch 15: 0/37 Loss: 0.256769\n",
      "2025-04-23 14:32: Train Epoch 15: 0/37 Loss: 0.256769\n",
      "2025-04-23 14:32: Train Epoch 15: 0/37 Loss: 0.256769\n",
      "2025-04-23 14:32: Train Epoch 15: 20/37 Loss: 0.016004\n",
      "2025-04-23 14:32: Train Epoch 15: 20/37 Loss: 0.016004\n",
      "2025-04-23 14:32: Train Epoch 15: 20/37 Loss: 0.016004\n",
      "2025-04-23 14:32: Train Epoch 15: 20/37 Loss: 0.016004\n",
      "2025-04-23 14:32: Train Epoch 15: 20/37 Loss: 0.016004\n",
      "2025-04-23 14:32: **********Train Epoch 15: averaged Loss: 0.185701\n",
      "2025-04-23 14:32: **********Train Epoch 15: averaged Loss: 0.185701\n",
      "2025-04-23 14:32: **********Train Epoch 15: averaged Loss: 0.185701\n",
      "2025-04-23 14:32: **********Train Epoch 15: averaged Loss: 0.185701\n",
      "2025-04-23 14:32: **********Train Epoch 15: averaged Loss: 0.185701\n",
      "2025-04-23 14:32: **********Val Epoch 15: average Loss: 0.372123\n",
      "2025-04-23 14:32: **********Val Epoch 15: average Loss: 0.372123\n",
      "2025-04-23 14:32: **********Val Epoch 15: average Loss: 0.372123\n",
      "2025-04-23 14:32: **********Val Epoch 15: average Loss: 0.372123\n",
      "2025-04-23 14:32: **********Val Epoch 15: average Loss: 0.372123\n",
      "2025-04-23 14:32: Train Epoch 16: 0/37 Loss: 0.253221\n",
      "2025-04-23 14:32: Train Epoch 16: 0/37 Loss: 0.253221\n",
      "2025-04-23 14:32: Train Epoch 16: 0/37 Loss: 0.253221\n",
      "2025-04-23 14:32: Train Epoch 16: 0/37 Loss: 0.253221\n",
      "2025-04-23 14:32: Train Epoch 16: 0/37 Loss: 0.253221\n",
      "2025-04-23 14:32: Train Epoch 16: 20/37 Loss: 0.018901\n",
      "2025-04-23 14:32: Train Epoch 16: 20/37 Loss: 0.018901\n",
      "2025-04-23 14:32: Train Epoch 16: 20/37 Loss: 0.018901\n",
      "2025-04-23 14:32: Train Epoch 16: 20/37 Loss: 0.018901\n",
      "2025-04-23 14:32: Train Epoch 16: 20/37 Loss: 0.018901\n",
      "2025-04-23 14:32: **********Train Epoch 16: averaged Loss: 0.185285\n",
      "2025-04-23 14:32: **********Train Epoch 16: averaged Loss: 0.185285\n",
      "2025-04-23 14:32: **********Train Epoch 16: averaged Loss: 0.185285\n",
      "2025-04-23 14:32: **********Train Epoch 16: averaged Loss: 0.185285\n",
      "2025-04-23 14:32: **********Train Epoch 16: averaged Loss: 0.185285\n",
      "2025-04-23 14:32: **********Val Epoch 16: average Loss: 0.375218\n",
      "2025-04-23 14:32: **********Val Epoch 16: average Loss: 0.375218\n",
      "2025-04-23 14:32: **********Val Epoch 16: average Loss: 0.375218\n",
      "2025-04-23 14:32: **********Val Epoch 16: average Loss: 0.375218\n",
      "2025-04-23 14:32: **********Val Epoch 16: average Loss: 0.375218\n",
      "2025-04-23 14:32: Train Epoch 17: 0/37 Loss: 0.250126\n",
      "2025-04-23 14:32: Train Epoch 17: 0/37 Loss: 0.250126\n",
      "2025-04-23 14:32: Train Epoch 17: 0/37 Loss: 0.250126\n",
      "2025-04-23 14:32: Train Epoch 17: 0/37 Loss: 0.250126\n",
      "2025-04-23 14:32: Train Epoch 17: 0/37 Loss: 0.250126\n",
      "2025-04-23 14:32: Train Epoch 17: 20/37 Loss: 0.021753\n",
      "2025-04-23 14:32: Train Epoch 17: 20/37 Loss: 0.021753\n",
      "2025-04-23 14:32: Train Epoch 17: 20/37 Loss: 0.021753\n",
      "2025-04-23 14:32: Train Epoch 17: 20/37 Loss: 0.021753\n",
      "2025-04-23 14:32: Train Epoch 17: 20/37 Loss: 0.021753\n",
      "2025-04-23 14:32: **********Train Epoch 17: averaged Loss: 0.184973\n",
      "2025-04-23 14:32: **********Train Epoch 17: averaged Loss: 0.184973\n",
      "2025-04-23 14:32: **********Train Epoch 17: averaged Loss: 0.184973\n",
      "2025-04-23 14:32: **********Train Epoch 17: averaged Loss: 0.184973\n",
      "2025-04-23 14:32: **********Train Epoch 17: averaged Loss: 0.184973\n",
      "2025-04-23 14:32: **********Val Epoch 17: average Loss: 0.377955\n",
      "2025-04-23 14:32: **********Val Epoch 17: average Loss: 0.377955\n",
      "2025-04-23 14:32: **********Val Epoch 17: average Loss: 0.377955\n",
      "2025-04-23 14:32: **********Val Epoch 17: average Loss: 0.377955\n",
      "2025-04-23 14:32: **********Val Epoch 17: average Loss: 0.377955\n",
      "2025-04-23 14:32: Train Epoch 18: 0/37 Loss: 0.247388\n",
      "2025-04-23 14:32: Train Epoch 18: 0/37 Loss: 0.247388\n",
      "2025-04-23 14:32: Train Epoch 18: 0/37 Loss: 0.247388\n",
      "2025-04-23 14:32: Train Epoch 18: 0/37 Loss: 0.247388\n",
      "2025-04-23 14:32: Train Epoch 18: 0/37 Loss: 0.247388\n",
      "2025-04-23 14:32: Train Epoch 18: 20/37 Loss: 0.024343\n",
      "2025-04-23 14:32: Train Epoch 18: 20/37 Loss: 0.024343\n",
      "2025-04-23 14:32: Train Epoch 18: 20/37 Loss: 0.024343\n",
      "2025-04-23 14:32: Train Epoch 18: 20/37 Loss: 0.024343\n",
      "2025-04-23 14:32: Train Epoch 18: 20/37 Loss: 0.024343\n",
      "2025-04-23 14:32: **********Train Epoch 18: averaged Loss: 0.184710\n",
      "2025-04-23 14:32: **********Train Epoch 18: averaged Loss: 0.184710\n",
      "2025-04-23 14:32: **********Train Epoch 18: averaged Loss: 0.184710\n",
      "2025-04-23 14:32: **********Train Epoch 18: averaged Loss: 0.184710\n",
      "2025-04-23 14:32: **********Train Epoch 18: averaged Loss: 0.184710\n",
      "2025-04-23 14:32: **********Val Epoch 18: average Loss: 0.380377\n",
      "2025-04-23 14:32: **********Val Epoch 18: average Loss: 0.380377\n",
      "2025-04-23 14:32: **********Val Epoch 18: average Loss: 0.380377\n",
      "2025-04-23 14:32: **********Val Epoch 18: average Loss: 0.380377\n",
      "2025-04-23 14:32: **********Val Epoch 18: average Loss: 0.380377\n",
      "2025-04-23 14:32: Train Epoch 19: 0/37 Loss: 0.244967\n",
      "2025-04-23 14:32: Train Epoch 19: 0/37 Loss: 0.244967\n",
      "2025-04-23 14:32: Train Epoch 19: 0/37 Loss: 0.244967\n",
      "2025-04-23 14:32: Train Epoch 19: 0/37 Loss: 0.244967\n",
      "2025-04-23 14:32: Train Epoch 19: 0/37 Loss: 0.244967\n",
      "2025-04-23 14:32: Train Epoch 19: 20/37 Loss: 0.026695\n",
      "2025-04-23 14:32: Train Epoch 19: 20/37 Loss: 0.026695\n",
      "2025-04-23 14:32: Train Epoch 19: 20/37 Loss: 0.026695\n",
      "2025-04-23 14:32: Train Epoch 19: 20/37 Loss: 0.026695\n",
      "2025-04-23 14:32: Train Epoch 19: 20/37 Loss: 0.026695\n",
      "2025-04-23 14:32: **********Train Epoch 19: averaged Loss: 0.184516\n",
      "2025-04-23 14:32: **********Train Epoch 19: averaged Loss: 0.184516\n",
      "2025-04-23 14:32: **********Train Epoch 19: averaged Loss: 0.184516\n",
      "2025-04-23 14:32: **********Train Epoch 19: averaged Loss: 0.184516\n",
      "2025-04-23 14:32: **********Train Epoch 19: averaged Loss: 0.184516\n",
      "2025-04-23 14:32: **********Val Epoch 19: average Loss: 0.382606\n",
      "2025-04-23 14:32: **********Val Epoch 19: average Loss: 0.382606\n",
      "2025-04-23 14:32: **********Val Epoch 19: average Loss: 0.382606\n",
      "2025-04-23 14:32: **********Val Epoch 19: average Loss: 0.382606\n",
      "2025-04-23 14:32: **********Val Epoch 19: average Loss: 0.382606\n",
      "2025-04-23 14:32: Train Epoch 20: 0/37 Loss: 0.242738\n",
      "2025-04-23 14:32: Train Epoch 20: 0/37 Loss: 0.242738\n",
      "2025-04-23 14:32: Train Epoch 20: 0/37 Loss: 0.242738\n",
      "2025-04-23 14:32: Train Epoch 20: 0/37 Loss: 0.242738\n",
      "2025-04-23 14:32: Train Epoch 20: 0/37 Loss: 0.242738\n",
      "2025-04-23 14:32: Train Epoch 20: 20/37 Loss: 0.028888\n",
      "2025-04-23 14:32: Train Epoch 20: 20/37 Loss: 0.028888\n",
      "2025-04-23 14:32: Train Epoch 20: 20/37 Loss: 0.028888\n",
      "2025-04-23 14:32: Train Epoch 20: 20/37 Loss: 0.028888\n",
      "2025-04-23 14:32: Train Epoch 20: 20/37 Loss: 0.028888\n",
      "2025-04-23 14:32: **********Train Epoch 20: averaged Loss: 0.184362\n",
      "2025-04-23 14:32: **********Train Epoch 20: averaged Loss: 0.184362\n",
      "2025-04-23 14:32: **********Train Epoch 20: averaged Loss: 0.184362\n",
      "2025-04-23 14:32: **********Train Epoch 20: averaged Loss: 0.184362\n",
      "2025-04-23 14:32: **********Train Epoch 20: averaged Loss: 0.184362\n",
      "2025-04-23 14:32: **********Val Epoch 20: average Loss: 0.384728\n",
      "2025-04-23 14:32: **********Val Epoch 20: average Loss: 0.384728\n",
      "2025-04-23 14:32: **********Val Epoch 20: average Loss: 0.384728\n",
      "2025-04-23 14:32: **********Val Epoch 20: average Loss: 0.384728\n",
      "2025-04-23 14:32: **********Val Epoch 20: average Loss: 0.384728\n",
      "2025-04-23 14:32: Train Epoch 21: 0/37 Loss: 0.240616\n",
      "2025-04-23 14:32: Train Epoch 21: 0/37 Loss: 0.240616\n",
      "2025-04-23 14:32: Train Epoch 21: 0/37 Loss: 0.240616\n",
      "2025-04-23 14:32: Train Epoch 21: 0/37 Loss: 0.240616\n",
      "2025-04-23 14:32: Train Epoch 21: 0/37 Loss: 0.240616\n",
      "2025-04-23 14:32: Train Epoch 21: 20/37 Loss: 0.030959\n",
      "2025-04-23 14:32: Train Epoch 21: 20/37 Loss: 0.030959\n",
      "2025-04-23 14:32: Train Epoch 21: 20/37 Loss: 0.030959\n",
      "2025-04-23 14:32: Train Epoch 21: 20/37 Loss: 0.030959\n",
      "2025-04-23 14:32: Train Epoch 21: 20/37 Loss: 0.030959\n",
      "2025-04-23 14:32: **********Train Epoch 21: averaged Loss: 0.184165\n",
      "2025-04-23 14:32: **********Train Epoch 21: averaged Loss: 0.184165\n",
      "2025-04-23 14:32: **********Train Epoch 21: averaged Loss: 0.184165\n",
      "2025-04-23 14:32: **********Train Epoch 21: averaged Loss: 0.184165\n",
      "2025-04-23 14:32: **********Train Epoch 21: averaged Loss: 0.184165\n",
      "2025-04-23 14:32: **********Val Epoch 21: average Loss: 0.386534\n",
      "2025-04-23 14:32: **********Val Epoch 21: average Loss: 0.386534\n",
      "2025-04-23 14:32: **********Val Epoch 21: average Loss: 0.386534\n",
      "2025-04-23 14:32: **********Val Epoch 21: average Loss: 0.386534\n",
      "2025-04-23 14:32: **********Val Epoch 21: average Loss: 0.386534\n",
      "2025-04-23 14:32: Train Epoch 22: 0/37 Loss: 0.238810\n",
      "2025-04-23 14:32: Train Epoch 22: 0/37 Loss: 0.238810\n",
      "2025-04-23 14:32: Train Epoch 22: 0/37 Loss: 0.238810\n",
      "2025-04-23 14:32: Train Epoch 22: 0/37 Loss: 0.238810\n",
      "2025-04-23 14:32: Train Epoch 22: 0/37 Loss: 0.238810\n",
      "2025-04-23 14:32: Train Epoch 22: 20/37 Loss: 0.032692\n",
      "2025-04-23 14:32: Train Epoch 22: 20/37 Loss: 0.032692\n",
      "2025-04-23 14:32: Train Epoch 22: 20/37 Loss: 0.032692\n",
      "2025-04-23 14:32: Train Epoch 22: 20/37 Loss: 0.032692\n",
      "2025-04-23 14:32: Train Epoch 22: 20/37 Loss: 0.032692\n",
      "2025-04-23 14:32: **********Train Epoch 22: averaged Loss: 0.184041\n",
      "2025-04-23 14:32: **********Train Epoch 22: averaged Loss: 0.184041\n",
      "2025-04-23 14:32: **********Train Epoch 22: averaged Loss: 0.184041\n",
      "2025-04-23 14:32: **********Train Epoch 22: averaged Loss: 0.184041\n",
      "2025-04-23 14:32: **********Train Epoch 22: averaged Loss: 0.184041\n",
      "2025-04-23 14:32: **********Val Epoch 22: average Loss: 0.388171\n",
      "2025-04-23 14:32: **********Val Epoch 22: average Loss: 0.388171\n",
      "2025-04-23 14:32: **********Val Epoch 22: average Loss: 0.388171\n",
      "2025-04-23 14:32: **********Val Epoch 22: average Loss: 0.388171\n",
      "2025-04-23 14:32: **********Val Epoch 22: average Loss: 0.388171\n",
      "2025-04-23 14:32: Train Epoch 23: 0/37 Loss: 0.237173\n",
      "2025-04-23 14:32: Train Epoch 23: 0/37 Loss: 0.237173\n",
      "2025-04-23 14:32: Train Epoch 23: 0/37 Loss: 0.237173\n",
      "2025-04-23 14:32: Train Epoch 23: 0/37 Loss: 0.237173\n",
      "2025-04-23 14:32: Train Epoch 23: 0/37 Loss: 0.237173\n",
      "2025-04-23 14:32: Train Epoch 23: 20/37 Loss: 0.034290\n",
      "2025-04-23 14:32: Train Epoch 23: 20/37 Loss: 0.034290\n",
      "2025-04-23 14:32: Train Epoch 23: 20/37 Loss: 0.034290\n",
      "2025-04-23 14:32: Train Epoch 23: 20/37 Loss: 0.034290\n",
      "2025-04-23 14:32: Train Epoch 23: 20/37 Loss: 0.034290\n",
      "2025-04-23 14:32: **********Train Epoch 23: averaged Loss: 0.183935\n",
      "2025-04-23 14:32: **********Train Epoch 23: averaged Loss: 0.183935\n",
      "2025-04-23 14:32: **********Train Epoch 23: averaged Loss: 0.183935\n",
      "2025-04-23 14:32: **********Train Epoch 23: averaged Loss: 0.183935\n",
      "2025-04-23 14:32: **********Train Epoch 23: averaged Loss: 0.183935\n",
      "2025-04-23 14:32: **********Val Epoch 23: average Loss: 0.389676\n",
      "2025-04-23 14:32: **********Val Epoch 23: average Loss: 0.389676\n",
      "2025-04-23 14:32: **********Val Epoch 23: average Loss: 0.389676\n",
      "2025-04-23 14:32: **********Val Epoch 23: average Loss: 0.389676\n",
      "2025-04-23 14:32: **********Val Epoch 23: average Loss: 0.389676\n",
      "2025-04-23 14:32: Train Epoch 24: 0/37 Loss: 0.235667\n",
      "2025-04-23 14:32: Train Epoch 24: 0/37 Loss: 0.235667\n",
      "2025-04-23 14:32: Train Epoch 24: 0/37 Loss: 0.235667\n",
      "2025-04-23 14:32: Train Epoch 24: 0/37 Loss: 0.235667\n",
      "2025-04-23 14:32: Train Epoch 24: 0/37 Loss: 0.235667\n",
      "2025-04-23 14:32: Train Epoch 24: 20/37 Loss: 0.035763\n",
      "2025-04-23 14:32: Train Epoch 24: 20/37 Loss: 0.035763\n",
      "2025-04-23 14:32: Train Epoch 24: 20/37 Loss: 0.035763\n",
      "2025-04-23 14:32: Train Epoch 24: 20/37 Loss: 0.035763\n",
      "2025-04-23 14:32: Train Epoch 24: 20/37 Loss: 0.035763\n",
      "2025-04-23 14:32: **********Train Epoch 24: averaged Loss: 0.183850\n",
      "2025-04-23 14:32: **********Train Epoch 24: averaged Loss: 0.183850\n",
      "2025-04-23 14:32: **********Train Epoch 24: averaged Loss: 0.183850\n",
      "2025-04-23 14:32: **********Train Epoch 24: averaged Loss: 0.183850\n",
      "2025-04-23 14:32: **********Train Epoch 24: averaged Loss: 0.183850\n",
      "2025-04-23 14:32: **********Val Epoch 24: average Loss: 0.391082\n",
      "2025-04-23 14:32: **********Val Epoch 24: average Loss: 0.391082\n",
      "2025-04-23 14:32: **********Val Epoch 24: average Loss: 0.391082\n",
      "2025-04-23 14:32: **********Val Epoch 24: average Loss: 0.391082\n",
      "2025-04-23 14:32: **********Val Epoch 24: average Loss: 0.391082\n",
      "2025-04-23 14:32: Train Epoch 25: 0/37 Loss: 0.234262\n",
      "2025-04-23 14:32: Train Epoch 25: 0/37 Loss: 0.234262\n",
      "2025-04-23 14:32: Train Epoch 25: 0/37 Loss: 0.234262\n",
      "2025-04-23 14:32: Train Epoch 25: 0/37 Loss: 0.234262\n",
      "2025-04-23 14:32: Train Epoch 25: 0/37 Loss: 0.234262\n",
      "2025-04-23 14:32: Train Epoch 25: 20/37 Loss: 0.037144\n",
      "2025-04-23 14:32: Train Epoch 25: 20/37 Loss: 0.037144\n",
      "2025-04-23 14:32: Train Epoch 25: 20/37 Loss: 0.037144\n",
      "2025-04-23 14:32: Train Epoch 25: 20/37 Loss: 0.037144\n",
      "2025-04-23 14:32: Train Epoch 25: 20/37 Loss: 0.037144\n",
      "2025-04-23 14:32: **********Train Epoch 25: averaged Loss: 0.183782\n",
      "2025-04-23 14:32: **********Train Epoch 25: averaged Loss: 0.183782\n",
      "2025-04-23 14:32: **********Train Epoch 25: averaged Loss: 0.183782\n",
      "2025-04-23 14:32: **********Train Epoch 25: averaged Loss: 0.183782\n",
      "2025-04-23 14:32: **********Train Epoch 25: averaged Loss: 0.183782\n",
      "2025-04-23 14:32: **********Val Epoch 25: average Loss: 0.392418\n",
      "2025-04-23 14:32: **********Val Epoch 25: average Loss: 0.392418\n",
      "2025-04-23 14:32: **********Val Epoch 25: average Loss: 0.392418\n",
      "2025-04-23 14:32: **********Val Epoch 25: average Loss: 0.392418\n",
      "2025-04-23 14:32: **********Val Epoch 25: average Loss: 0.392418\n",
      "2025-04-23 14:32: Train Epoch 26: 0/37 Loss: 0.232926\n",
      "2025-04-23 14:32: Train Epoch 26: 0/37 Loss: 0.232926\n",
      "2025-04-23 14:32: Train Epoch 26: 0/37 Loss: 0.232926\n",
      "2025-04-23 14:32: Train Epoch 26: 0/37 Loss: 0.232926\n",
      "2025-04-23 14:32: Train Epoch 26: 0/37 Loss: 0.232926\n",
      "2025-04-23 14:32: Train Epoch 26: 20/37 Loss: 0.038459\n",
      "2025-04-23 14:32: Train Epoch 26: 20/37 Loss: 0.038459\n",
      "2025-04-23 14:32: Train Epoch 26: 20/37 Loss: 0.038459\n",
      "2025-04-23 14:32: Train Epoch 26: 20/37 Loss: 0.038459\n",
      "2025-04-23 14:32: Train Epoch 26: 20/37 Loss: 0.038459\n",
      "2025-04-23 14:32: **********Train Epoch 26: averaged Loss: 0.183730\n",
      "2025-04-23 14:32: **********Train Epoch 26: averaged Loss: 0.183730\n",
      "2025-04-23 14:32: **********Train Epoch 26: averaged Loss: 0.183730\n",
      "2025-04-23 14:32: **********Train Epoch 26: averaged Loss: 0.183730\n",
      "2025-04-23 14:32: **********Train Epoch 26: averaged Loss: 0.183730\n",
      "2025-04-23 14:32: **********Val Epoch 26: average Loss: 0.393738\n",
      "2025-04-23 14:32: **********Val Epoch 26: average Loss: 0.393738\n",
      "2025-04-23 14:32: **********Val Epoch 26: average Loss: 0.393738\n",
      "2025-04-23 14:32: **********Val Epoch 26: average Loss: 0.393738\n",
      "2025-04-23 14:32: **********Val Epoch 26: average Loss: 0.393738\n",
      "2025-04-23 14:32: Train Epoch 27: 0/37 Loss: 0.231606\n",
      "2025-04-23 14:32: Train Epoch 27: 0/37 Loss: 0.231606\n",
      "2025-04-23 14:32: Train Epoch 27: 0/37 Loss: 0.231606\n",
      "2025-04-23 14:32: Train Epoch 27: 0/37 Loss: 0.231606\n",
      "2025-04-23 14:32: Train Epoch 27: 0/37 Loss: 0.231606\n",
      "2025-04-23 14:32: Train Epoch 27: 20/37 Loss: 0.039770\n",
      "2025-04-23 14:32: Train Epoch 27: 20/37 Loss: 0.039770\n",
      "2025-04-23 14:32: Train Epoch 27: 20/37 Loss: 0.039770\n",
      "2025-04-23 14:32: Train Epoch 27: 20/37 Loss: 0.039770\n",
      "2025-04-23 14:32: Train Epoch 27: 20/37 Loss: 0.039770\n",
      "2025-04-23 14:32: **********Train Epoch 27: averaged Loss: 0.183681\n",
      "2025-04-23 14:32: **********Train Epoch 27: averaged Loss: 0.183681\n",
      "2025-04-23 14:32: **********Train Epoch 27: averaged Loss: 0.183681\n",
      "2025-04-23 14:32: **********Train Epoch 27: averaged Loss: 0.183681\n",
      "2025-04-23 14:32: **********Train Epoch 27: averaged Loss: 0.183681\n",
      "2025-04-23 14:32: **********Val Epoch 27: average Loss: 0.395055\n",
      "2025-04-23 14:32: **********Val Epoch 27: average Loss: 0.395055\n",
      "2025-04-23 14:32: **********Val Epoch 27: average Loss: 0.395055\n",
      "2025-04-23 14:32: **********Val Epoch 27: average Loss: 0.395055\n",
      "2025-04-23 14:32: **********Val Epoch 27: average Loss: 0.395055\n",
      "2025-04-23 14:32: Train Epoch 28: 0/37 Loss: 0.230288\n",
      "2025-04-23 14:32: Train Epoch 28: 0/37 Loss: 0.230288\n",
      "2025-04-23 14:32: Train Epoch 28: 0/37 Loss: 0.230288\n",
      "2025-04-23 14:32: Train Epoch 28: 0/37 Loss: 0.230288\n",
      "2025-04-23 14:32: Train Epoch 28: 0/37 Loss: 0.230288\n",
      "2025-04-23 14:32: Train Epoch 28: 20/37 Loss: 0.041069\n",
      "2025-04-23 14:32: Train Epoch 28: 20/37 Loss: 0.041069\n",
      "2025-04-23 14:32: Train Epoch 28: 20/37 Loss: 0.041069\n",
      "2025-04-23 14:32: Train Epoch 28: 20/37 Loss: 0.041069\n",
      "2025-04-23 14:32: Train Epoch 28: 20/37 Loss: 0.041069\n",
      "2025-04-23 14:32: **********Train Epoch 28: averaged Loss: 0.183606\n",
      "2025-04-23 14:32: **********Train Epoch 28: averaged Loss: 0.183606\n",
      "2025-04-23 14:32: **********Train Epoch 28: averaged Loss: 0.183606\n",
      "2025-04-23 14:32: **********Train Epoch 28: averaged Loss: 0.183606\n",
      "2025-04-23 14:32: **********Train Epoch 28: averaged Loss: 0.183606\n",
      "2025-04-23 14:32: **********Val Epoch 28: average Loss: 0.396266\n",
      "2025-04-23 14:32: **********Val Epoch 28: average Loss: 0.396266\n",
      "2025-04-23 14:32: **********Val Epoch 28: average Loss: 0.396266\n",
      "2025-04-23 14:32: **********Val Epoch 28: average Loss: 0.396266\n",
      "2025-04-23 14:32: **********Val Epoch 28: average Loss: 0.396266\n",
      "2025-04-23 14:32: Train Epoch 29: 0/37 Loss: 0.229078\n",
      "2025-04-23 14:32: Train Epoch 29: 0/37 Loss: 0.229078\n",
      "2025-04-23 14:32: Train Epoch 29: 0/37 Loss: 0.229078\n",
      "2025-04-23 14:32: Train Epoch 29: 0/37 Loss: 0.229078\n",
      "2025-04-23 14:32: Train Epoch 29: 0/37 Loss: 0.229078\n",
      "2025-04-23 14:32: Train Epoch 29: 20/37 Loss: 0.042223\n",
      "2025-04-23 14:32: Train Epoch 29: 20/37 Loss: 0.042223\n",
      "2025-04-23 14:32: Train Epoch 29: 20/37 Loss: 0.042223\n",
      "2025-04-23 14:32: Train Epoch 29: 20/37 Loss: 0.042223\n",
      "2025-04-23 14:32: Train Epoch 29: 20/37 Loss: 0.042223\n",
      "2025-04-23 14:32: **********Train Epoch 29: averaged Loss: 0.183519\n",
      "2025-04-23 14:32: **********Train Epoch 29: averaged Loss: 0.183519\n",
      "2025-04-23 14:32: **********Train Epoch 29: averaged Loss: 0.183519\n",
      "2025-04-23 14:32: **********Train Epoch 29: averaged Loss: 0.183519\n",
      "2025-04-23 14:32: **********Train Epoch 29: averaged Loss: 0.183519\n",
      "2025-04-23 14:32: **********Val Epoch 29: average Loss: 0.397314\n",
      "2025-04-23 14:32: **********Val Epoch 29: average Loss: 0.397314\n",
      "2025-04-23 14:32: **********Val Epoch 29: average Loss: 0.397314\n",
      "2025-04-23 14:32: **********Val Epoch 29: average Loss: 0.397314\n",
      "2025-04-23 14:32: **********Val Epoch 29: average Loss: 0.397314\n",
      "2025-04-23 14:32: Train Epoch 30: 0/37 Loss: 0.228030\n",
      "2025-04-23 14:32: Train Epoch 30: 0/37 Loss: 0.228030\n",
      "2025-04-23 14:32: Train Epoch 30: 0/37 Loss: 0.228030\n",
      "2025-04-23 14:32: Train Epoch 30: 0/37 Loss: 0.228030\n",
      "2025-04-23 14:32: Train Epoch 30: 0/37 Loss: 0.228030\n",
      "2025-04-23 14:32: Train Epoch 30: 20/37 Loss: 0.043219\n",
      "2025-04-23 14:32: Train Epoch 30: 20/37 Loss: 0.043219\n",
      "2025-04-23 14:32: Train Epoch 30: 20/37 Loss: 0.043219\n",
      "2025-04-23 14:32: Train Epoch 30: 20/37 Loss: 0.043219\n",
      "2025-04-23 14:32: Train Epoch 30: 20/37 Loss: 0.043219\n",
      "2025-04-23 14:32: **********Train Epoch 30: averaged Loss: 0.183439\n",
      "2025-04-23 14:32: **********Train Epoch 30: averaged Loss: 0.183439\n",
      "2025-04-23 14:32: **********Train Epoch 30: averaged Loss: 0.183439\n",
      "2025-04-23 14:32: **********Train Epoch 30: averaged Loss: 0.183439\n",
      "2025-04-23 14:32: **********Train Epoch 30: averaged Loss: 0.183439\n",
      "2025-04-23 14:32: **********Val Epoch 30: average Loss: 0.398175\n",
      "2025-04-23 14:32: **********Val Epoch 30: average Loss: 0.398175\n",
      "2025-04-23 14:32: **********Val Epoch 30: average Loss: 0.398175\n",
      "2025-04-23 14:32: **********Val Epoch 30: average Loss: 0.398175\n",
      "2025-04-23 14:32: **********Val Epoch 30: average Loss: 0.398175\n",
      "2025-04-23 14:32: Train Epoch 31: 0/37 Loss: 0.227168\n",
      "2025-04-23 14:32: Train Epoch 31: 0/37 Loss: 0.227168\n",
      "2025-04-23 14:32: Train Epoch 31: 0/37 Loss: 0.227168\n",
      "2025-04-23 14:32: Train Epoch 31: 0/37 Loss: 0.227168\n",
      "2025-04-23 14:32: Train Epoch 31: 0/37 Loss: 0.227168\n",
      "2025-04-23 14:32: Train Epoch 31: 20/37 Loss: 0.044047\n",
      "2025-04-23 14:32: Train Epoch 31: 20/37 Loss: 0.044047\n",
      "2025-04-23 14:32: Train Epoch 31: 20/37 Loss: 0.044047\n",
      "2025-04-23 14:32: Train Epoch 31: 20/37 Loss: 0.044047\n",
      "2025-04-23 14:32: Train Epoch 31: 20/37 Loss: 0.044047\n",
      "2025-04-23 14:32: **********Train Epoch 31: averaged Loss: 0.183400\n",
      "2025-04-23 14:32: **********Train Epoch 31: averaged Loss: 0.183400\n",
      "2025-04-23 14:32: **********Train Epoch 31: averaged Loss: 0.183400\n",
      "2025-04-23 14:32: **********Train Epoch 31: averaged Loss: 0.183400\n",
      "2025-04-23 14:32: **********Train Epoch 31: averaged Loss: 0.183400\n",
      "2025-04-23 14:32: **********Val Epoch 31: average Loss: 0.398955\n",
      "2025-04-23 14:32: **********Val Epoch 31: average Loss: 0.398955\n",
      "2025-04-23 14:32: **********Val Epoch 31: average Loss: 0.398955\n",
      "2025-04-23 14:32: **********Val Epoch 31: average Loss: 0.398955\n",
      "2025-04-23 14:32: **********Val Epoch 31: average Loss: 0.398955\n",
      "2025-04-23 14:32: Train Epoch 32: 0/37 Loss: 0.226388\n",
      "2025-04-23 14:32: Train Epoch 32: 0/37 Loss: 0.226388\n",
      "2025-04-23 14:32: Train Epoch 32: 0/37 Loss: 0.226388\n",
      "2025-04-23 14:32: Train Epoch 32: 0/37 Loss: 0.226388\n",
      "2025-04-23 14:32: Train Epoch 32: 0/37 Loss: 0.226388\n",
      "2025-04-23 14:32: Train Epoch 32: 20/37 Loss: 0.044813\n",
      "2025-04-23 14:32: Train Epoch 32: 20/37 Loss: 0.044813\n",
      "2025-04-23 14:32: Train Epoch 32: 20/37 Loss: 0.044813\n",
      "2025-04-23 14:32: Train Epoch 32: 20/37 Loss: 0.044813\n",
      "2025-04-23 14:32: Train Epoch 32: 20/37 Loss: 0.044813\n",
      "2025-04-23 14:32: **********Train Epoch 32: averaged Loss: 0.183380\n",
      "2025-04-23 14:32: **********Train Epoch 32: averaged Loss: 0.183380\n",
      "2025-04-23 14:32: **********Train Epoch 32: averaged Loss: 0.183380\n",
      "2025-04-23 14:32: **********Train Epoch 32: averaged Loss: 0.183380\n",
      "2025-04-23 14:32: **********Train Epoch 32: averaged Loss: 0.183380\n",
      "2025-04-23 14:32: **********Val Epoch 32: average Loss: 0.399723\n",
      "2025-04-23 14:32: **********Val Epoch 32: average Loss: 0.399723\n",
      "2025-04-23 14:32: **********Val Epoch 32: average Loss: 0.399723\n",
      "2025-04-23 14:32: **********Val Epoch 32: average Loss: 0.399723\n",
      "2025-04-23 14:32: **********Val Epoch 32: average Loss: 0.399723\n",
      "2025-04-23 14:32: Train Epoch 33: 0/37 Loss: 0.225620\n",
      "2025-04-23 14:32: Train Epoch 33: 0/37 Loss: 0.225620\n",
      "2025-04-23 14:32: Train Epoch 33: 0/37 Loss: 0.225620\n",
      "2025-04-23 14:32: Train Epoch 33: 0/37 Loss: 0.225620\n",
      "2025-04-23 14:32: Train Epoch 33: 0/37 Loss: 0.225620\n",
      "2025-04-23 14:32: Train Epoch 33: 20/37 Loss: 0.045570\n",
      "2025-04-23 14:32: Train Epoch 33: 20/37 Loss: 0.045570\n",
      "2025-04-23 14:32: Train Epoch 33: 20/37 Loss: 0.045570\n",
      "2025-04-23 14:32: Train Epoch 33: 20/37 Loss: 0.045570\n",
      "2025-04-23 14:32: Train Epoch 33: 20/37 Loss: 0.045570\n",
      "2025-04-23 14:32: **********Train Epoch 33: averaged Loss: 0.183354\n",
      "2025-04-23 14:32: **********Train Epoch 33: averaged Loss: 0.183354\n",
      "2025-04-23 14:32: **********Train Epoch 33: averaged Loss: 0.183354\n",
      "2025-04-23 14:32: **********Train Epoch 33: averaged Loss: 0.183354\n",
      "2025-04-23 14:32: **********Train Epoch 33: averaged Loss: 0.183354\n",
      "2025-04-23 14:32: **********Val Epoch 33: average Loss: 0.400462\n",
      "2025-04-23 14:32: **********Val Epoch 33: average Loss: 0.400462\n",
      "2025-04-23 14:32: **********Val Epoch 33: average Loss: 0.400462\n",
      "2025-04-23 14:32: **********Val Epoch 33: average Loss: 0.400462\n",
      "2025-04-23 14:32: **********Val Epoch 33: average Loss: 0.400462\n",
      "2025-04-23 14:32: Train Epoch 34: 0/37 Loss: 0.224881\n",
      "2025-04-23 14:32: Train Epoch 34: 0/37 Loss: 0.224881\n",
      "2025-04-23 14:32: Train Epoch 34: 0/37 Loss: 0.224881\n",
      "2025-04-23 14:32: Train Epoch 34: 0/37 Loss: 0.224881\n",
      "2025-04-23 14:32: Train Epoch 34: 0/37 Loss: 0.224881\n",
      "2025-04-23 14:32: Train Epoch 34: 20/37 Loss: 0.046296\n",
      "2025-04-23 14:32: Train Epoch 34: 20/37 Loss: 0.046296\n",
      "2025-04-23 14:32: Train Epoch 34: 20/37 Loss: 0.046296\n",
      "2025-04-23 14:32: Train Epoch 34: 20/37 Loss: 0.046296\n",
      "2025-04-23 14:32: Train Epoch 34: 20/37 Loss: 0.046296\n",
      "2025-04-23 14:32: **********Train Epoch 34: averaged Loss: 0.183329\n",
      "2025-04-23 14:32: **********Train Epoch 34: averaged Loss: 0.183329\n",
      "2025-04-23 14:32: **********Train Epoch 34: averaged Loss: 0.183329\n",
      "2025-04-23 14:32: **********Train Epoch 34: averaged Loss: 0.183329\n",
      "2025-04-23 14:32: **********Train Epoch 34: averaged Loss: 0.183329\n",
      "2025-04-23 14:32: **********Val Epoch 34: average Loss: 0.401169\n",
      "2025-04-23 14:32: **********Val Epoch 34: average Loss: 0.401169\n",
      "2025-04-23 14:32: **********Val Epoch 34: average Loss: 0.401169\n",
      "2025-04-23 14:32: **********Val Epoch 34: average Loss: 0.401169\n",
      "2025-04-23 14:32: **********Val Epoch 34: average Loss: 0.401169\n",
      "2025-04-23 14:32: Train Epoch 35: 0/37 Loss: 0.224174\n",
      "2025-04-23 14:32: Train Epoch 35: 0/37 Loss: 0.224174\n",
      "2025-04-23 14:32: Train Epoch 35: 0/37 Loss: 0.224174\n",
      "2025-04-23 14:32: Train Epoch 35: 0/37 Loss: 0.224174\n",
      "2025-04-23 14:32: Train Epoch 35: 0/37 Loss: 0.224174\n",
      "2025-04-23 14:32: Train Epoch 35: 20/37 Loss: 0.046990\n",
      "2025-04-23 14:32: Train Epoch 35: 20/37 Loss: 0.046990\n",
      "2025-04-23 14:32: Train Epoch 35: 20/37 Loss: 0.046990\n",
      "2025-04-23 14:32: Train Epoch 35: 20/37 Loss: 0.046990\n",
      "2025-04-23 14:32: Train Epoch 35: 20/37 Loss: 0.046990\n",
      "2025-04-23 14:32: **********Train Epoch 35: averaged Loss: 0.183305\n",
      "2025-04-23 14:32: **********Train Epoch 35: averaged Loss: 0.183305\n",
      "2025-04-23 14:32: **********Train Epoch 35: averaged Loss: 0.183305\n",
      "2025-04-23 14:32: **********Train Epoch 35: averaged Loss: 0.183305\n",
      "2025-04-23 14:32: **********Train Epoch 35: averaged Loss: 0.183305\n",
      "2025-04-23 14:32: **********Val Epoch 35: average Loss: 0.401844\n",
      "2025-04-23 14:32: **********Val Epoch 35: average Loss: 0.401844\n",
      "2025-04-23 14:32: **********Val Epoch 35: average Loss: 0.401844\n",
      "2025-04-23 14:32: **********Val Epoch 35: average Loss: 0.401844\n",
      "2025-04-23 14:32: **********Val Epoch 35: average Loss: 0.401844\n",
      "2025-04-23 14:32: Train Epoch 36: 0/37 Loss: 0.223500\n",
      "2025-04-23 14:32: Train Epoch 36: 0/37 Loss: 0.223500\n",
      "2025-04-23 14:32: Train Epoch 36: 0/37 Loss: 0.223500\n",
      "2025-04-23 14:32: Train Epoch 36: 0/37 Loss: 0.223500\n",
      "2025-04-23 14:32: Train Epoch 36: 0/37 Loss: 0.223500\n",
      "2025-04-23 14:32: Train Epoch 36: 20/37 Loss: 0.047658\n",
      "2025-04-23 14:32: Train Epoch 36: 20/37 Loss: 0.047658\n",
      "2025-04-23 14:32: Train Epoch 36: 20/37 Loss: 0.047658\n",
      "2025-04-23 14:32: Train Epoch 36: 20/37 Loss: 0.047658\n",
      "2025-04-23 14:32: Train Epoch 36: 20/37 Loss: 0.047658\n",
      "2025-04-23 14:32: **********Train Epoch 36: averaged Loss: 0.183291\n",
      "2025-04-23 14:32: **********Train Epoch 36: averaged Loss: 0.183291\n",
      "2025-04-23 14:32: **********Train Epoch 36: averaged Loss: 0.183291\n",
      "2025-04-23 14:32: **********Train Epoch 36: averaged Loss: 0.183291\n",
      "2025-04-23 14:32: **********Train Epoch 36: averaged Loss: 0.183291\n",
      "2025-04-23 14:32: **********Val Epoch 36: average Loss: 0.402513\n",
      "2025-04-23 14:32: **********Val Epoch 36: average Loss: 0.402513\n",
      "2025-04-23 14:32: **********Val Epoch 36: average Loss: 0.402513\n",
      "2025-04-23 14:32: **********Val Epoch 36: average Loss: 0.402513\n",
      "2025-04-23 14:32: **********Val Epoch 36: average Loss: 0.402513\n",
      "2025-04-23 14:32: Train Epoch 37: 0/37 Loss: 0.222830\n",
      "2025-04-23 14:32: Train Epoch 37: 0/37 Loss: 0.222830\n",
      "2025-04-23 14:32: Train Epoch 37: 0/37 Loss: 0.222830\n",
      "2025-04-23 14:32: Train Epoch 37: 0/37 Loss: 0.222830\n",
      "2025-04-23 14:32: Train Epoch 37: 0/37 Loss: 0.222830\n",
      "2025-04-23 14:32: Train Epoch 37: 20/37 Loss: 0.048325\n",
      "2025-04-23 14:32: Train Epoch 37: 20/37 Loss: 0.048325\n",
      "2025-04-23 14:32: Train Epoch 37: 20/37 Loss: 0.048325\n",
      "2025-04-23 14:32: Train Epoch 37: 20/37 Loss: 0.048325\n",
      "2025-04-23 14:32: Train Epoch 37: 20/37 Loss: 0.048325\n",
      "2025-04-23 14:32: **********Train Epoch 37: averaged Loss: 0.183273\n",
      "2025-04-23 14:32: **********Train Epoch 37: averaged Loss: 0.183273\n",
      "2025-04-23 14:32: **********Train Epoch 37: averaged Loss: 0.183273\n",
      "2025-04-23 14:32: **********Train Epoch 37: averaged Loss: 0.183273\n",
      "2025-04-23 14:32: **********Train Epoch 37: averaged Loss: 0.183273\n",
      "2025-04-23 14:32: **********Val Epoch 37: average Loss: 0.403158\n",
      "2025-04-23 14:32: **********Val Epoch 37: average Loss: 0.403158\n",
      "2025-04-23 14:32: **********Val Epoch 37: average Loss: 0.403158\n",
      "2025-04-23 14:32: **********Val Epoch 37: average Loss: 0.403158\n",
      "2025-04-23 14:32: **********Val Epoch 37: average Loss: 0.403158\n",
      "2025-04-23 14:32: Train Epoch 38: 0/37 Loss: 0.222185\n",
      "2025-04-23 14:32: Train Epoch 38: 0/37 Loss: 0.222185\n",
      "2025-04-23 14:32: Train Epoch 38: 0/37 Loss: 0.222185\n",
      "2025-04-23 14:32: Train Epoch 38: 0/37 Loss: 0.222185\n",
      "2025-04-23 14:32: Train Epoch 38: 0/37 Loss: 0.222185\n",
      "2025-04-23 14:32: Train Epoch 38: 20/37 Loss: 0.048960\n",
      "2025-04-23 14:32: Train Epoch 38: 20/37 Loss: 0.048960\n",
      "2025-04-23 14:32: Train Epoch 38: 20/37 Loss: 0.048960\n",
      "2025-04-23 14:32: Train Epoch 38: 20/37 Loss: 0.048960\n",
      "2025-04-23 14:32: Train Epoch 38: 20/37 Loss: 0.048960\n",
      "2025-04-23 14:32: **********Train Epoch 38: averaged Loss: 0.183260\n",
      "2025-04-23 14:32: **********Train Epoch 38: averaged Loss: 0.183260\n",
      "2025-04-23 14:32: **********Train Epoch 38: averaged Loss: 0.183260\n",
      "2025-04-23 14:32: **********Train Epoch 38: averaged Loss: 0.183260\n",
      "2025-04-23 14:32: **********Train Epoch 38: averaged Loss: 0.183260\n",
      "2025-04-23 14:32: **********Val Epoch 38: average Loss: 0.403796\n",
      "2025-04-23 14:32: **********Val Epoch 38: average Loss: 0.403796\n",
      "2025-04-23 14:32: **********Val Epoch 38: average Loss: 0.403796\n",
      "2025-04-23 14:32: **********Val Epoch 38: average Loss: 0.403796\n",
      "2025-04-23 14:32: **********Val Epoch 38: average Loss: 0.403796\n",
      "2025-04-23 14:32: Train Epoch 39: 0/37 Loss: 0.221548\n",
      "2025-04-23 14:32: Train Epoch 39: 0/37 Loss: 0.221548\n",
      "2025-04-23 14:32: Train Epoch 39: 0/37 Loss: 0.221548\n",
      "2025-04-23 14:32: Train Epoch 39: 0/37 Loss: 0.221548\n",
      "2025-04-23 14:32: Train Epoch 39: 0/37 Loss: 0.221548\n",
      "2025-04-23 14:32: Train Epoch 39: 20/37 Loss: 0.049595\n",
      "2025-04-23 14:32: Train Epoch 39: 20/37 Loss: 0.049595\n",
      "2025-04-23 14:32: Train Epoch 39: 20/37 Loss: 0.049595\n",
      "2025-04-23 14:32: Train Epoch 39: 20/37 Loss: 0.049595\n",
      "2025-04-23 14:32: Train Epoch 39: 20/37 Loss: 0.049595\n",
      "2025-04-23 14:33: **********Train Epoch 39: averaged Loss: 0.183248\n",
      "2025-04-23 14:33: **********Train Epoch 39: averaged Loss: 0.183248\n",
      "2025-04-23 14:33: **********Train Epoch 39: averaged Loss: 0.183248\n",
      "2025-04-23 14:33: **********Train Epoch 39: averaged Loss: 0.183248\n",
      "2025-04-23 14:33: **********Train Epoch 39: averaged Loss: 0.183248\n",
      "2025-04-23 14:33: **********Val Epoch 39: average Loss: 0.404433\n",
      "2025-04-23 14:33: **********Val Epoch 39: average Loss: 0.404433\n",
      "2025-04-23 14:33: **********Val Epoch 39: average Loss: 0.404433\n",
      "2025-04-23 14:33: **********Val Epoch 39: average Loss: 0.404433\n",
      "2025-04-23 14:33: **********Val Epoch 39: average Loss: 0.404433\n",
      "2025-04-23 14:33: Train Epoch 40: 0/37 Loss: 0.220911\n",
      "2025-04-23 14:33: Train Epoch 40: 0/37 Loss: 0.220911\n",
      "2025-04-23 14:33: Train Epoch 40: 0/37 Loss: 0.220911\n",
      "2025-04-23 14:33: Train Epoch 40: 0/37 Loss: 0.220911\n",
      "2025-04-23 14:33: Train Epoch 40: 0/37 Loss: 0.220911\n",
      "2025-04-23 14:33: Train Epoch 40: 20/37 Loss: 0.050218\n",
      "2025-04-23 14:33: Train Epoch 40: 20/37 Loss: 0.050218\n",
      "2025-04-23 14:33: Train Epoch 40: 20/37 Loss: 0.050218\n",
      "2025-04-23 14:33: Train Epoch 40: 20/37 Loss: 0.050218\n",
      "2025-04-23 14:33: Train Epoch 40: 20/37 Loss: 0.050218\n",
      "2025-04-23 14:33: **********Train Epoch 40: averaged Loss: 0.183219\n",
      "2025-04-23 14:33: **********Train Epoch 40: averaged Loss: 0.183219\n",
      "2025-04-23 14:33: **********Train Epoch 40: averaged Loss: 0.183219\n",
      "2025-04-23 14:33: **********Train Epoch 40: averaged Loss: 0.183219\n",
      "2025-04-23 14:33: **********Train Epoch 40: averaged Loss: 0.183219\n",
      "2025-04-23 14:33: **********Val Epoch 40: average Loss: 0.405014\n",
      "2025-04-23 14:33: **********Val Epoch 40: average Loss: 0.405014\n",
      "2025-04-23 14:33: **********Val Epoch 40: average Loss: 0.405014\n",
      "2025-04-23 14:33: **********Val Epoch 40: average Loss: 0.405014\n",
      "2025-04-23 14:33: **********Val Epoch 40: average Loss: 0.405014\n",
      "2025-04-23 14:33: Train Epoch 41: 0/37 Loss: 0.220329\n",
      "2025-04-23 14:33: Train Epoch 41: 0/37 Loss: 0.220329\n",
      "2025-04-23 14:33: Train Epoch 41: 0/37 Loss: 0.220329\n",
      "2025-04-23 14:33: Train Epoch 41: 0/37 Loss: 0.220329\n",
      "2025-04-23 14:33: Train Epoch 41: 0/37 Loss: 0.220329\n",
      "2025-04-23 14:33: Train Epoch 41: 20/37 Loss: 0.050759\n",
      "2025-04-23 14:33: Train Epoch 41: 20/37 Loss: 0.050759\n",
      "2025-04-23 14:33: Train Epoch 41: 20/37 Loss: 0.050759\n",
      "2025-04-23 14:33: Train Epoch 41: 20/37 Loss: 0.050759\n",
      "2025-04-23 14:33: Train Epoch 41: 20/37 Loss: 0.050759\n",
      "2025-04-23 14:33: **********Train Epoch 41: averaged Loss: 0.183164\n",
      "2025-04-23 14:33: **********Train Epoch 41: averaged Loss: 0.183164\n",
      "2025-04-23 14:33: **********Train Epoch 41: averaged Loss: 0.183164\n",
      "2025-04-23 14:33: **********Train Epoch 41: averaged Loss: 0.183164\n",
      "2025-04-23 14:33: **********Train Epoch 41: averaged Loss: 0.183164\n",
      "2025-04-23 14:33: **********Val Epoch 41: average Loss: 0.405449\n",
      "2025-04-23 14:33: **********Val Epoch 41: average Loss: 0.405449\n",
      "2025-04-23 14:33: **********Val Epoch 41: average Loss: 0.405449\n",
      "2025-04-23 14:33: **********Val Epoch 41: average Loss: 0.405449\n",
      "2025-04-23 14:33: **********Val Epoch 41: average Loss: 0.405449\n",
      "2025-04-23 14:33: Train Epoch 42: 0/37 Loss: 0.219895\n",
      "2025-04-23 14:33: Train Epoch 42: 0/37 Loss: 0.219895\n",
      "2025-04-23 14:33: Train Epoch 42: 0/37 Loss: 0.219895\n",
      "2025-04-23 14:33: Train Epoch 42: 0/37 Loss: 0.219895\n",
      "2025-04-23 14:33: Train Epoch 42: 0/37 Loss: 0.219895\n",
      "2025-04-23 14:33: Train Epoch 42: 20/37 Loss: 0.051168\n",
      "2025-04-23 14:33: Train Epoch 42: 20/37 Loss: 0.051168\n",
      "2025-04-23 14:33: Train Epoch 42: 20/37 Loss: 0.051168\n",
      "2025-04-23 14:33: Train Epoch 42: 20/37 Loss: 0.051168\n",
      "2025-04-23 14:33: Train Epoch 42: 20/37 Loss: 0.051168\n",
      "2025-04-23 14:33: **********Train Epoch 42: averaged Loss: 0.183146\n",
      "2025-04-23 14:33: **********Train Epoch 42: averaged Loss: 0.183146\n",
      "2025-04-23 14:33: **********Train Epoch 42: averaged Loss: 0.183146\n",
      "2025-04-23 14:33: **********Train Epoch 42: averaged Loss: 0.183146\n",
      "2025-04-23 14:33: **********Train Epoch 42: averaged Loss: 0.183146\n",
      "2025-04-23 14:33: **********Val Epoch 42: average Loss: 0.405835\n",
      "2025-04-23 14:33: **********Val Epoch 42: average Loss: 0.405835\n",
      "2025-04-23 14:33: **********Val Epoch 42: average Loss: 0.405835\n",
      "2025-04-23 14:33: **********Val Epoch 42: average Loss: 0.405835\n",
      "2025-04-23 14:33: **********Val Epoch 42: average Loss: 0.405835\n",
      "2025-04-23 14:33: Train Epoch 43: 0/37 Loss: 0.219509\n",
      "2025-04-23 14:33: Train Epoch 43: 0/37 Loss: 0.219509\n",
      "2025-04-23 14:33: Train Epoch 43: 0/37 Loss: 0.219509\n",
      "2025-04-23 14:33: Train Epoch 43: 0/37 Loss: 0.219509\n",
      "2025-04-23 14:33: Train Epoch 43: 0/37 Loss: 0.219509\n",
      "2025-04-23 14:33: Train Epoch 43: 20/37 Loss: 0.051548\n",
      "2025-04-23 14:33: Train Epoch 43: 20/37 Loss: 0.051548\n",
      "2025-04-23 14:33: Train Epoch 43: 20/37 Loss: 0.051548\n",
      "2025-04-23 14:33: Train Epoch 43: 20/37 Loss: 0.051548\n",
      "2025-04-23 14:33: Train Epoch 43: 20/37 Loss: 0.051548\n",
      "2025-04-23 14:33: **********Train Epoch 43: averaged Loss: 0.183141\n",
      "2025-04-23 14:33: **********Train Epoch 43: averaged Loss: 0.183141\n",
      "2025-04-23 14:33: **********Train Epoch 43: averaged Loss: 0.183141\n",
      "2025-04-23 14:33: **********Train Epoch 43: averaged Loss: 0.183141\n",
      "2025-04-23 14:33: **********Train Epoch 43: averaged Loss: 0.183141\n",
      "2025-04-23 14:33: **********Val Epoch 43: average Loss: 0.406216\n",
      "2025-04-23 14:33: **********Val Epoch 43: average Loss: 0.406216\n",
      "2025-04-23 14:33: **********Val Epoch 43: average Loss: 0.406216\n",
      "2025-04-23 14:33: **********Val Epoch 43: average Loss: 0.406216\n",
      "2025-04-23 14:33: **********Val Epoch 43: average Loss: 0.406216\n",
      "2025-04-23 14:33: Train Epoch 44: 0/37 Loss: 0.219128\n",
      "2025-04-23 14:33: Train Epoch 44: 0/37 Loss: 0.219128\n",
      "2025-04-23 14:33: Train Epoch 44: 0/37 Loss: 0.219128\n",
      "2025-04-23 14:33: Train Epoch 44: 0/37 Loss: 0.219128\n",
      "2025-04-23 14:33: Train Epoch 44: 0/37 Loss: 0.219128\n",
      "2025-04-23 14:33: Train Epoch 44: 20/37 Loss: 0.051928\n",
      "2025-04-23 14:33: Train Epoch 44: 20/37 Loss: 0.051928\n",
      "2025-04-23 14:33: Train Epoch 44: 20/37 Loss: 0.051928\n",
      "2025-04-23 14:33: Train Epoch 44: 20/37 Loss: 0.051928\n",
      "2025-04-23 14:33: Train Epoch 44: 20/37 Loss: 0.051928\n",
      "2025-04-23 14:33: **********Train Epoch 44: averaged Loss: 0.183137\n",
      "2025-04-23 14:33: **********Train Epoch 44: averaged Loss: 0.183137\n",
      "2025-04-23 14:33: **********Train Epoch 44: averaged Loss: 0.183137\n",
      "2025-04-23 14:33: **********Train Epoch 44: averaged Loss: 0.183137\n",
      "2025-04-23 14:33: **********Train Epoch 44: averaged Loss: 0.183137\n",
      "2025-04-23 14:33: **********Val Epoch 44: average Loss: 0.406596\n",
      "2025-04-23 14:33: **********Val Epoch 44: average Loss: 0.406596\n",
      "2025-04-23 14:33: **********Val Epoch 44: average Loss: 0.406596\n",
      "2025-04-23 14:33: **********Val Epoch 44: average Loss: 0.406596\n",
      "2025-04-23 14:33: **********Val Epoch 44: average Loss: 0.406596\n",
      "2025-04-23 14:33: Train Epoch 45: 0/37 Loss: 0.218747\n",
      "2025-04-23 14:33: Train Epoch 45: 0/37 Loss: 0.218747\n",
      "2025-04-23 14:33: Train Epoch 45: 0/37 Loss: 0.218747\n",
      "2025-04-23 14:33: Train Epoch 45: 0/37 Loss: 0.218747\n",
      "2025-04-23 14:33: Train Epoch 45: 0/37 Loss: 0.218747\n",
      "2025-04-23 14:33: Train Epoch 45: 20/37 Loss: 0.052289\n",
      "2025-04-23 14:33: Train Epoch 45: 20/37 Loss: 0.052289\n",
      "2025-04-23 14:33: Train Epoch 45: 20/37 Loss: 0.052289\n",
      "2025-04-23 14:33: Train Epoch 45: 20/37 Loss: 0.052289\n",
      "2025-04-23 14:33: Train Epoch 45: 20/37 Loss: 0.052289\n",
      "2025-04-23 14:33: **********Train Epoch 45: averaged Loss: 0.183106\n",
      "2025-04-23 14:33: **********Train Epoch 45: averaged Loss: 0.183106\n",
      "2025-04-23 14:33: **********Train Epoch 45: averaged Loss: 0.183106\n",
      "2025-04-23 14:33: **********Train Epoch 45: averaged Loss: 0.183106\n",
      "2025-04-23 14:33: **********Train Epoch 45: averaged Loss: 0.183106\n",
      "2025-04-23 14:33: **********Val Epoch 45: average Loss: 0.406894\n",
      "2025-04-23 14:33: **********Val Epoch 45: average Loss: 0.406894\n",
      "2025-04-23 14:33: **********Val Epoch 45: average Loss: 0.406894\n",
      "2025-04-23 14:33: **********Val Epoch 45: average Loss: 0.406894\n",
      "2025-04-23 14:33: **********Val Epoch 45: average Loss: 0.406894\n",
      "2025-04-23 14:33: Train Epoch 46: 0/37 Loss: 0.218449\n",
      "2025-04-23 14:33: Train Epoch 46: 0/37 Loss: 0.218449\n",
      "2025-04-23 14:33: Train Epoch 46: 0/37 Loss: 0.218449\n",
      "2025-04-23 14:33: Train Epoch 46: 0/37 Loss: 0.218449\n",
      "2025-04-23 14:33: Train Epoch 46: 0/37 Loss: 0.218449\n",
      "2025-04-23 14:33: Train Epoch 46: 20/37 Loss: 0.052563\n",
      "2025-04-23 14:33: Train Epoch 46: 20/37 Loss: 0.052563\n",
      "2025-04-23 14:33: Train Epoch 46: 20/37 Loss: 0.052563\n",
      "2025-04-23 14:33: Train Epoch 46: 20/37 Loss: 0.052563\n",
      "2025-04-23 14:33: Train Epoch 46: 20/37 Loss: 0.052563\n",
      "2025-04-23 14:33: **********Train Epoch 46: averaged Loss: 0.183084\n",
      "2025-04-23 14:33: **********Train Epoch 46: averaged Loss: 0.183084\n",
      "2025-04-23 14:33: **********Train Epoch 46: averaged Loss: 0.183084\n",
      "2025-04-23 14:33: **********Train Epoch 46: averaged Loss: 0.183084\n",
      "2025-04-23 14:33: **********Train Epoch 46: averaged Loss: 0.183084\n",
      "2025-04-23 14:33: **********Val Epoch 46: average Loss: 0.407125\n",
      "2025-04-23 14:33: **********Val Epoch 46: average Loss: 0.407125\n",
      "2025-04-23 14:33: **********Val Epoch 46: average Loss: 0.407125\n",
      "2025-04-23 14:33: **********Val Epoch 46: average Loss: 0.407125\n",
      "2025-04-23 14:33: **********Val Epoch 46: average Loss: 0.407125\n",
      "2025-04-23 14:33: Train Epoch 47: 0/37 Loss: 0.218219\n",
      "2025-04-23 14:33: Train Epoch 47: 0/37 Loss: 0.218219\n",
      "2025-04-23 14:33: Train Epoch 47: 0/37 Loss: 0.218219\n",
      "2025-04-23 14:33: Train Epoch 47: 0/37 Loss: 0.218219\n",
      "2025-04-23 14:33: Train Epoch 47: 0/37 Loss: 0.218219\n",
      "2025-04-23 14:33: Train Epoch 47: 20/37 Loss: 0.052785\n",
      "2025-04-23 14:33: Train Epoch 47: 20/37 Loss: 0.052785\n",
      "2025-04-23 14:33: Train Epoch 47: 20/37 Loss: 0.052785\n",
      "2025-04-23 14:33: Train Epoch 47: 20/37 Loss: 0.052785\n",
      "2025-04-23 14:33: Train Epoch 47: 20/37 Loss: 0.052785\n",
      "2025-04-23 14:33: **********Train Epoch 47: averaged Loss: 0.183080\n",
      "2025-04-23 14:33: **********Train Epoch 47: averaged Loss: 0.183080\n",
      "2025-04-23 14:33: **********Train Epoch 47: averaged Loss: 0.183080\n",
      "2025-04-23 14:33: **********Train Epoch 47: averaged Loss: 0.183080\n",
      "2025-04-23 14:33: **********Train Epoch 47: averaged Loss: 0.183080\n",
      "2025-04-23 14:33: **********Val Epoch 47: average Loss: 0.407346\n",
      "2025-04-23 14:33: **********Val Epoch 47: average Loss: 0.407346\n",
      "2025-04-23 14:33: **********Val Epoch 47: average Loss: 0.407346\n",
      "2025-04-23 14:33: **********Val Epoch 47: average Loss: 0.407346\n",
      "2025-04-23 14:33: **********Val Epoch 47: average Loss: 0.407346\n",
      "2025-04-23 14:33: Train Epoch 48: 0/37 Loss: 0.217998\n",
      "2025-04-23 14:33: Train Epoch 48: 0/37 Loss: 0.217998\n",
      "2025-04-23 14:33: Train Epoch 48: 0/37 Loss: 0.217998\n",
      "2025-04-23 14:33: Train Epoch 48: 0/37 Loss: 0.217998\n",
      "2025-04-23 14:33: Train Epoch 48: 0/37 Loss: 0.217998\n",
      "2025-04-23 14:33: Train Epoch 48: 20/37 Loss: 0.053005\n",
      "2025-04-23 14:33: Train Epoch 48: 20/37 Loss: 0.053005\n",
      "2025-04-23 14:33: Train Epoch 48: 20/37 Loss: 0.053005\n",
      "2025-04-23 14:33: Train Epoch 48: 20/37 Loss: 0.053005\n",
      "2025-04-23 14:33: Train Epoch 48: 20/37 Loss: 0.053005\n",
      "2025-04-23 14:33: **********Train Epoch 48: averaged Loss: 0.183079\n",
      "2025-04-23 14:33: **********Train Epoch 48: averaged Loss: 0.183079\n",
      "2025-04-23 14:33: **********Train Epoch 48: averaged Loss: 0.183079\n",
      "2025-04-23 14:33: **********Train Epoch 48: averaged Loss: 0.183079\n",
      "2025-04-23 14:33: **********Train Epoch 48: averaged Loss: 0.183079\n",
      "2025-04-23 14:33: **********Val Epoch 48: average Loss: 0.407567\n",
      "2025-04-23 14:33: **********Val Epoch 48: average Loss: 0.407567\n",
      "2025-04-23 14:33: **********Val Epoch 48: average Loss: 0.407567\n",
      "2025-04-23 14:33: **********Val Epoch 48: average Loss: 0.407567\n",
      "2025-04-23 14:33: **********Val Epoch 48: average Loss: 0.407567\n",
      "2025-04-23 14:33: Train Epoch 49: 0/37 Loss: 0.217776\n",
      "2025-04-23 14:33: Train Epoch 49: 0/37 Loss: 0.217776\n",
      "2025-04-23 14:33: Train Epoch 49: 0/37 Loss: 0.217776\n",
      "2025-04-23 14:33: Train Epoch 49: 0/37 Loss: 0.217776\n",
      "2025-04-23 14:33: Train Epoch 49: 0/37 Loss: 0.217776\n",
      "2025-04-23 14:33: Train Epoch 49: 20/37 Loss: 0.053220\n",
      "2025-04-23 14:33: Train Epoch 49: 20/37 Loss: 0.053220\n",
      "2025-04-23 14:33: Train Epoch 49: 20/37 Loss: 0.053220\n",
      "2025-04-23 14:33: Train Epoch 49: 20/37 Loss: 0.053220\n",
      "2025-04-23 14:33: Train Epoch 49: 20/37 Loss: 0.053220\n",
      "2025-04-23 14:33: **********Train Epoch 49: averaged Loss: 0.183068\n",
      "2025-04-23 14:33: **********Train Epoch 49: averaged Loss: 0.183068\n",
      "2025-04-23 14:33: **********Train Epoch 49: averaged Loss: 0.183068\n",
      "2025-04-23 14:33: **********Train Epoch 49: averaged Loss: 0.183068\n",
      "2025-04-23 14:33: **********Train Epoch 49: averaged Loss: 0.183068\n",
      "2025-04-23 14:33: **********Val Epoch 49: average Loss: 0.407761\n",
      "2025-04-23 14:33: **********Val Epoch 49: average Loss: 0.407761\n",
      "2025-04-23 14:33: **********Val Epoch 49: average Loss: 0.407761\n",
      "2025-04-23 14:33: **********Val Epoch 49: average Loss: 0.407761\n",
      "2025-04-23 14:33: **********Val Epoch 49: average Loss: 0.407761\n",
      "2025-04-23 14:33: Train Epoch 50: 0/37 Loss: 0.217583\n",
      "2025-04-23 14:33: Train Epoch 50: 0/37 Loss: 0.217583\n",
      "2025-04-23 14:33: Train Epoch 50: 0/37 Loss: 0.217583\n",
      "2025-04-23 14:33: Train Epoch 50: 0/37 Loss: 0.217583\n",
      "2025-04-23 14:33: Train Epoch 50: 0/37 Loss: 0.217583\n",
      "2025-04-23 14:33: Train Epoch 50: 20/37 Loss: 0.053409\n",
      "2025-04-23 14:33: Train Epoch 50: 20/37 Loss: 0.053409\n",
      "2025-04-23 14:33: Train Epoch 50: 20/37 Loss: 0.053409\n",
      "2025-04-23 14:33: Train Epoch 50: 20/37 Loss: 0.053409\n",
      "2025-04-23 14:33: Train Epoch 50: 20/37 Loss: 0.053409\n",
      "2025-04-23 14:33: **********Train Epoch 50: averaged Loss: 0.183066\n",
      "2025-04-23 14:33: **********Train Epoch 50: averaged Loss: 0.183066\n",
      "2025-04-23 14:33: **********Train Epoch 50: averaged Loss: 0.183066\n",
      "2025-04-23 14:33: **********Train Epoch 50: averaged Loss: 0.183066\n",
      "2025-04-23 14:33: **********Train Epoch 50: averaged Loss: 0.183066\n",
      "2025-04-23 14:33: **********Val Epoch 50: average Loss: 0.407951\n",
      "2025-04-23 14:33: **********Val Epoch 50: average Loss: 0.407951\n",
      "2025-04-23 14:33: **********Val Epoch 50: average Loss: 0.407951\n",
      "2025-04-23 14:33: **********Val Epoch 50: average Loss: 0.407951\n",
      "2025-04-23 14:33: **********Val Epoch 50: average Loss: 0.407951\n",
      "2025-04-23 14:33: Train Epoch 51: 0/37 Loss: 0.217393\n",
      "2025-04-23 14:33: Train Epoch 51: 0/37 Loss: 0.217393\n",
      "2025-04-23 14:33: Train Epoch 51: 0/37 Loss: 0.217393\n",
      "2025-04-23 14:33: Train Epoch 51: 0/37 Loss: 0.217393\n",
      "2025-04-23 14:33: Train Epoch 51: 0/37 Loss: 0.217393\n",
      "2025-04-23 14:33: Train Epoch 51: 20/37 Loss: 0.053598\n",
      "2025-04-23 14:33: Train Epoch 51: 20/37 Loss: 0.053598\n",
      "2025-04-23 14:33: Train Epoch 51: 20/37 Loss: 0.053598\n",
      "2025-04-23 14:33: Train Epoch 51: 20/37 Loss: 0.053598\n",
      "2025-04-23 14:33: Train Epoch 51: 20/37 Loss: 0.053598\n",
      "2025-04-23 14:33: **********Train Epoch 51: averaged Loss: 0.183065\n",
      "2025-04-23 14:33: **********Train Epoch 51: averaged Loss: 0.183065\n",
      "2025-04-23 14:33: **********Train Epoch 51: averaged Loss: 0.183065\n",
      "2025-04-23 14:33: **********Train Epoch 51: averaged Loss: 0.183065\n",
      "2025-04-23 14:33: **********Train Epoch 51: averaged Loss: 0.183065\n",
      "2025-04-23 14:33: **********Val Epoch 51: average Loss: 0.408140\n",
      "2025-04-23 14:33: **********Val Epoch 51: average Loss: 0.408140\n",
      "2025-04-23 14:33: **********Val Epoch 51: average Loss: 0.408140\n",
      "2025-04-23 14:33: **********Val Epoch 51: average Loss: 0.408140\n",
      "2025-04-23 14:33: **********Val Epoch 51: average Loss: 0.408140\n",
      "2025-04-23 14:33: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:33: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:33: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:33: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:33: Validation performance didn't improve for 50 epochs. Training stops.\n",
      "2025-04-23 14:33: Total training time: 1.4296min, best loss: 0.057631\n",
      "2025-04-23 14:33: Total training time: 1.4296min, best loss: 0.057631\n",
      "2025-04-23 14:33: Total training time: 1.4296min, best loss: 0.057631\n",
      "2025-04-23 14:33: Total training time: 1.4296min, best loss: 0.057631\n",
      "2025-04-23 14:33: Total training time: 1.4296min, best loss: 0.057631\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5900, MSE: 0.5906\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5900, MSE: 0.5906\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5900, MSE: 0.5906\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5900, MSE: 0.5906\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5900, MSE: 0.5906\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5963, MSE: 0.5978\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5963, MSE: 0.5978\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5963, MSE: 0.5978\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5963, MSE: 0.5978\n",
      "2025-04-23 14:33: Average Horizon, MAE: 0.5963, MSE: 0.5978\n"
     ]
    }
   ],
   "source": [
    "# TRAINING EXECUTION\n",
    "\n",
    "\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Pearson correlation coefficient between two PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Pearson correlation coefficient.\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are of type float32\n",
    "    x = x.float()\n",
    "    y = y.float()\n",
    "\n",
    "    # Compute the mean of each tensor\n",
    "    mean_x = torch.mean(x)\n",
    "    mean_y = torch.mean(y)\n",
    "\n",
    "    # Compute the deviations from the mean\n",
    "    dev_x = x - mean_x\n",
    "    dev_y = y - mean_y\n",
    "\n",
    "    # Compute the covariance between x and y\n",
    "    covariance = torch.sum(dev_x * dev_y)\n",
    "\n",
    "    # Compute the standard deviations of x and y\n",
    "    std_x = torch.sqrt(torch.sum(dev_x**2))\n",
    "    std_y = torch.sqrt(torch.sum(dev_y**2))\n",
    "\n",
    "    # Compute the Pearson correlation coefficient\n",
    "    pearson_corr = covariance / (std_x * std_y)\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "\n",
    "def rank_tensor(x):\n",
    "    \"\"\"\n",
    "    Return the ranks of elements in a tensor.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): Input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Ranks of the input tensor elements.\n",
    "    \"\"\"\n",
    "    # Get the sorted indices\n",
    "    sorted_indices = torch.argsort(x)\n",
    "\n",
    "    # Create an empty tensor to hold the ranks\n",
    "    ranks = torch.zeros_like(sorted_indices, dtype=torch.float)\n",
    "\n",
    "    # Assign ranks based on sorted indices\n",
    "    ranks[sorted_indices] = torch.arange(1, len(x) + 1).float()\n",
    "\n",
    "    return ranks\n",
    "\n",
    "\n",
    "def rank_information_coefficient(x, y):\n",
    "    \"\"\"\n",
    "    Calculate the Rank Information Coefficient (RIC) or Spearman's Rank Correlation Coefficient.\n",
    "\n",
    "    Args:\n",
    "    x (torch.Tensor): First input tensor.\n",
    "    y (torch.Tensor): Second input tensor.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Rank Information Coefficient (RIC).\n",
    "    \"\"\"\n",
    "    # Get the ranks of the elements in x and y\n",
    "    rank_x = rank_tensor(x)\n",
    "    rank_y = rank_tensor(y)\n",
    "\n",
    "    # Calculate the mean rank for both tensors\n",
    "    mean_rank_x = torch.mean(rank_x)\n",
    "    mean_rank_y = torch.mean(rank_y)\n",
    "\n",
    "    # Calculate the covariance of the rank variables\n",
    "    covariance = torch.sum((rank_x - mean_rank_x) * (rank_y - mean_rank_y))\n",
    "\n",
    "    # Calculate the standard deviations of the ranks\n",
    "    std_rank_x = torch.sqrt(torch.sum((rank_x - mean_rank_x) ** 2))\n",
    "    std_rank_y = torch.sqrt(torch.sum((rank_y - mean_rank_y) ** 2))\n",
    "\n",
    "    # Calculate the Spearman rank correlation (RIC)\n",
    "    ric = covariance / (std_rank_x * std_rank_y)\n",
    "\n",
    "    return ric\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    Mode = \"train\"\n",
    "    DEBUG = \"FALSE\"  # 'TRUE'\n",
    "    DATASET = \"PEMSD8\"  # PEMSD4 or PEMSD8\n",
    "    DEVICE = \"cuda:0\"\n",
    "    MODEL = \"AGCRN\"\n",
    "\n",
    "    # get configuration\n",
    "    config_file = \"./{}_{}.conf\".format(DATASET, MODEL)\n",
    "    # print('Read configuration file: %s' % (config_file))\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read(config_file)\n",
    "\n",
    "    # parser\n",
    "    args = {\n",
    "        \"dataset\": DATASET,\n",
    "        \"mode\": Mode,\n",
    "        \"device\": DEVICE,\n",
    "        \"debug\": DEBUG,\n",
    "        \"model\": MODEL,\n",
    "        \"cuda\": True,\n",
    "        \"val_ratio\": 0.15,\n",
    "        \"test_ratio\": 0.15,\n",
    "        \"lag\": window,\n",
    "        \"horizon\": predict,\n",
    "        \"num_nodes\": XX.shape[2],\n",
    "        \"tod\": False,\n",
    "        \"normalizer\": \"std\",\n",
    "        \"column_wise\": False,\n",
    "        \"default_graph\": True,\n",
    "        \"input_dim\": 1,\n",
    "        \"output_dim\": 1,\n",
    "        \"embed_dim\": 10,  # \"cheb_k\":3, #GCN param\n",
    "        # \"gat_heads\": 2,\n",
    "        \"rnn_units\": 128,\n",
    "        \"num_layers\": 3,\n",
    "        \"loss_func\": \"mae\",\n",
    "        \"seed\": 1,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": 1100,\n",
    "        \"lr_init\": 0.001,\n",
    "        \"lr_decay\": True,\n",
    "        \"lr_decay_rate\": 0.5,\n",
    "        \"lr_decay_step\": [40, 70, 100],\n",
    "        \"early_stop\": True,\n",
    "        \"early_stop_patience\": 50,\n",
    "        \"grad_norm\": False,\n",
    "        \"max_grad_norm\": 5,\n",
    "        \"real_value\": False,\n",
    "        \"mae_thresh\": None,\n",
    "        \"mape_thresh\": 0,\n",
    "        \"log_dir\": \"./\",\n",
    "        \"log_step\": 20,\n",
    "        \"plot\": False,\n",
    "        \"teacher_forcing\": False,\n",
    "        \"d_in\": 32,\n",
    "        \"hid\": 32,\n",
    "    }\n",
    "    model_args = ModelArgs(\n",
    "        args.get(\"d_in\"),\n",
    "        args.get(\"num_layers\"),\n",
    "        args.get(\"num_nodes\"),\n",
    "        args.get(\"lag\"),\n",
    "        args.get(\"horizon\"),\n",
    "    )\n",
    "    model = SAMBA_GraphSAGE(\n",
    "        model_args,\n",
    "        args.get(\"hid\"),\n",
    "        args.get(\"lag\"),\n",
    "        args.get(\"horizon\"),\n",
    "        args.get(\"embed_dim\"),\n",
    "    )\n",
    "\n",
    "    model = model.cuda()\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "        else:\n",
    "            nn.init.uniform_(p)\n",
    "    print_model_parameters(model, only_num=False)\n",
    "\n",
    "    # if args.get(\"loss_func\") == \"mask_mae\":\n",
    "    #     loss = masked_mae_loss(scaler, mask_value=0.0)\n",
    "    if args.get(\"loss_func\") == \"mae\":\n",
    "        loss = torch.nn.L1Loss().to(args.get(\"device\"))\n",
    "    elif args.get(\"loss_func\") == \"mse\":\n",
    "        loss = torch.nn.MSELoss().to(args.get(\"device\"))\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        params=model.parameters(),\n",
    "        lr=args.get(\"lr_init\"),\n",
    "        eps=1.0e-8,\n",
    "        weight_decay=0,\n",
    "        amsgrad=False,\n",
    "    )\n",
    "    # learning rate decay\n",
    "    lr_scheduler = None\n",
    "    if args.get(\"lr_decay\"):\n",
    "        print(\"Applying learning rate decay.\")\n",
    "        lr_decay_steps = [int(i) for i in args.get(\"lr_decay_step\")]\n",
    "        lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "            optimizer=optimizer,\n",
    "            milestones=[\n",
    "                0.5 * args.get(\"epochs\"),\n",
    "                0.7 * args.get(\"epochs\"),\n",
    "                0.9 * args.get(\"epochs\"),\n",
    "            ],\n",
    "            gamma=0.1,\n",
    "        )\n",
    "    # lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=64)\n",
    "\n",
    "    # start training\n",
    "    trainer = Trainer(\n",
    "        model,\n",
    "        loss,\n",
    "        optimizer,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        test_loader,\n",
    "        args=args,\n",
    "        lr_scheduler=lr_scheduler,\n",
    "    )\n",
    "    trainer.train()\n",
    "    y1, y2 = trainer.test(trainer.model, trainer.args, test_loader, trainer.logger)\n",
    "    y_p = np.array(y1[:, 0, :].cpu())\n",
    "\n",
    "    y_t = np.array(y2[:, 0, :].cpu())\n",
    "\n",
    "    y_p = mmn.inverse_transform(y_p)\n",
    "\n",
    "    y_t = mmn.inverse_transform(y_t)\n",
    "\n",
    "    # y_p=(y_p-mean)/std\n",
    "    # y_t=(y_t-mean)/std\n",
    "\n",
    "    y_p = torch.tensor(y_p)\n",
    "    y_t = torch.tensor(y_t)\n",
    "\n",
    "    diff = y_p[1:] - y_p[:-1]\n",
    "    return_p = diff / y_p[:-1]\n",
    "\n",
    "    diff = y_t[1:] - y_t[:-1]\n",
    "    return_t = diff / y_t[:-1]\n",
    "\n",
    "    mae, rmse, _ = All_Metrics(return_p, return_t, None, None)\n",
    "\n",
    "    IC = pearson_correlation(return_t, return_p)\n",
    "\n",
    "    RIC = rank_information_coefficient(return_t[:, 0], return_p[:, 0])\n",
    "\n",
    "    result_train_file = os.path.join(\"AGCRN_Model\", \"milan\", \"call\")\n",
    "\n",
    "    save_model(trainer, result_train_file, i + 1)\n",
    "\n",
    "    with open(\"samba_IXIC_staticG.txt\", \"a\") as f:\n",
    "        f.write(\"Epoch: \" + str(i))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"IC: \" + str(np.array(IC)))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"RIC: \" + str(np.array(RIC)))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"mae: \" + str(np.array(mae)))\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"rmse: \" + str(np.array(rmse)))\n",
    "        f.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "134ab3b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:33:21.542545Z",
     "iopub.status.busy": "2025-04-23T14:33:21.542266Z",
     "iopub.status.idle": "2025-04-23T14:33:21.546891Z",
     "shell.execute_reply": "2025-04-23T14:33:21.546044Z"
    },
    "papermill": {
     "duration": 0.218365,
     "end_time": "2025-04-23T14:33:21.547912",
     "exception": false,
     "start_time": "2025-04-23T14:33:21.329547",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "IC: -0.001168349\n",
      "RIC: 0.23124823\n",
      "mae: 0.011068595\n",
      "rmse: 0.015999382\n",
      "\n",
      "Epoch: 1\n",
      "IC: 0.10952199\n",
      "RIC: 0.3469959\n",
      "mae: 0.010698768\n",
      "rmse: 0.01495801\n",
      "\n",
      "Epoch: 2\n",
      "IC: 0.03843931\n",
      "RIC: 0.2993636\n",
      "mae: 0.010786145\n",
      "rmse: 0.01502626\n",
      "\n",
      "Epoch: 3\n",
      "IC: 0.038464323\n",
      "RIC: 0.33738926\n",
      "mae: 0.0105692195\n",
      "rmse: 0.014851957\n",
      "\n",
      "Epoch: 4\n",
      "IC: 0.07484091\n",
      "RIC: -0.008800711\n",
      "mae: 0.010887196\n",
      "rmse: 0.015099954\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"samba_IXIC_staticG.txt\", \"r\") as p:\n",
    "    print(p.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647aa57",
   "metadata": {
    "papermill": {
     "duration": 0.212203,
     "end_time": "2025-04-23T14:33:21.970748",
     "exception": false,
     "start_time": "2025-04-23T14:33:21.758545",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# VISUALLIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ae055ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:33:22.395750Z",
     "iopub.status.busy": "2025-04-23T14:33:22.395069Z",
     "iopub.status.idle": "2025-04-23T14:33:23.068967Z",
     "shell.execute_reply": "2025-04-23T14:33:23.068235Z"
    },
    "papermill": {
     "duration": 0.888507,
     "end_time": "2025-04-23T14:33:23.070346",
     "exception": false,
     "start_time": "2025-04-23T14:33:22.181839",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args:  ModelArgs(d_model=32, n_layer=3, vocab_size=82, seq_in=5, seq_out=1, d_state=128, expand=2, dt_rank=2, d_conv=3, pad_vocab_size_multiple=8, conv_bias=True, bias=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f2773715550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "graphviz.set_jupyter_format(\"png\")\n",
    "\n",
    "Mode = \"train\"\n",
    "DEBUG = \"FALSE\"  #'''True'\n",
    "DATASET = \"PEMSD8\"  # PEMSD4 or PEMSD8\n",
    "DEVICE = \"cuda:0\"\n",
    "MODEL = \"AGCRN\"\n",
    "\n",
    "# get configuration\n",
    "config_file = \"./{}_{}.conf\".format(DATASET, MODEL)\n",
    "# print('Read configuration file: %s' % (config_file))\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# parser\n",
    "args = {\n",
    "    \"dataset\": DATASET,\n",
    "    \"mode\": Mode,\n",
    "    \"device\": DEVICE,\n",
    "    \"debug\": DEBUG,\n",
    "    \"model\": MODEL,\n",
    "    \"cuda\": True,\n",
    "    \"val_ratio\": 0.15,\n",
    "    \"test_ratio\": 0.15,\n",
    "    \"lag\": window,\n",
    "    \"horizon\": predict,\n",
    "    \"num_nodes\": XX.shape[2],\n",
    "    \"tod\": False,\n",
    "    \"normalizer\": \"std\",\n",
    "    \"column_wise\": False,\n",
    "    \"default_graph\": True,\n",
    "    \"input_dim\": 1,\n",
    "    \"output_dim\": 1,\n",
    "    \"embed_dim\": 10,\n",
    "    \"cheb_k\": 3,  # GCN param\n",
    "    \"rnn_units\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"loss_func\": \"mae\",\n",
    "    \"seed\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1100,\n",
    "    \"lr_init\": 0.001,\n",
    "    \"lr_decay\": True,\n",
    "    \"lr_decay_rate\": 0.5,\n",
    "    \"lr_decay_step\": [40, 70, 100],\n",
    "    \"early_stop\": True,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \"grad_norm\": False,\n",
    "    \"max_grad_norm\": 5,\n",
    "    \"real_value\": False,\n",
    "    \"mae_thresh\": None,\n",
    "    \"mape_thresh\": 0,\n",
    "    \"log_dir\": \"./\",\n",
    "    \"log_step\": 20,\n",
    "    \"plot\": False,\n",
    "    \"teacher_forcing\": False,\n",
    "    \"d_in\": 32,\n",
    "    \"hid\": 32,\n",
    "}\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    args.get(\"d_in\"),\n",
    "    args.get(\"num_layers\"),\n",
    "    args.get(\"num_nodes\"),\n",
    "    args.get(\"lag\"),\n",
    "    args.get(\"horizon\"),\n",
    ")\n",
    "print(\"model_args: \", model_args)\n",
    "# model = SAMBA_GATv2(model_args, args.get('hid'), 2)\n",
    "model = SAMBA_GraphSAGE(\n",
    "    model_args,\n",
    "    args.get(\"hid\"),\n",
    "    args.get(\"lag\"),\n",
    "    args.get(\"horizon\"),\n",
    "    args.get(\"embed_dim\"),\n",
    ")\n",
    "\n",
    "# 1 train epoch\n",
    "model_graph = draw_graph(\n",
    "    model,\n",
    "    input_size=[\n",
    "        (64, 5, 82),\n",
    "    ],\n",
    "    depth=1,\n",
    "    expand_nested=True,\n",
    ")\n",
    "model_graph.resize_graph(scale=5.0)  # scale as per the view\n",
    "# model_graph.visual_graph.render(format='svg')\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4fa1e178",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T14:33:23.546719Z",
     "iopub.status.busy": "2025-04-23T14:33:23.546477Z",
     "iopub.status.idle": "2025-04-23T14:33:23.901333Z",
     "shell.execute_reply": "2025-04-23T14:33:23.900549Z"
    },
    "papermill": {
     "duration": 0.568501,
     "end_time": "2025-04-23T14:33:23.902734",
     "exception": false,
     "start_time": "2025-04-23T14:33:23.334233",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_args:  ModelArgs(d_model=32, n_layer=3, vocab_size=82, seq_in=5, seq_out=1, d_state=128, expand=2, dt_rank=2, d_conv=3, pad_vocab_size_multiple=8, conv_bias=True, bias=False)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f2773f3a0d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchview import draw_graph\n",
    "import graphviz\n",
    "\n",
    "graphviz.set_jupyter_format(\"png\")\n",
    "\n",
    "Mode = \"train\"\n",
    "DEBUG = \"FALSE\"  #'''True'\n",
    "DATASET = \"PEMSD8\"  # PEMSD4 or PEMSD8\n",
    "DEVICE = \"cuda:0\"\n",
    "MODEL = \"AGCRN\"\n",
    "\n",
    "# get configuration\n",
    "config_file = \"./{}_{}.conf\".format(DATASET, MODEL)\n",
    "# print('Read configuration file: %s' % (config_file))\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "# parser\n",
    "args = {\n",
    "    \"dataset\": DATASET,\n",
    "    \"mode\": Mode,\n",
    "    \"device\": DEVICE,\n",
    "    \"debug\": DEBUG,\n",
    "    \"model\": MODEL,\n",
    "    \"cuda\": True,\n",
    "    \"val_ratio\": 0.15,\n",
    "    \"test_ratio\": 0.15,\n",
    "    \"lag\": window,\n",
    "    \"horizon\": predict,\n",
    "    \"num_nodes\": XX.shape[2],\n",
    "    \"tod\": False,\n",
    "    \"normalizer\": \"std\",\n",
    "    \"column_wise\": False,\n",
    "    \"default_graph\": True,\n",
    "    \"input_dim\": 1,\n",
    "    \"output_dim\": 1,\n",
    "    \"embed_dim\": 10,\n",
    "    \"cheb_k\": 3,  # GCN param\n",
    "    \"gat_heads\": 2,\n",
    "    \"rnn_units\": 128,\n",
    "    \"num_layers\": 3,\n",
    "    \"loss_func\": \"mae\",\n",
    "    \"seed\": 1,\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 1100,\n",
    "    \"lr_init\": 0.001,\n",
    "    \"lr_decay\": True,\n",
    "    \"lr_decay_rate\": 0.5,\n",
    "    \"lr_decay_step\": [40, 70, 100],\n",
    "    \"early_stop\": True,\n",
    "    \"early_stop_patience\": 200,\n",
    "    \"grad_norm\": False,\n",
    "    \"max_grad_norm\": 5,\n",
    "    \"real_value\": False,\n",
    "    \"mae_thresh\": None,\n",
    "    \"mape_thresh\": 0,\n",
    "    \"log_dir\": \"./\",\n",
    "    \"log_step\": 20,\n",
    "    \"plot\": False,\n",
    "    \"teacher_forcing\": False,\n",
    "    \"d_in\": 32,\n",
    "    \"hid\": 32,\n",
    "}\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    args.get(\"d_in\"),\n",
    "    args.get(\"num_layers\"),\n",
    "    args.get(\"num_nodes\"),\n",
    "    args.get(\"lag\"),\n",
    "    args.get(\"horizon\"),\n",
    ")\n",
    "print(\"model_args: \", model_args)\n",
    "# init model\n",
    "model = SAMBA(\n",
    "    model_args,\n",
    "    args.get(\"hid\"),\n",
    "    args.get(\"lag\"),\n",
    "    args.get(\"horizon\"),\n",
    "    args.get(\"embed_dim\"),\n",
    "    args.get(\"cheb_k\"),\n",
    ")\n",
    "# model = Mamba(ModelArgs(args.get(\"d_in\"),args.get(\"num_layers\"),args.get(\"num_nodes\"),args.get('lag'),args.get('horizon'))\n",
    "#               ,args.get('hid'))\n",
    "\n",
    "# 1 train epoch\n",
    "model_graph = draw_graph(\n",
    "    model,\n",
    "    input_size=[\n",
    "        (64, 5, 82),\n",
    "    ],\n",
    "    depth=1,\n",
    "    expand_nested=True,\n",
    ")\n",
    "model_graph.resize_graph(scale=5.0)  # scale as per the view\n",
    "# model_graph.visual_graph.render(format='svg')\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83341b",
   "metadata": {
    "papermill": {
     "duration": 0.212359,
     "end_time": "2025-04-23T14:33:24.334287",
     "exception": false,
     "start_time": "2025-04-23T14:33:24.121928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6368769,
     "sourceId": 10290628,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 868.694484,
   "end_time": "2025-04-23T14:33:27.546488",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-23T14:18:58.852004",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
